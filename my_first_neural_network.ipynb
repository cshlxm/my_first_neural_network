{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 你的第一个神经网络\n",
    "\n",
    "在此项目中，你将构建你的第一个神经网络，并用该网络预测每日自行车租客人数。我们提供了一些代码，但是需要你来实现神经网络（大部分内容）。提交此项目后，欢迎进一步探索该数据和模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载和准备数据\n",
    "\n",
    "构建神经网络的关键一步是正确地准备数据。不同尺度级别的变量使网络难以高效地掌握正确的权重。我们在下方已经提供了加载和准备数据的代码。你很快将进一步学习这些代码！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = 'Bike-Sharing-Dataset/hour.csv'\n",
    "\n",
    "rides = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据简介\n",
    "\n",
    "此数据集包含的是从 2011 年 1 月 1 日到 2012 年 12 月 31 日期间每天每小时的骑车人数。骑车用户分成临时用户和注册用户，cnt 列是骑车用户数汇总列。你可以在上方看到前几行数据。\n",
    "\n",
    "下图展示的是数据集中前 10 天左右的骑车人数（某些天不一定是 24 个条目，所以不是精确的 10 天）。你可以在这里看到每小时租金。这些数据很复杂！周末的骑行人数少些，工作日上下班期间是骑行高峰期。我们还可以从上方的数据中看到温度、湿度和风速信息，所有这些信息都会影响骑行人数。你需要用你的模型展示所有这些数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10cf2e860>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvgAAAIPCAYAAAAGtapCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzMzM6uQ/wNsebXPxKcHXgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10790c8d0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 263,
       "width": 380
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rides[:24*10].plot(x='dteday', y='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 虚拟变量（哑变量）\n",
    "\n",
    "下面是一些分类变量，例如季节、天气、月份。要在我们的模型中包含这些数据，我们需要创建二进制虚拟变量。用 Pandas 库中的 `get_dummies()` 就可以轻松实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "      <th>season_1</th>\n",
       "      <th>season_2</th>\n",
       "      <th>...</th>\n",
       "      <th>hr_21</th>\n",
       "      <th>hr_22</th>\n",
       "      <th>hr_23</th>\n",
       "      <th>weekday_0</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   yr  holiday  temp   hum  windspeed  casual  registered  cnt  season_1  \\\n",
       "0   0        0  0.24  0.81        0.0       3          13   16         1   \n",
       "1   0        0  0.22  0.80        0.0       8          32   40         1   \n",
       "2   0        0  0.22  0.80        0.0       5          27   32         1   \n",
       "3   0        0  0.24  0.75        0.0       3          10   13         1   \n",
       "4   0        0  0.24  0.75        0.0       0           1    1         1   \n",
       "\n",
       "   season_2    ...      hr_21  hr_22  hr_23  weekday_0  weekday_1  weekday_2  \\\n",
       "0         0    ...          0      0      0          0          0          0   \n",
       "1         0    ...          0      0      0          0          0          0   \n",
       "2         0    ...          0      0      0          0          0          0   \n",
       "3         0    ...          0      0      0          0          0          0   \n",
       "4         0    ...          0      0      0          0          0          0   \n",
       "\n",
       "   weekday_3  weekday_4  weekday_5  weekday_6  \n",
       "0          0          0          0          1  \n",
       "1          0          0          0          1  \n",
       "2          0          0          0          1  \n",
       "3          0          0          0          1  \n",
       "4          0          0          0          1  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\n",
    "for each in dummy_fields:\n",
    "    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
    "    rides = pd.concat([rides, dummies], axis=1)\n",
    "\n",
    "fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', \n",
    "                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']\n",
    "data = rides.drop(fields_to_drop, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调整目标变量\n",
    "\n",
    "为了更轻松地训练网络，我们将对每个连续变量标准化，即转换和调整变量，使它们的均值为 0，标准差为 1。\n",
    "\n",
    "我们会保存换算因子，以便当我们使用网络进行预测时可以还原数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
    "# Store scalings in a dictionary so we can convert back later\n",
    "scaled_features = {}\n",
    "for each in quant_features:\n",
    "    mean, std = data[each].mean(), data[each].std()\n",
    "    scaled_features[each] = [mean, std]\n",
    "    data.loc[:, each] = (data[each] - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据拆分为训练、测试和验证数据集\n",
    "\n",
    "我们将大约最后 21 天的数据保存为测试数据集，这些数据集会在训练完网络后使用。我们将使用该数据集进行预测，并与实际的骑行人数进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save data for approximately the last 21 days \n",
    "test_data = data[-21*24:]\n",
    "\n",
    "# Now remove the test data from the data set \n",
    "data = data[:-21*24]\n",
    "\n",
    "# Separate the data into features and targets\n",
    "target_fields = ['cnt', 'casual', 'registered']\n",
    "features, targets = data.drop(target_fields, axis=1), data[target_fields]\n",
    "test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将数据拆分为两个数据集，一个用作训练，一个在网络训练完后用来验证网络。因为数据是有时间序列特性的，所以我们用历史数据进行训练，然后尝试预测未来数据（验证数据集）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hold out the last 60 days or so of the remaining data as a validation set\n",
    "train_features, train_targets = features[:-60*24], targets[:-60*24]\n",
    "val_features, val_targets = features[-60*24:], targets[-60*24:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始构建网络\n",
    "\n",
    "下面你将构建自己的网络。我们已经构建好结构和反向传递部分。你将实现网络的前向传递部分。还需要设置超参数：学习速率、隐藏单元的数量，以及训练传递数量。\n",
    "\n",
    "<img src=\"assets/neural_network.png\" width=300px>\n",
    "\n",
    "该网络有两个层级，一个隐藏层和一个输出层。隐藏层级将使用 S 型函数作为激活函数。输出层只有一个节点，用于递归，节点的输出和节点的输入相同。即激活函数是 $f(x)=x$。这种函数获得输入信号，并生成输出信号，但是会考虑阈值，称为激活函数。我们完成网络的每个层级，并计算每个神经元的输出。一个层级的所有输出变成下一层级神经元的输入。这一流程叫做前向传播（forward propagation）。\n",
    "\n",
    "我们在神经网络中使用权重将信号从输入层传播到输出层。我们还使用权重将错误从输出层传播回网络，以便更新权重。这叫做反向传播（backpropagation）。\n",
    "\n",
    "> **提示**：你需要为反向传播实现计算输出激活函数 ($f(x) = x$) 的导数。如果你不熟悉微积分，其实该函数就等同于等式 $y = x$。该等式的斜率是多少？也就是导数 $f(x)$。\n",
    "\n",
    "\n",
    "你需要完成以下任务：\n",
    "\n",
    "1. 实现 S 型激活函数。将 `__init__` 中的 `self.activation_function`  设为你的 S 型函数。\n",
    "2. 在 `train` 方法中实现前向传递。\n",
    "3. 在 `train` 方法中实现反向传播算法，包括计算输出错误。\n",
    "4. 在 `run` 方法中实现前向传递。\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes**-0.5, \n",
    "                                       (self.input_nodes, self.hidden_nodes))\n",
    "\n",
    "        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
    "                                       (self.hidden_nodes, self.output_nodes))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        #### TODO: Set self.activation_function to your implemented sigmoid function ####\n",
    "        #\n",
    "        # Note: in Python, you can define a function with a lambda expression,\n",
    "        # as shown below.\n",
    "        self.activation_function = lambda x : ( 1 / (1+np.exp(-x)) )  # Replace 0 with your sigmoid calculation.\n",
    "        \n",
    "        ### If the lambda code above is not something you're familiar with,\n",
    "        # You can uncomment out the following three lines and put your \n",
    "        # implementation there instead.\n",
    "        #\n",
    "        #def sigmoid(x):\n",
    "        #    return 0  # Replace 0 with your sigmoid calculation here\n",
    "        #self.activation_function = sigmoid\n",
    "                    \n",
    "    \n",
    "    def train(self, features, targets):\n",
    "        ''' Train the network on batch of features and targets. \n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            \n",
    "            features: 2D array, each row is one data record, each column is a feature\n",
    "            targets: 1D array of target values\n",
    "        \n",
    "        '''\n",
    "        n_records = features.shape[0]\n",
    "        delta_weights_i_h = np.zeros(self.weights_input_to_hidden.shape)\n",
    "        delta_weights_h_o = np.zeros(self.weights_hidden_to_output.shape)\n",
    "        for X, y in zip(features, targets):\n",
    "            #### Implement the forward pass here ####\n",
    "            ### Forward pass ###\n",
    "            # TODO: Hidden layer - Replace these values with your calculations.\n",
    "            hidden_inputs =  np.dot(X, self.weights_input_to_hidden)  # signals into hidden layer\n",
    "            hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
    "\n",
    "            # TODO: Output layer - Replace these values with your calculations.\n",
    "            final_inputs =np.dot(hidden_outputs, self.weights_hidden_to_output) # signals into final output layer\n",
    "            final_outputs = final_inputs # signals from final output layer\n",
    "            \n",
    "            #### Implement the backward pass here ####\n",
    "            ### Backward pass ###\n",
    "\n",
    "            # TODO: Output error - Replace this value with your calculations.\n",
    "            error =  y - final_outputs  # Output layer error is the difference between desired target and actual output.\n",
    "            output_error_term = error\n",
    "            # TODO: Calculate the hidden layer's contribution to the error\n",
    "            hidden_error =output_error_term * self.weights_hidden_to_output\n",
    "            \n",
    "            # TODO: Backpropagated error terms - Replace these values with your calculations.\n",
    "         \n",
    "            hidden_error_term = hidden_error * hidden_outputs[:, None] * (1 - hidden_outputs[:, None])\n",
    "            # Weight step (input to hidden)\n",
    "            delta_weights_i_h +=  (hidden_error_term * X[:, None].T).T\n",
    "            # Weight step (hidden to output)\n",
    "            delta_weights_h_o +=(output_error_term * hidden_outputs[:, None].T).T\n",
    "\n",
    "\n",
    "        # TODO: Update the weights - Replace these values with your calculations.\n",
    "        self.weights_hidden_to_output +=self.lr * delta_weights_h_o / n_records# update hidden-to-output weights with gradient descent step\n",
    "        self.weights_input_to_hidden += self.lr * delta_weights_i_h / n_records# update input-to-hidden weights with gradient descent step\n",
    " \n",
    "    def run(self, features):\n",
    "        ''' Run a forward pass through the network with input features \n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            features: 1D array of feature values\n",
    "        '''\n",
    "        \n",
    "        #### Implement the forward pass here ####\n",
    "        # TODO: Hidden layer - replace these values with the appropriate calculations.\n",
    "        hidden_inputs = np.dot(features, self.weights_input_to_hidden)    # signals into hidden layer\n",
    "        hidden_outputs =self.activation_function(hidden_inputs)    # signals from hidden layer\n",
    "        \n",
    "        # TODO: Output layer - Replace these values with the appropriate calculations.\n",
    "        final_inputs =np.dot(hidden_outputs, self.weights_hidden_to_output)  # signals into final output layer\n",
    "        final_outputs = final_inputs  # signals from final output layer \n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE(y, Y):\n",
    "    return np.mean((y-Y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单元测试\n",
    "\n",
    "运行这些单元测试，检查你的网络实现是否正确。这样可以帮助你确保网络已正确实现，然后再开始训练网络。这些测试必须成功才能通过此项目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.009s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=5 errors=0 failures=0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "inputs = np.array([[0.5, -0.2, 0.1]])\n",
    "targets = np.array([[0.4]])\n",
    "test_w_i_h = np.array([[0.1, -0.2],\n",
    "                       [0.4, 0.5],\n",
    "                       [-0.3, 0.2]])\n",
    "test_w_h_o = np.array([[0.3],\n",
    "                       [-0.1]])\n",
    "\n",
    "class TestMethods(unittest.TestCase):\n",
    "    \n",
    "    ##########\n",
    "    # Unit tests for data loading\n",
    "    ##########\n",
    "    \n",
    "    def test_data_path(self):\n",
    "        # Test that file path to dataset has been unaltered\n",
    "        self.assertTrue(data_path.lower() == 'bike-sharing-dataset/hour.csv')\n",
    "        \n",
    "    def test_data_loaded(self):\n",
    "        # Test that data frame loaded\n",
    "        self.assertTrue(isinstance(rides, pd.DataFrame))\n",
    "    \n",
    "    ##########\n",
    "    # Unit tests for network functionality\n",
    "    ##########\n",
    "\n",
    "    def test_activation(self):\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        # Test that the activation function is a sigmoid\n",
    "        self.assertTrue(np.all(network.activation_function(0.5) == 1/(1+np.exp(-0.5))))\n",
    "\n",
    "    def test_train(self):\n",
    "        # Test that weights are updated correctly on training\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        network.weights_input_to_hidden = test_w_i_h.copy()\n",
    "        network.weights_hidden_to_output = test_w_h_o.copy()\n",
    "        \n",
    "        network.train(inputs, targets)\n",
    "        self.assertTrue(np.allclose(network.weights_hidden_to_output, \n",
    "                                    np.array([[ 0.37275328], \n",
    "                                              [-0.03172939]])))\n",
    "        self.assertTrue(np.allclose(network.weights_input_to_hidden,\n",
    "                                    np.array([[ 0.10562014, -0.20185996], \n",
    "                                              [0.39775194, 0.50074398], \n",
    "                                              [-0.29887597, 0.19962801]])))\n",
    "\n",
    "    def test_run(self):\n",
    "        # Test correctness of run method\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        network.weights_input_to_hidden = test_w_i_h.copy()\n",
    "        network.weights_hidden_to_output = test_w_h_o.copy()\n",
    "\n",
    "        self.assertTrue(np.allclose(network.run(inputs), 0.09998924))\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestMethods())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练网络\n",
    "\n",
    "现在你将设置网络的超参数。策略是设置的超参数使训练集上的错误很小但是数据不会过拟合。如果网络训练时间太长，或者有太多的隐藏节点，可能就会过于针对特定训练集，无法泛化到验证数据集。即当训练集的损失降低时，验证集的损失将开始增大。\n",
    "\n",
    "你还将采用随机梯度下降 (SGD) 方法训练网络。对于每次训练，都获取随机样本数据，而不是整个数据集。与普通梯度下降相比，训练次数要更多，但是每次时间更短。这样的话，网络训练效率更高。稍后你将详细了解 SGD。\n",
    "\n",
    "\n",
    "### 选择迭代次数\n",
    "\n",
    "也就是训练网络时从训练数据中抽样的批次数量。迭代次数越多，模型就与数据越拟合。但是，如果迭代次数太多，模型就无法很好地泛化到其他数据，这叫做过拟合。你需要选择一个使训练损失很低并且验证损失保持中等水平的数字。当你开始过拟合时，你会发现训练损失继续下降，但是验证损失开始上升。\n",
    "\n",
    "### 选择学习速率\n",
    "\n",
    "速率可以调整权重更新幅度。如果速率太大，权重就会太大，导致网络无法与数据相拟合。建议从 0.1 开始。如果网络在与数据拟合时遇到问题，尝试降低学习速率。注意，学习速率越低，权重更新的步长就越小，神经网络收敛的时间就越长。\n",
    "\n",
    "\n",
    "### 选择隐藏节点数量\n",
    "\n",
    "隐藏节点越多，模型的预测结果就越准确。尝试不同的隐藏节点的数量，看看对性能有何影响。你可以查看损失字典，寻找网络性能指标。如果隐藏单元的数量太少，那么模型就没有足够的空间进行学习，如果太多，则学习方向就有太多的选择。选择隐藏单元数量的技巧在于找到合适的平衡点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress: 0.0% ... Training loss: 1.542 ... Validation loss: 2.471\r",
      "Progress: 0.0% ... Training loss: 1.320 ... Validation loss: 1.320\r",
      "Progress: 0.0% ... Training loss: 1.042 ... Validation loss: 1.684\r",
      "Progress: 0.0% ... Training loss: 0.970 ... Validation loss: 1.512\r",
      "Progress: 0.1% ... Training loss: 0.956 ... Validation loss: 1.510\r",
      "Progress: 0.1% ... Training loss: 0.921 ... Validation loss: 1.336\r",
      "Progress: 0.1% ... Training loss: 0.941 ... Validation loss: 1.551\r",
      "Progress: 0.1% ... Training loss: 0.922 ... Validation loss: 1.260\r",
      "Progress: 0.1% ... Training loss: 0.878 ... Validation loss: 1.390\r",
      "Progress: 0.1% ... Training loss: 0.861 ... Validation loss: 1.374\r",
      "Progress: 0.1% ... Training loss: 0.939 ... Validation loss: 1.669\r",
      "Progress: 0.1% ... Training loss: 1.037 ... Validation loss: 1.176\r",
      "Progress: 0.1% ... Training loss: 0.864 ... Validation loss: 1.550\r",
      "Progress: 0.2% ... Training loss: 1.004 ... Validation loss: 1.147\r",
      "Progress: 0.2% ... Training loss: 1.167 ... Validation loss: 2.179\r",
      "Progress: 0.2% ... Training loss: 1.421 ... Validation loss: 1.254\r",
      "Progress: 0.2% ... Training loss: 0.949 ... Validation loss: 1.856\r",
      "Progress: 0.2% ... Training loss: 0.881 ... Validation loss: 1.104\r",
      "Progress: 0.2% ... Training loss: 0.839 ... Validation loss: 1.650\r",
      "Progress: 0.2% ... Training loss: 0.787 ... Validation loss: 1.112\r",
      "Progress: 0.2% ... Training loss: 0.713 ... Validation loss: 1.312\r",
      "Progress: 0.3% ... Training loss: 0.722 ... Validation loss: 1.147\r",
      "Progress: 0.3% ... Training loss: 0.718 ... Validation loss: 1.425\r",
      "Progress: 0.3% ... Training loss: 0.795 ... Validation loss: 1.049\r",
      "Progress: 0.3% ... Training loss: 0.908 ... Validation loss: 1.889\r",
      "Progress: 0.3% ... Training loss: 0.985 ... Validation loss: 1.021\r",
      "Progress: 0.3% ... Training loss: 0.965 ... Validation loss: 2.000\r",
      "Progress: 0.3% ... Training loss: 0.865 ... Validation loss: 1.005\r",
      "Progress: 0.3% ... Training loss: 0.797 ... Validation loss: 1.658\r",
      "Progress: 0.4% ... Training loss: 0.736 ... Validation loss: 1.032\r",
      "Progress: 0.4% ... Training loss: 0.719 ... Validation loss: 1.466\r",
      "Progress: 0.4% ... Training loss: 0.671 ... Validation loss: 1.088\r",
      "Progress: 0.4% ... Training loss: 0.730 ... Validation loss: 1.481\r",
      "Progress: 0.4% ... Training loss: 0.720 ... Validation loss: 1.006\r",
      "Progress: 0.4% ... Training loss: 0.659 ... Validation loss: 1.258\r",
      "Progress: 0.4% ... Training loss: 0.649 ... Validation loss: 1.207\r",
      "Progress: 0.5% ... Training loss: 0.640 ... Validation loss: 1.105\r",
      "Progress: 0.5% ... Training loss: 0.637 ... Validation loss: 1.087\r",
      "Progress: 0.5% ... Training loss: 0.631 ... Validation loss: 1.121\r",
      "Progress: 0.5% ... Training loss: 0.645 ... Validation loss: 1.027\r",
      "Progress: 0.5% ... Training loss: 0.635 ... Validation loss: 1.188\r",
      "Progress: 0.5% ... Training loss: 0.624 ... Validation loss: 1.129\r",
      "Progress: 0.5% ... Training loss: 0.628 ... Validation loss: 1.164\r",
      "Progress: 0.5% ... Training loss: 0.650 ... Validation loss: 0.982\r",
      "Progress: 0.6% ... Training loss: 0.632 ... Validation loss: 1.196\r",
      "Progress: 0.6% ... Training loss: 0.618 ... Validation loss: 1.013\r",
      "Progress: 0.6% ... Training loss: 0.622 ... Validation loss: 0.991\r",
      "Progress: 0.6% ... Training loss: 0.627 ... Validation loss: 1.185\r",
      "Progress: 0.6% ... Training loss: 0.622 ... Validation loss: 0.967\r",
      "Progress: 0.6% ... Training loss: 0.632 ... Validation loss: 1.196\r",
      "Progress: 0.6% ... Training loss: 0.594 ... Validation loss: 1.044\r",
      "Progress: 0.6% ... Training loss: 0.608 ... Validation loss: 0.950\r",
      "Progress: 0.7% ... Training loss: 0.606 ... Validation loss: 1.114\r",
      "Progress: 0.7% ... Training loss: 0.623 ... Validation loss: 0.922\r",
      "Progress: 0.7% ... Training loss: 0.582 ... Validation loss: 1.030\r",
      "Progress: 0.7% ... Training loss: 0.576 ... Validation loss: 0.980\r",
      "Progress: 0.7% ... Training loss: 0.572 ... Validation loss: 0.966\r",
      "Progress: 0.7% ... Training loss: 0.601 ... Validation loss: 1.087\r",
      "Progress: 0.7% ... Training loss: 0.567 ... Validation loss: 0.944\r",
      "Progress: 0.7% ... Training loss: 0.578 ... Validation loss: 0.903\r",
      "Progress: 0.8% ... Training loss: 0.558 ... Validation loss: 0.935\r",
      "Progress: 0.8% ... Training loss: 0.576 ... Validation loss: 1.022\r",
      "Progress: 0.8% ... Training loss: 0.555 ... Validation loss: 0.956\r",
      "Progress: 0.8% ... Training loss: 0.568 ... Validation loss: 0.876\r",
      "Progress: 0.8% ... Training loss: 0.549 ... Validation loss: 0.944\r",
      "Progress: 0.8% ... Training loss: 0.583 ... Validation loss: 0.854\r",
      "Progress: 0.8% ... Training loss: 0.673 ... Validation loss: 1.178\r",
      "Progress: 0.8% ... Training loss: 0.781 ... Validation loss: 0.918\r",
      "Progress: 0.8% ... Training loss: 0.741 ... Validation loss: 1.264\r",
      "Progress: 0.9% ... Training loss: 0.681 ... Validation loss: 0.867\r",
      "Progress: 0.9% ... Training loss: 0.623 ... Validation loss: 1.078\r",
      "Progress: 0.9% ... Training loss: 0.613 ... Validation loss: 0.841\r",
      "Progress: 0.9% ... Training loss: 0.545 ... Validation loss: 0.941\r",
      "Progress: 0.9% ... Training loss: 0.518 ... Validation loss: 0.842\r",
      "Progress: 0.9% ... Training loss: 0.514 ... Validation loss: 0.868\r",
      "Progress: 0.9% ... Training loss: 0.515 ... Validation loss: 0.827\r",
      "Progress: 0.9% ... Training loss: 0.509 ... Validation loss: 0.860\r",
      "Progress: 1.0% ... Training loss: 0.524 ... Validation loss: 0.808\r",
      "Progress: 1.0% ... Training loss: 0.502 ... Validation loss: 0.813\r",
      "Progress: 1.0% ... Training loss: 0.501 ... Validation loss: 0.842\r",
      "Progress: 1.0% ... Training loss: 0.518 ... Validation loss: 0.794\r",
      "Progress: 1.0% ... Training loss: 0.493 ... Validation loss: 0.829\r",
      "Progress: 1.0% ... Training loss: 0.544 ... Validation loss: 0.801\r",
      "Progress: 1.0% ... Training loss: 0.481 ... Validation loss: 0.795\r",
      "Progress: 1.1% ... Training loss: 0.479 ... Validation loss: 0.783\r",
      "Progress: 1.1% ... Training loss: 0.500 ... Validation loss: 0.841\r",
      "Progress: 1.1% ... Training loss: 0.506 ... Validation loss: 0.781\r",
      "Progress: 1.1% ... Training loss: 0.469 ... Validation loss: 0.774\r",
      "Progress: 1.1% ... Training loss: 0.472 ... Validation loss: 0.767\r",
      "Progress: 1.1% ... Training loss: 0.481 ... Validation loss: 0.766\r",
      "Progress: 1.1% ... Training loss: 0.472 ... Validation loss: 0.782\r",
      "Progress: 1.1% ... Training loss: 0.468 ... Validation loss: 0.779\r",
      "Progress: 1.1% ... Training loss: 0.481 ... Validation loss: 0.767\r",
      "Progress: 1.2% ... Training loss: 0.477 ... Validation loss: 0.785\r",
      "Progress: 1.2% ... Training loss: 0.450 ... Validation loss: 0.748\r",
      "Progress: 1.2% ... Training loss: 0.449 ... Validation loss: 0.735\r",
      "Progress: 1.2% ... Training loss: 0.450 ... Validation loss: 0.746\r",
      "Progress: 1.2% ... Training loss: 0.453 ... Validation loss: 0.733\r",
      "Progress: 1.2% ... Training loss: 0.439 ... Validation loss: 0.733\r",
      "Progress: 1.2% ... Training loss: 0.437 ... Validation loss: 0.730\r",
      "Progress: 1.2% ... Training loss: 0.440 ... Validation loss: 0.729\r",
      "Progress: 1.3% ... Training loss: 0.429 ... Validation loss: 0.714\r",
      "Progress: 1.3% ... Training loss: 0.429 ... Validation loss: 0.709\r",
      "Progress: 1.3% ... Training loss: 0.423 ... Validation loss: 0.696\r",
      "Progress: 1.3% ... Training loss: 0.422 ... Validation loss: 0.689\r",
      "Progress: 1.3% ... Training loss: 0.436 ... Validation loss: 0.681\r",
      "Progress: 1.3% ... Training loss: 0.414 ... Validation loss: 0.687\r",
      "Progress: 1.3% ... Training loss: 0.422 ... Validation loss: 0.690\r",
      "Progress: 1.4% ... Training loss: 0.408 ... Validation loss: 0.675\r",
      "Progress: 1.4% ... Training loss: 0.405 ... Validation loss: 0.673\r",
      "Progress: 1.4% ... Training loss: 0.419 ... Validation loss: 0.686\r",
      "Progress: 1.4% ... Training loss: 0.420 ... Validation loss: 0.682\r",
      "Progress: 1.4% ... Training loss: 0.399 ... Validation loss: 0.662\r",
      "Progress: 1.4% ... Training loss: 0.404 ... Validation loss: 0.654\r",
      "Progress: 1.4% ... Training loss: 0.410 ... Validation loss: 0.673\r",
      "Progress: 1.4% ... Training loss: 0.414 ... Validation loss: 0.663\r",
      "Progress: 1.4% ... Training loss: 0.430 ... Validation loss: 0.688\r",
      "Progress: 1.5% ... Training loss: 0.384 ... Validation loss: 0.646\r",
      "Progress: 1.5% ... Training loss: 0.383 ... Validation loss: 0.646\r",
      "Progress: 1.5% ... Training loss: 0.379 ... Validation loss: 0.635\r",
      "Progress: 1.5% ... Training loss: 0.378 ... Validation loss: 0.638\r",
      "Progress: 1.5% ... Training loss: 0.391 ... Validation loss: 0.642\r",
      "Progress: 1.5% ... Training loss: 0.475 ... Validation loss: 0.725\r",
      "Progress: 1.5% ... Training loss: 0.480 ... Validation loss: 0.720\r",
      "Progress: 1.6% ... Training loss: 0.459 ... Validation loss: 0.709\r",
      "Progress: 1.6% ... Training loss: 0.485 ... Validation loss: 0.721\r",
      "Progress: 1.6% ... Training loss: 0.531 ... Validation loss: 0.748\r",
      "Progress: 1.6% ... Training loss: 0.694 ... Validation loss: 0.922\r",
      "Progress: 1.6% ... Training loss: 0.878 ... Validation loss: 1.024\r",
      "Progress: 1.6% ... Training loss: 0.859 ... Validation loss: 1.070\r",
      "Progress: 1.6% ... Training loss: 0.581 ... Validation loss: 0.807\r",
      "Progress: 1.6% ... Training loss: 0.386 ... Validation loss: 0.632\r",
      "Progress: 1.6% ... Training loss: 0.364 ... Validation loss: 0.603\r",
      "Progress: 1.7% ... Training loss: 0.363 ... Validation loss: 0.592\r",
      "Progress: 1.7% ... Training loss: 0.358 ... Validation loss: 0.593\r",
      "Progress: 1.7% ... Training loss: 0.367 ... Validation loss: 0.595\r",
      "Progress: 1.7% ... Training loss: 0.359 ... Validation loss: 0.579\r",
      "Progress: 1.7% ... Training loss: 0.362 ... Validation loss: 0.579\r",
      "Progress: 1.7% ... Training loss: 0.379 ... Validation loss: 0.604\r",
      "Progress: 1.7% ... Training loss: 0.354 ... Validation loss: 0.578\r",
      "Progress: 1.8% ... Training loss: 0.378 ... Validation loss: 0.604\r",
      "Progress: 1.8% ... Training loss: 0.388 ... Validation loss: 0.618\r",
      "Progress: 1.8% ... Training loss: 0.414 ... Validation loss: 0.635\r",
      "Progress: 1.8% ... Training loss: 0.384 ... Validation loss: 0.630\r",
      "Progress: 1.8% ... Training loss: 0.466 ... Validation loss: 0.673\r",
      "Progress: 1.8% ... Training loss: 0.490 ... Validation loss: 0.706\r",
      "Progress: 1.8% ... Training loss: 0.490 ... Validation loss: 0.683\r",
      "Progress: 1.8% ... Training loss: 0.480 ... Validation loss: 0.697\r",
      "Progress: 1.9% ... Training loss: 0.527 ... Validation loss: 0.744\r",
      "Progress: 1.9% ... Training loss: 0.481 ... Validation loss: 0.685\r",
      "Progress: 1.9% ... Training loss: 0.439 ... Validation loss: 0.667\r",
      "Progress: 1.9% ... Training loss: 0.478 ... Validation loss: 0.658\r",
      "Progress: 1.9% ... Training loss: 0.373 ... Validation loss: 0.593\r",
      "Progress: 1.9% ... Training loss: 0.352 ... Validation loss: 0.562\r",
      "Progress: 1.9% ... Training loss: 0.344 ... Validation loss: 0.563\r",
      "Progress: 1.9% ... Training loss: 0.366 ... Validation loss: 0.562\r",
      "Progress: 1.9% ... Training loss: 0.335 ... Validation loss: 0.547\r",
      "Progress: 2.0% ... Training loss: 0.336 ... Validation loss: 0.536\r",
      "Progress: 2.0% ... Training loss: 0.347 ... Validation loss: 0.561\r",
      "Progress: 2.0% ... Training loss: 0.379 ... Validation loss: 0.585\r",
      "Progress: 2.0% ... Training loss: 0.392 ... Validation loss: 0.614\r",
      "Progress: 2.0% ... Training loss: 0.393 ... Validation loss: 0.598\r",
      "Progress: 2.0% ... Training loss: 0.332 ... Validation loss: 0.543\r",
      "Progress: 2.0% ... Training loss: 0.335 ... Validation loss: 0.538\r",
      "Progress: 2.0% ... Training loss: 0.329 ... Validation loss: 0.536\r",
      "Progress: 2.1% ... Training loss: 0.326 ... Validation loss: 0.535\r",
      "Progress: 2.1% ... Training loss: 0.326 ... Validation loss: 0.533\r",
      "Progress: 2.1% ... Training loss: 0.356 ... Validation loss: 0.563\r",
      "Progress: 2.1% ... Training loss: 0.398 ... Validation loss: 0.597\r",
      "Progress: 2.1% ... Training loss: 0.347 ... Validation loss: 0.544\r",
      "Progress: 2.1% ... Training loss: 0.370 ... Validation loss: 0.571\r",
      "Progress: 2.1% ... Training loss: 0.349 ... Validation loss: 0.556\r",
      "Progress: 2.1% ... Training loss: 0.449 ... Validation loss: 0.648\r",
      "Progress: 2.2% ... Training loss: 0.390 ... Validation loss: 0.596\r",
      "Progress: 2.2% ... Training loss: 0.410 ... Validation loss: 0.614\r",
      "Progress: 2.2% ... Training loss: 0.465 ... Validation loss: 0.624\r",
      "Progress: 2.2% ... Training loss: 0.354 ... Validation loss: 0.554\r",
      "Progress: 2.2% ... Training loss: 0.435 ... Validation loss: 0.579\r",
      "Progress: 2.2% ... Training loss: 0.498 ... Validation loss: 0.715\r",
      "Progress: 2.2% ... Training loss: 0.680 ... Validation loss: 0.809\r",
      "Progress: 2.2% ... Training loss: 0.497 ... Validation loss: 0.703\r",
      "Progress: 2.3% ... Training loss: 0.466 ... Validation loss: 0.637\r",
      "Progress: 2.3% ... Training loss: 0.449 ... Validation loss: 0.656\r",
      "Progress: 2.3% ... Training loss: 0.417 ... Validation loss: 0.562\r",
      "Progress: 2.3% ... Training loss: 0.485 ... Validation loss: 0.713\r",
      "Progress: 2.3% ... Training loss: 0.478 ... Validation loss: 0.620\r",
      "Progress: 2.3% ... Training loss: 0.413 ... Validation loss: 0.631\r",
      "Progress: 2.3% ... Training loss: 0.431 ... Validation loss: 0.560\r",
      "Progress: 2.4% ... Training loss: 0.401 ... Validation loss: 0.643\r",
      "Progress: 2.4% ... Training loss: 0.353 ... Validation loss: 0.516\r",
      "Progress: 2.4% ... Training loss: 0.344 ... Validation loss: 0.566\r",
      "Progress: 2.4% ... Training loss: 0.369 ... Validation loss: 0.513\r",
      "Progress: 2.4% ... Training loss: 0.467 ... Validation loss: 0.707\r",
      "Progress: 2.4% ... Training loss: 0.479 ... Validation loss: 0.606\r",
      "Progress: 2.4% ... Training loss: 0.464 ... Validation loss: 0.723\r",
      "Progress: 2.4% ... Training loss: 0.380 ... Validation loss: 0.538\r",
      "Progress: 2.5% ... Training loss: 0.457 ... Validation loss: 0.700\r",
      "Progress: 2.5% ... Training loss: 0.443 ... Validation loss: 0.586\r",
      "Progress: 2.5% ... Training loss: 0.396 ... Validation loss: 0.624\r",
      "Progress: 2.5% ... Training loss: 0.350 ... Validation loss: 0.536\r",
      "Progress: 2.5% ... Training loss: 0.359 ... Validation loss: 0.567\r",
      "Progress: 2.5% ... Training loss: 0.316 ... Validation loss: 0.520\r",
      "Progress: 2.5% ... Training loss: 0.335 ... Validation loss: 0.518\r",
      "Progress: 2.5% ... Training loss: 0.339 ... Validation loss: 0.550\r",
      "Progress: 2.5% ... Training loss: 0.318 ... Validation loss: 0.500\r",
      "Progress: 2.6% ... Training loss: 0.322 ... Validation loss: 0.514\r",
      "Progress: 2.6% ... Training loss: 0.319 ... Validation loss: 0.494\r",
      "Progress: 2.6% ... Training loss: 0.312 ... Validation loss: 0.497\r",
      "Progress: 2.6% ... Training loss: 0.316 ... Validation loss: 0.511\r",
      "Progress: 2.6% ... Training loss: 0.324 ... Validation loss: 0.505\r",
      "Progress: 2.6% ... Training loss: 0.311 ... Validation loss: 0.501\r",
      "Progress: 2.6% ... Training loss: 0.311 ... Validation loss: 0.507\r",
      "Progress: 2.6% ... Training loss: 0.316 ... Validation loss: 0.505\r",
      "Progress: 2.7% ... Training loss: 0.314 ... Validation loss: 0.495\r",
      "Progress: 2.7% ... Training loss: 0.351 ... Validation loss: 0.529\r",
      "Progress: 2.7% ... Training loss: 0.344 ... Validation loss: 0.535\r",
      "Progress: 2.7% ... Training loss: 0.380 ... Validation loss: 0.561\r",
      "Progress: 2.7% ... Training loss: 0.340 ... Validation loss: 0.531\r",
      "Progress: 2.7% ... Training loss: 0.315 ... Validation loss: 0.512\r",
      "Progress: 2.7% ... Training loss: 0.308 ... Validation loss: 0.495\r",
      "Progress: 2.8% ... Training loss: 0.317 ... Validation loss: 0.512\r",
      "Progress: 2.8% ... Training loss: 0.319 ... Validation loss: 0.515\r",
      "Progress: 2.8% ... Training loss: 0.315 ... Validation loss: 0.503\r",
      "Progress: 2.8% ... Training loss: 0.326 ... Validation loss: 0.517\r",
      "Progress: 2.8% ... Training loss: 0.307 ... Validation loss: 0.497\r",
      "Progress: 2.8% ... Training loss: 0.312 ... Validation loss: 0.486\r",
      "Progress: 2.8% ... Training loss: 0.312 ... Validation loss: 0.501\r",
      "Progress: 2.8% ... Training loss: 0.312 ... Validation loss: 0.500\r",
      "Progress: 2.9% ... Training loss: 0.310 ... Validation loss: 0.496\r",
      "Progress: 2.9% ... Training loss: 0.307 ... Validation loss: 0.494\r",
      "Progress: 2.9% ... Training loss: 0.497 ... Validation loss: 0.643\r",
      "Progress: 2.9% ... Training loss: 0.427 ... Validation loss: 0.600\r",
      "Progress: 2.9% ... Training loss: 0.530 ... Validation loss: 0.720\r",
      "Progress: 2.9% ... Training loss: 0.404 ... Validation loss: 0.587\r",
      "Progress: 2.9% ... Training loss: 0.318 ... Validation loss: 0.515\r",
      "Progress: 2.9% ... Training loss: 0.366 ... Validation loss: 0.551\r",
      "Progress: 3.0% ... Training loss: 0.322 ... Validation loss: 0.518\r",
      "Progress: 3.0% ... Training loss: 0.312 ... Validation loss: 0.501\r",
      "Progress: 3.0% ... Training loss: 0.363 ... Validation loss: 0.514\r",
      "Progress: 3.0% ... Training loss: 0.397 ... Validation loss: 0.568\r",
      "Progress: 3.0% ... Training loss: 0.476 ... Validation loss: 0.613\r",
      "Progress: 3.0% ... Training loss: 0.363 ... Validation loss: 0.536\r",
      "Progress: 3.0% ... Training loss: 0.332 ... Validation loss: 0.513\r",
      "Progress: 3.0% ... Training loss: 0.313 ... Validation loss: 0.493\r",
      "Progress: 3.0% ... Training loss: 0.319 ... Validation loss: 0.491\r",
      "Progress: 3.1% ... Training loss: 0.315 ... Validation loss: 0.496\r",
      "Progress: 3.1% ... Training loss: 0.340 ... Validation loss: 0.514\r",
      "Progress: 3.1% ... Training loss: 0.382 ... Validation loss: 0.563\r",
      "Progress: 3.1% ... Training loss: 0.408 ... Validation loss: 0.595\r",
      "Progress: 3.1% ... Training loss: 0.335 ... Validation loss: 0.513\r",
      "Progress: 3.1% ... Training loss: 0.384 ... Validation loss: 0.509\r",
      "Progress: 3.1% ... Training loss: 0.324 ... Validation loss: 0.501\r",
      "Progress: 3.1% ... Training loss: 0.315 ... Validation loss: 0.473\r",
      "Progress: 3.2% ... Training loss: 0.333 ... Validation loss: 0.513\r",
      "Progress: 3.2% ... Training loss: 0.302 ... Validation loss: 0.481\r",
      "Progress: 3.2% ... Training loss: 0.316 ... Validation loss: 0.472\r",
      "Progress: 3.2% ... Training loss: 0.329 ... Validation loss: 0.516\r",
      "Progress: 3.2% ... Training loss: 0.334 ... Validation loss: 0.492\r",
      "Progress: 3.2% ... Training loss: 0.310 ... Validation loss: 0.486\r",
      "Progress: 3.2% ... Training loss: 0.301 ... Validation loss: 0.480\r",
      "Progress: 3.2% ... Training loss: 0.322 ... Validation loss: 0.495\r",
      "Progress: 3.3% ... Training loss: 0.405 ... Validation loss: 0.588\r",
      "Progress: 3.3% ... Training loss: 0.375 ... Validation loss: 0.514\r",
      "Progress: 3.3% ... Training loss: 0.336 ... Validation loss: 0.522\r",
      "Progress: 3.3% ... Training loss: 0.355 ... Validation loss: 0.513\r",
      "Progress: 3.3% ... Training loss: 0.305 ... Validation loss: 0.488\r",
      "Progress: 3.3% ... Training loss: 0.306 ... Validation loss: 0.474\r",
      "Progress: 3.3% ... Training loss: 0.301 ... Validation loss: 0.482\r",
      "Progress: 3.4% ... Training loss: 0.301 ... Validation loss: 0.476\r",
      "Progress: 3.4% ... Training loss: 0.300 ... Validation loss: 0.482\r",
      "Progress: 3.4% ... Training loss: 0.305 ... Validation loss: 0.474\r",
      "Progress: 3.4% ... Training loss: 0.369 ... Validation loss: 0.557\r",
      "Progress: 3.4% ... Training loss: 0.333 ... Validation loss: 0.474\r",
      "Progress: 3.4% ... Training loss: 0.307 ... Validation loss: 0.474\r",
      "Progress: 3.4% ... Training loss: 0.302 ... Validation loss: 0.475\r",
      "Progress: 3.4% ... Training loss: 0.299 ... Validation loss: 0.472\r",
      "Progress: 3.5% ... Training loss: 0.307 ... Validation loss: 0.479\r",
      "Progress: 3.5% ... Training loss: 0.329 ... Validation loss: 0.504\r",
      "Progress: 3.5% ... Training loss: 0.316 ... Validation loss: 0.483\r",
      "Progress: 3.5% ... Training loss: 0.328 ... Validation loss: 0.503\r",
      "Progress: 3.5% ... Training loss: 0.408 ... Validation loss: 0.566\r",
      "Progress: 3.5% ... Training loss: 0.416 ... Validation loss: 0.565\r",
      "Progress: 3.5% ... Training loss: 0.345 ... Validation loss: 0.537\r",
      "Progress: 3.5% ... Training loss: 0.304 ... Validation loss: 0.482\r",
      "Progress: 3.5% ... Training loss: 0.300 ... Validation loss: 0.484\r",
      "Progress: 3.6% ... Training loss: 0.300 ... Validation loss: 0.465\r",
      "Progress: 3.6% ... Training loss: 0.306 ... Validation loss: 0.476\r",
      "Progress: 3.6% ... Training loss: 0.296 ... Validation loss: 0.467\r",
      "Progress: 3.6% ... Training loss: 0.314 ... Validation loss: 0.495\r",
      "Progress: 3.6% ... Training loss: 0.309 ... Validation loss: 0.482\r",
      "Progress: 3.6% ... Training loss: 0.298 ... Validation loss: 0.482\r",
      "Progress: 3.6% ... Training loss: 0.300 ... Validation loss: 0.474\r",
      "Progress: 3.6% ... Training loss: 0.301 ... Validation loss: 0.481\r",
      "Progress: 3.7% ... Training loss: 0.298 ... Validation loss: 0.476\r",
      "Progress: 3.7% ... Training loss: 0.302 ... Validation loss: 0.474\r",
      "Progress: 3.7% ... Training loss: 0.313 ... Validation loss: 0.475\r",
      "Progress: 3.7% ... Training loss: 0.311 ... Validation loss: 0.489\r",
      "Progress: 3.7% ... Training loss: 0.320 ... Validation loss: 0.474\r",
      "Progress: 3.7% ... Training loss: 0.320 ... Validation loss: 0.490\r",
      "Progress: 3.7% ... Training loss: 0.334 ... Validation loss: 0.466\r",
      "Progress: 3.8% ... Training loss: 0.325 ... Validation loss: 0.498\r",
      "Progress: 3.8% ... Training loss: 0.297 ... Validation loss: 0.471\r",
      "Progress: 3.8% ... Training loss: 0.304 ... Validation loss: 0.482\r",
      "Progress: 3.8% ... Training loss: 0.297 ... Validation loss: 0.478\r",
      "Progress: 3.8% ... Training loss: 0.315 ... Validation loss: 0.484\r",
      "Progress: 3.8% ... Training loss: 0.295 ... Validation loss: 0.469\r",
      "Progress: 3.8% ... Training loss: 0.306 ... Validation loss: 0.485\r",
      "Progress: 3.8% ... Training loss: 0.298 ... Validation loss: 0.480\r",
      "Progress: 3.9% ... Training loss: 0.298 ... Validation loss: 0.479\r",
      "Progress: 3.9% ... Training loss: 0.297 ... Validation loss: 0.464\r",
      "Progress: 3.9% ... Training loss: 0.300 ... Validation loss: 0.483\r",
      "Progress: 3.9% ... Training loss: 0.297 ... Validation loss: 0.479\r",
      "Progress: 3.9% ... Training loss: 0.297 ... Validation loss: 0.481\r",
      "Progress: 3.9% ... Training loss: 0.314 ... Validation loss: 0.496\r",
      "Progress: 3.9% ... Training loss: 0.311 ... Validation loss: 0.478\r",
      "Progress: 3.9% ... Training loss: 0.313 ... Validation loss: 0.488\r",
      "Progress: 4.0% ... Training loss: 0.366 ... Validation loss: 0.543\r",
      "Progress: 4.0% ... Training loss: 0.327 ... Validation loss: 0.498\r",
      "Progress: 4.0% ... Training loss: 0.300 ... Validation loss: 0.472\r",
      "Progress: 4.0% ... Training loss: 0.299 ... Validation loss: 0.466\r",
      "Progress: 4.0% ... Training loss: 0.350 ... Validation loss: 0.521\r",
      "Progress: 4.0% ... Training loss: 0.345 ... Validation loss: 0.513\r",
      "Progress: 4.0% ... Training loss: 0.319 ... Validation loss: 0.497\r",
      "Progress: 4.0% ... Training loss: 0.299 ... Validation loss: 0.465\r",
      "Progress: 4.0% ... Training loss: 0.294 ... Validation loss: 0.472\r",
      "Progress: 4.1% ... Training loss: 0.293 ... Validation loss: 0.470\r",
      "Progress: 4.1% ... Training loss: 0.295 ... Validation loss: 0.479\r",
      "Progress: 4.1% ... Training loss: 0.294 ... Validation loss: 0.477\r",
      "Progress: 4.1% ... Training loss: 0.296 ... Validation loss: 0.474\r",
      "Progress: 4.1% ... Training loss: 0.295 ... Validation loss: 0.476\r",
      "Progress: 4.1% ... Training loss: 0.311 ... Validation loss: 0.493\r",
      "Progress: 4.1% ... Training loss: 0.294 ... Validation loss: 0.476\r",
      "Progress: 4.2% ... Training loss: 0.319 ... Validation loss: 0.502\r",
      "Progress: 4.2% ... Training loss: 0.294 ... Validation loss: 0.472\r",
      "Progress: 4.2% ... Training loss: 0.293 ... Validation loss: 0.471\r",
      "Progress: 4.2% ... Training loss: 0.300 ... Validation loss: 0.484\r",
      "Progress: 4.2% ... Training loss: 0.295 ... Validation loss: 0.474\r",
      "Progress: 4.2% ... Training loss: 0.301 ... Validation loss: 0.480\r",
      "Progress: 4.2% ... Training loss: 0.318 ... Validation loss: 0.504\r",
      "Progress: 4.2% ... Training loss: 0.308 ... Validation loss: 0.484\r",
      "Progress: 4.2% ... Training loss: 0.305 ... Validation loss: 0.477\r",
      "Progress: 4.3% ... Training loss: 0.294 ... Validation loss: 0.473\r",
      "Progress: 4.3% ... Training loss: 0.311 ... Validation loss: 0.494\r",
      "Progress: 4.3% ... Training loss: 0.302 ... Validation loss: 0.470\r",
      "Progress: 4.3% ... Training loss: 0.308 ... Validation loss: 0.490\r",
      "Progress: 4.3% ... Training loss: 0.295 ... Validation loss: 0.472\r",
      "Progress: 4.3% ... Training loss: 0.330 ... Validation loss: 0.524\r",
      "Progress: 4.3% ... Training loss: 0.337 ... Validation loss: 0.491\r",
      "Progress: 4.3% ... Training loss: 0.304 ... Validation loss: 0.475\r",
      "Progress: 4.4% ... Training loss: 0.300 ... Validation loss: 0.462\r",
      "Progress: 4.4% ... Training loss: 0.294 ... Validation loss: 0.481\r",
      "Progress: 4.4% ... Training loss: 0.293 ... Validation loss: 0.467\r",
      "Progress: 4.4% ... Training loss: 0.291 ... Validation loss: 0.470\r",
      "Progress: 4.4% ... Training loss: 0.299 ... Validation loss: 0.479\r",
      "Progress: 4.4% ... Training loss: 0.380 ... Validation loss: 0.548\r",
      "Progress: 4.4% ... Training loss: 0.351 ... Validation loss: 0.503\r",
      "Progress: 4.5% ... Training loss: 0.333 ... Validation loss: 0.521\r",
      "Progress: 4.5% ... Training loss: 0.335 ... Validation loss: 0.506\r",
      "Progress: 4.5% ... Training loss: 0.317 ... Validation loss: 0.494\r",
      "Progress: 4.5% ... Training loss: 0.333 ... Validation loss: 0.506\r",
      "Progress: 4.5% ... Training loss: 0.312 ... Validation loss: 0.496\r",
      "Progress: 4.5% ... Training loss: 0.296 ... Validation loss: 0.475\r",
      "Progress: 4.5% ... Training loss: 0.294 ... Validation loss: 0.475\r",
      "Progress: 4.5% ... Training loss: 0.294 ... Validation loss: 0.479\r",
      "Progress: 4.5% ... Training loss: 0.303 ... Validation loss: 0.467\r",
      "Progress: 4.6% ... Training loss: 0.290 ... Validation loss: 0.462\r",
      "Progress: 4.6% ... Training loss: 0.339 ... Validation loss: 0.505\r",
      "Progress: 4.6% ... Training loss: 0.325 ... Validation loss: 0.490\r",
      "Progress: 4.6% ... Training loss: 0.298 ... Validation loss: 0.466\r",
      "Progress: 4.6% ... Training loss: 0.288 ... Validation loss: 0.463\r",
      "Progress: 4.6% ... Training loss: 0.296 ... Validation loss: 0.480\r",
      "Progress: 4.6% ... Training loss: 0.294 ... Validation loss: 0.474\r",
      "Progress: 4.7% ... Training loss: 0.293 ... Validation loss: 0.467\r",
      "Progress: 4.7% ... Training loss: 0.287 ... Validation loss: 0.461\r",
      "Progress: 4.7% ... Training loss: 0.290 ... Validation loss: 0.458\r",
      "Progress: 4.7% ... Training loss: 0.299 ... Validation loss: 0.472\r",
      "Progress: 4.7% ... Training loss: 0.332 ... Validation loss: 0.505\r",
      "Progress: 4.7% ... Training loss: 0.381 ... Validation loss: 0.532\r",
      "Progress: 4.7% ... Training loss: 0.393 ... Validation loss: 0.555\r",
      "Progress: 4.7% ... Training loss: 0.315 ... Validation loss: 0.487\r",
      "Progress: 4.8% ... Training loss: 0.384 ... Validation loss: 0.511\r",
      "Progress: 4.8% ... Training loss: 0.320 ... Validation loss: 0.494\r",
      "Progress: 4.8% ... Training loss: 0.292 ... Validation loss: 0.462\r",
      "Progress: 4.8% ... Training loss: 0.294 ... Validation loss: 0.454\r",
      "Progress: 4.8% ... Training loss: 0.290 ... Validation loss: 0.452\r",
      "Progress: 4.8% ... Training loss: 0.300 ... Validation loss: 0.460\r",
      "Progress: 4.8% ... Training loss: 0.312 ... Validation loss: 0.477\r",
      "Progress: 4.8% ... Training loss: 0.291 ... Validation loss: 0.481\r",
      "Progress: 4.8% ... Training loss: 0.288 ... Validation loss: 0.463\r",
      "Progress: 4.9% ... Training loss: 0.291 ... Validation loss: 0.465\r",
      "Progress: 4.9% ... Training loss: 0.336 ... Validation loss: 0.500\r",
      "Progress: 4.9% ... Training loss: 0.300 ... Validation loss: 0.465\r",
      "Progress: 4.9% ... Training loss: 0.295 ... Validation loss: 0.464\r",
      "Progress: 4.9% ... Training loss: 0.302 ... Validation loss: 0.474\r",
      "Progress: 4.9% ... Training loss: 0.290 ... Validation loss: 0.469\r",
      "Progress: 4.9% ... Training loss: 0.294 ... Validation loss: 0.477\r",
      "Progress: 5.0% ... Training loss: 0.294 ... Validation loss: 0.470\r",
      "Progress: 5.0% ... Training loss: 0.288 ... Validation loss: 0.454\r",
      "Progress: 5.0% ... Training loss: 0.285 ... Validation loss: 0.465\r",
      "Progress: 5.0% ... Training loss: 0.286 ... Validation loss: 0.459\r",
      "Progress: 5.0% ... Training loss: 0.285 ... Validation loss: 0.455\r",
      "Progress: 5.0% ... Training loss: 0.287 ... Validation loss: 0.456\r",
      "Progress: 5.0% ... Training loss: 0.286 ... Validation loss: 0.459\r",
      "Progress: 5.0% ... Training loss: 0.293 ... Validation loss: 0.463\r",
      "Progress: 5.0% ... Training loss: 0.304 ... Validation loss: 0.488\r",
      "Progress: 5.1% ... Training loss: 0.291 ... Validation loss: 0.459\r",
      "Progress: 5.1% ... Training loss: 0.299 ... Validation loss: 0.470\r",
      "Progress: 5.1% ... Training loss: 0.302 ... Validation loss: 0.467\r",
      "Progress: 5.1% ... Training loss: 0.295 ... Validation loss: 0.453\r",
      "Progress: 5.1% ... Training loss: 0.304 ... Validation loss: 0.485\r",
      "Progress: 5.1% ... Training loss: 0.285 ... Validation loss: 0.463\r",
      "Progress: 5.1% ... Training loss: 0.333 ... Validation loss: 0.481\r",
      "Progress: 5.2% ... Training loss: 0.304 ... Validation loss: 0.494\r",
      "Progress: 5.2% ... Training loss: 0.334 ... Validation loss: 0.482\r",
      "Progress: 5.2% ... Training loss: 0.324 ... Validation loss: 0.494\r",
      "Progress: 5.2% ... Training loss: 0.290 ... Validation loss: 0.455\r",
      "Progress: 5.2% ... Training loss: 0.287 ... Validation loss: 0.458\r",
      "Progress: 5.2% ... Training loss: 0.289 ... Validation loss: 0.455\r",
      "Progress: 5.2% ... Training loss: 0.289 ... Validation loss: 0.455\r",
      "Progress: 5.2% ... Training loss: 0.282 ... Validation loss: 0.455\r",
      "Progress: 5.2% ... Training loss: 0.293 ... Validation loss: 0.466\r",
      "Progress: 5.3% ... Training loss: 0.291 ... Validation loss: 0.463\r",
      "Progress: 5.3% ... Training loss: 0.286 ... Validation loss: 0.463\r",
      "Progress: 5.3% ... Training loss: 0.300 ... Validation loss: 0.472\r",
      "Progress: 5.3% ... Training loss: 0.282 ... Validation loss: 0.456\r",
      "Progress: 5.3% ... Training loss: 0.302 ... Validation loss: 0.469\r",
      "Progress: 5.3% ... Training loss: 0.294 ... Validation loss: 0.477\r",
      "Progress: 5.3% ... Training loss: 0.366 ... Validation loss: 0.518\r",
      "Progress: 5.3% ... Training loss: 0.366 ... Validation loss: 0.517\r",
      "Progress: 5.4% ... Training loss: 0.353 ... Validation loss: 0.544\r",
      "Progress: 5.4% ... Training loss: 0.303 ... Validation loss: 0.480\r",
      "Progress: 5.4% ... Training loss: 0.285 ... Validation loss: 0.461\r",
      "Progress: 5.4% ... Training loss: 0.300 ... Validation loss: 0.476\r",
      "Progress: 5.4% ... Training loss: 0.282 ... Validation loss: 0.454\r",
      "Progress: 5.4% ... Training loss: 0.290 ... Validation loss: 0.457\r",
      "Progress: 5.4% ... Training loss: 0.281 ... Validation loss: 0.453\r",
      "Progress: 5.5% ... Training loss: 0.404 ... Validation loss: 0.556\r",
      "Progress: 5.5% ... Training loss: 0.381 ... Validation loss: 0.523\r",
      "Progress: 5.5% ... Training loss: 0.327 ... Validation loss: 0.498\r",
      "Progress: 5.5% ... Training loss: 0.349 ... Validation loss: 0.502\r",
      "Progress: 5.5% ... Training loss: 0.315 ... Validation loss: 0.476\r",
      "Progress: 5.5% ... Training loss: 0.353 ... Validation loss: 0.513\r",
      "Progress: 5.5% ... Training loss: 0.296 ... Validation loss: 0.483\r",
      "Progress: 5.5% ... Training loss: 0.282 ... Validation loss: 0.455\r",
      "Progress: 5.5% ... Training loss: 0.283 ... Validation loss: 0.457\r",
      "Progress: 5.6% ... Training loss: 0.324 ... Validation loss: 0.490\r",
      "Progress: 5.6% ... Training loss: 0.410 ... Validation loss: 0.537\r",
      "Progress: 5.6% ... Training loss: 0.436 ... Validation loss: 0.579\r",
      "Progress: 5.6% ... Training loss: 0.334 ... Validation loss: 0.494\r",
      "Progress: 5.6% ... Training loss: 0.368 ... Validation loss: 0.527\r",
      "Progress: 5.6% ... Training loss: 0.504 ... Validation loss: 0.606\r",
      "Progress: 5.6% ... Training loss: 0.494 ... Validation loss: 0.651\r",
      "Progress: 5.7% ... Training loss: 0.333 ... Validation loss: 0.497\r",
      "Progress: 5.7% ... Training loss: 0.306 ... Validation loss: 0.470\r",
      "Progress: 5.7% ... Training loss: 0.288 ... Validation loss: 0.463\r",
      "Progress: 5.7% ... Training loss: 0.280 ... Validation loss: 0.451\r",
      "Progress: 5.7% ... Training loss: 0.279 ... Validation loss: 0.450\r",
      "Progress: 5.7% ... Training loss: 0.285 ... Validation loss: 0.447\r",
      "Progress: 5.7% ... Training loss: 0.278 ... Validation loss: 0.447\r",
      "Progress: 5.7% ... Training loss: 0.279 ... Validation loss: 0.451\r",
      "Progress: 5.8% ... Training loss: 0.279 ... Validation loss: 0.451\r",
      "Progress: 5.8% ... Training loss: 0.279 ... Validation loss: 0.459\r",
      "Progress: 5.8% ... Training loss: 0.281 ... Validation loss: 0.459\r",
      "Progress: 5.8% ... Training loss: 0.279 ... Validation loss: 0.453\r",
      "Progress: 5.8% ... Training loss: 0.279 ... Validation loss: 0.458\r",
      "Progress: 5.8% ... Training loss: 0.282 ... Validation loss: 0.459\r",
      "Progress: 5.8% ... Training loss: 0.282 ... Validation loss: 0.462\r",
      "Progress: 5.8% ... Training loss: 0.300 ... Validation loss: 0.478\r",
      "Progress: 5.8% ... Training loss: 0.281 ... Validation loss: 0.455\r",
      "Progress: 5.9% ... Training loss: 0.279 ... Validation loss: 0.458\r",
      "Progress: 5.9% ... Training loss: 0.285 ... Validation loss: 0.468\r",
      "Progress: 5.9% ... Training loss: 0.281 ... Validation loss: 0.445\r",
      "Progress: 5.9% ... Training loss: 0.278 ... Validation loss: 0.447\r",
      "Progress: 5.9% ... Training loss: 0.278 ... Validation loss: 0.445\r",
      "Progress: 5.9% ... Training loss: 0.286 ... Validation loss: 0.457\r",
      "Progress: 5.9% ... Training loss: 0.281 ... Validation loss: 0.454\r",
      "Progress: 6.0% ... Training loss: 0.308 ... Validation loss: 0.482\r",
      "Progress: 6.0% ... Training loss: 0.283 ... Validation loss: 0.454\r",
      "Progress: 6.0% ... Training loss: 0.287 ... Validation loss: 0.459\r",
      "Progress: 6.0% ... Training loss: 0.301 ... Validation loss: 0.459\r",
      "Progress: 6.0% ... Training loss: 0.278 ... Validation loss: 0.444\r",
      "Progress: 6.0% ... Training loss: 0.276 ... Validation loss: 0.447\r",
      "Progress: 6.0% ... Training loss: 0.286 ... Validation loss: 0.451\r",
      "Progress: 6.0% ... Training loss: 0.279 ... Validation loss: 0.462\r",
      "Progress: 6.0% ... Training loss: 0.279 ... Validation loss: 0.454\r",
      "Progress: 6.1% ... Training loss: 0.285 ... Validation loss: 0.443\r",
      "Progress: 6.1% ... Training loss: 0.279 ... Validation loss: 0.447\r",
      "Progress: 6.1% ... Training loss: 0.315 ... Validation loss: 0.495\r",
      "Progress: 6.1% ... Training loss: 0.344 ... Validation loss: 0.489\r",
      "Progress: 6.1% ... Training loss: 0.326 ... Validation loss: 0.500\r",
      "Progress: 6.1% ... Training loss: 0.329 ... Validation loss: 0.471\r",
      "Progress: 6.1% ... Training loss: 0.346 ... Validation loss: 0.520\r",
      "Progress: 6.2% ... Training loss: 0.300 ... Validation loss: 0.460\r",
      "Progress: 6.2% ... Training loss: 0.310 ... Validation loss: 0.491\r",
      "Progress: 6.2% ... Training loss: 0.336 ... Validation loss: 0.481\r",
      "Progress: 6.2% ... Training loss: 0.338 ... Validation loss: 0.519\r",
      "Progress: 6.2% ... Training loss: 0.369 ... Validation loss: 0.486\r",
      "Progress: 6.2% ... Training loss: 0.419 ... Validation loss: 0.583\r",
      "Progress: 6.2% ... Training loss: 0.329 ... Validation loss: 0.484\r",
      "Progress: 6.2% ... Training loss: 0.310 ... Validation loss: 0.504\r",
      "Progress: 6.2% ... Training loss: 0.281 ... Validation loss: 0.454\r",
      "Progress: 6.3% ... Training loss: 0.287 ... Validation loss: 0.483\r",
      "Progress: 6.3% ... Training loss: 0.280 ... Validation loss: 0.449\r",
      "Progress: 6.3% ... Training loss: 0.280 ... Validation loss: 0.469\r",
      "Progress: 6.3% ... Training loss: 0.314 ... Validation loss: 0.460\r",
      "Progress: 6.3% ... Training loss: 0.338 ... Validation loss: 0.516\r",
      "Progress: 6.3% ... Training loss: 0.317 ... Validation loss: 0.488\r",
      "Progress: 6.3% ... Training loss: 0.280 ... Validation loss: 0.467\r",
      "Progress: 6.3% ... Training loss: 0.286 ... Validation loss: 0.448\r",
      "Progress: 6.4% ... Training loss: 0.275 ... Validation loss: 0.449\r",
      "Progress: 6.4% ... Training loss: 0.276 ... Validation loss: 0.462\r",
      "Progress: 6.4% ... Training loss: 0.278 ... Validation loss: 0.466\r",
      "Progress: 6.4% ... Training loss: 0.276 ... Validation loss: 0.452\r",
      "Progress: 6.4% ... Training loss: 0.281 ... Validation loss: 0.466\r",
      "Progress: 6.4% ... Training loss: 0.279 ... Validation loss: 0.464\r",
      "Progress: 6.4% ... Training loss: 0.277 ... Validation loss: 0.464\r",
      "Progress: 6.5% ... Training loss: 0.274 ... Validation loss: 0.459\r",
      "Progress: 6.5% ... Training loss: 0.273 ... Validation loss: 0.451\r",
      "Progress: 6.5% ... Training loss: 0.288 ... Validation loss: 0.473\r",
      "Progress: 6.5% ... Training loss: 0.273 ... Validation loss: 0.457\r",
      "Progress: 6.5% ... Training loss: 0.285 ... Validation loss: 0.470\r",
      "Progress: 6.5% ... Training loss: 0.336 ... Validation loss: 0.489\r",
      "Progress: 6.5% ... Training loss: 0.292 ... Validation loss: 0.464\r",
      "Progress: 6.5% ... Training loss: 0.275 ... Validation loss: 0.444\r",
      "Progress: 6.5% ... Training loss: 0.271 ... Validation loss: 0.441\r",
      "Progress: 6.6% ... Training loss: 0.286 ... Validation loss: 0.441\r",
      "Progress: 6.6% ... Training loss: 0.271 ... Validation loss: 0.445\r",
      "Progress: 6.6% ... Training loss: 0.289 ... Validation loss: 0.461\r",
      "Progress: 6.6% ... Training loss: 0.277 ... Validation loss: 0.453\r",
      "Progress: 6.6% ... Training loss: 0.278 ... Validation loss: 0.453\r",
      "Progress: 6.6% ... Training loss: 0.287 ... Validation loss: 0.456\r",
      "Progress: 6.6% ... Training loss: 0.272 ... Validation loss: 0.442\r",
      "Progress: 6.7% ... Training loss: 0.272 ... Validation loss: 0.442\r",
      "Progress: 6.7% ... Training loss: 0.289 ... Validation loss: 0.448\r",
      "Progress: 6.7% ... Training loss: 0.285 ... Validation loss: 0.453\r",
      "Progress: 6.7% ... Training loss: 0.271 ... Validation loss: 0.442\r",
      "Progress: 6.7% ... Training loss: 0.286 ... Validation loss: 0.451\r",
      "Progress: 6.7% ... Training loss: 0.278 ... Validation loss: 0.436\r",
      "Progress: 6.7% ... Training loss: 0.274 ... Validation loss: 0.449\r",
      "Progress: 6.7% ... Training loss: 0.273 ... Validation loss: 0.443\r",
      "Progress: 6.8% ... Training loss: 0.283 ... Validation loss: 0.439\r",
      "Progress: 6.8% ... Training loss: 0.331 ... Validation loss: 0.500\r",
      "Progress: 6.8% ... Training loss: 0.296 ... Validation loss: 0.467\r",
      "Progress: 6.8% ... Training loss: 0.273 ... Validation loss: 0.453\r",
      "Progress: 6.8% ... Training loss: 0.271 ... Validation loss: 0.453\r",
      "Progress: 6.8% ... Training loss: 0.285 ... Validation loss: 0.464\r",
      "Progress: 6.8% ... Training loss: 0.288 ... Validation loss: 0.448\r",
      "Progress: 6.8% ... Training loss: 0.273 ... Validation loss: 0.449\r",
      "Progress: 6.8% ... Training loss: 0.273 ... Validation loss: 0.441\r",
      "Progress: 6.9% ... Training loss: 0.273 ... Validation loss: 0.452\r",
      "Progress: 6.9% ... Training loss: 0.268 ... Validation loss: 0.449\r",
      "Progress: 6.9% ... Training loss: 0.269 ... Validation loss: 0.448\r",
      "Progress: 6.9% ... Training loss: 0.269 ... Validation loss: 0.449\r",
      "Progress: 6.9% ... Training loss: 0.268 ... Validation loss: 0.443\r",
      "Progress: 6.9% ... Training loss: 0.278 ... Validation loss: 0.458\r",
      "Progress: 6.9% ... Training loss: 0.272 ... Validation loss: 0.456\r",
      "Progress: 7.0% ... Training loss: 0.281 ... Validation loss: 0.461\r",
      "Progress: 7.0% ... Training loss: 0.294 ... Validation loss: 0.463\r",
      "Progress: 7.0% ... Training loss: 0.287 ... Validation loss: 0.463\r",
      "Progress: 7.0% ... Training loss: 0.282 ... Validation loss: 0.452\r",
      "Progress: 7.0% ... Training loss: 0.274 ... Validation loss: 0.442\r",
      "Progress: 7.0% ... Training loss: 0.304 ... Validation loss: 0.443\r",
      "Progress: 7.0% ... Training loss: 0.314 ... Validation loss: 0.477\r",
      "Progress: 7.0% ... Training loss: 0.284 ... Validation loss: 0.455\r",
      "Progress: 7.0% ... Training loss: 0.268 ... Validation loss: 0.448\r",
      "Progress: 7.1% ... Training loss: 0.267 ... Validation loss: 0.447\r",
      "Progress: 7.1% ... Training loss: 0.275 ... Validation loss: 0.455\r",
      "Progress: 7.1% ... Training loss: 0.277 ... Validation loss: 0.457\r",
      "Progress: 7.1% ... Training loss: 0.266 ... Validation loss: 0.444\r",
      "Progress: 7.1% ... Training loss: 0.271 ... Validation loss: 0.452\r",
      "Progress: 7.1% ... Training loss: 0.298 ... Validation loss: 0.444\r",
      "Progress: 7.1% ... Training loss: 0.279 ... Validation loss: 0.453\r",
      "Progress: 7.2% ... Training loss: 0.335 ... Validation loss: 0.515\r",
      "Progress: 7.2% ... Training loss: 0.306 ... Validation loss: 0.466\r",
      "Progress: 7.2% ... Training loss: 0.268 ... Validation loss: 0.446\r",
      "Progress: 7.2% ... Training loss: 0.267 ... Validation loss: 0.448\r",
      "Progress: 7.2% ... Training loss: 0.268 ... Validation loss: 0.448\r",
      "Progress: 7.2% ... Training loss: 0.276 ... Validation loss: 0.452\r",
      "Progress: 7.2% ... Training loss: 0.277 ... Validation loss: 0.448\r",
      "Progress: 7.2% ... Training loss: 0.284 ... Validation loss: 0.442\r",
      "Progress: 7.2% ... Training loss: 0.273 ... Validation loss: 0.446\r",
      "Progress: 7.3% ... Training loss: 0.304 ... Validation loss: 0.448\r",
      "Progress: 7.3% ... Training loss: 0.278 ... Validation loss: 0.447\r",
      "Progress: 7.3% ... Training loss: 0.266 ... Validation loss: 0.439\r",
      "Progress: 7.3% ... Training loss: 0.267 ... Validation loss: 0.444\r",
      "Progress: 7.3% ... Training loss: 0.274 ... Validation loss: 0.447\r",
      "Progress: 7.3% ... Training loss: 0.274 ... Validation loss: 0.453\r",
      "Progress: 7.3% ... Training loss: 0.292 ... Validation loss: 0.481\r",
      "Progress: 7.3% ... Training loss: 0.272 ... Validation loss: 0.444\r",
      "Progress: 7.4% ... Training loss: 0.291 ... Validation loss: 0.469\r",
      "Progress: 7.4% ... Training loss: 0.311 ... Validation loss: 0.471\r",
      "Progress: 7.4% ... Training loss: 0.264 ... Validation loss: 0.445\r",
      "Progress: 7.4% ... Training loss: 0.281 ... Validation loss: 0.443\r",
      "Progress: 7.4% ... Training loss: 0.271 ... Validation loss: 0.445\r",
      "Progress: 7.4% ... Training loss: 0.266 ... Validation loss: 0.440\r",
      "Progress: 7.4% ... Training loss: 0.267 ... Validation loss: 0.447\r",
      "Progress: 7.5% ... Training loss: 0.265 ... Validation loss: 0.439\r",
      "Progress: 7.5% ... Training loss: 0.265 ... Validation loss: 0.441\r",
      "Progress: 7.5% ... Training loss: 0.270 ... Validation loss: 0.438\r",
      "Progress: 7.5% ... Training loss: 0.266 ... Validation loss: 0.443\r",
      "Progress: 7.5% ... Training loss: 0.269 ... Validation loss: 0.435\r",
      "Progress: 7.5% ... Training loss: 0.266 ... Validation loss: 0.432\r",
      "Progress: 7.5% ... Training loss: 0.266 ... Validation loss: 0.435\r",
      "Progress: 7.5% ... Training loss: 0.268 ... Validation loss: 0.432\r",
      "Progress: 7.5% ... Training loss: 0.264 ... Validation loss: 0.438\r",
      "Progress: 7.6% ... Training loss: 0.277 ... Validation loss: 0.456\r",
      "Progress: 7.6% ... Training loss: 0.294 ... Validation loss: 0.471\r",
      "Progress: 7.6% ... Training loss: 0.294 ... Validation loss: 0.456\r",
      "Progress: 7.6% ... Training loss: 0.273 ... Validation loss: 0.444\r",
      "Progress: 7.6% ... Training loss: 0.298 ... Validation loss: 0.464\r",
      "Progress: 7.6% ... Training loss: 0.281 ... Validation loss: 0.447\r",
      "Progress: 7.6% ... Training loss: 0.268 ... Validation loss: 0.439\r",
      "Progress: 7.7% ... Training loss: 0.302 ... Validation loss: 0.450\r",
      "Progress: 7.7% ... Training loss: 0.303 ... Validation loss: 0.478\r",
      "Progress: 7.7% ... Training loss: 0.264 ... Validation loss: 0.437\r",
      "Progress: 7.7% ... Training loss: 0.310 ... Validation loss: 0.484\r",
      "Progress: 7.7% ... Training loss: 0.277 ... Validation loss: 0.453\r",
      "Progress: 7.7% ... Training loss: 0.263 ... Validation loss: 0.439\r",
      "Progress: 7.7% ... Training loss: 0.281 ... Validation loss: 0.442\r",
      "Progress: 7.7% ... Training loss: 0.279 ... Validation loss: 0.454\r",
      "Progress: 7.8% ... Training loss: 0.266 ... Validation loss: 0.430\r",
      "Progress: 7.8% ... Training loss: 0.265 ... Validation loss: 0.433\r",
      "Progress: 7.8% ... Training loss: 0.262 ... Validation loss: 0.434\r",
      "Progress: 7.8% ... Training loss: 0.268 ... Validation loss: 0.439\r",
      "Progress: 7.8% ... Training loss: 0.268 ... Validation loss: 0.436\r",
      "Progress: 7.8% ... Training loss: 0.267 ... Validation loss: 0.434\r",
      "Progress: 7.8% ... Training loss: 0.267 ... Validation loss: 0.431\r",
      "Progress: 7.8% ... Training loss: 0.274 ... Validation loss: 0.455\r",
      "Progress: 7.8% ... Training loss: 0.263 ... Validation loss: 0.448\r",
      "Progress: 7.9% ... Training loss: 0.262 ... Validation loss: 0.435\r",
      "Progress: 7.9% ... Training loss: 0.261 ... Validation loss: 0.437\r",
      "Progress: 7.9% ... Training loss: 0.284 ... Validation loss: 0.443\r",
      "Progress: 7.9% ... Training loss: 0.261 ... Validation loss: 0.430\r",
      "Progress: 7.9% ... Training loss: 0.271 ... Validation loss: 0.449\r",
      "Progress: 7.9% ... Training loss: 0.266 ... Validation loss: 0.429\r",
      "Progress: 7.9% ... Training loss: 0.278 ... Validation loss: 0.454\r",
      "Progress: 8.0% ... Training loss: 0.263 ... Validation loss: 0.443\r",
      "Progress: 8.0% ... Training loss: 0.267 ... Validation loss: 0.452\r",
      "Progress: 8.0% ... Training loss: 0.264 ... Validation loss: 0.450\r",
      "Progress: 8.0% ... Training loss: 0.260 ... Validation loss: 0.443\r",
      "Progress: 8.0% ... Training loss: 0.260 ... Validation loss: 0.438\r",
      "Progress: 8.0% ... Training loss: 0.260 ... Validation loss: 0.437\r",
      "Progress: 8.0% ... Training loss: 0.259 ... Validation loss: 0.439\r",
      "Progress: 8.0% ... Training loss: 0.272 ... Validation loss: 0.437\r",
      "Progress: 8.1% ... Training loss: 0.270 ... Validation loss: 0.433\r",
      "Progress: 8.1% ... Training loss: 0.264 ... Validation loss: 0.430\r",
      "Progress: 8.1% ... Training loss: 0.305 ... Validation loss: 0.488\r",
      "Progress: 8.1% ... Training loss: 0.303 ... Validation loss: 0.473\r",
      "Progress: 8.1% ... Training loss: 0.280 ... Validation loss: 0.461\r",
      "Progress: 8.1% ... Training loss: 0.272 ... Validation loss: 0.452\r",
      "Progress: 8.1% ... Training loss: 0.306 ... Validation loss: 0.468\r",
      "Progress: 8.1% ... Training loss: 0.267 ... Validation loss: 0.457\r",
      "Progress: 8.2% ... Training loss: 0.278 ... Validation loss: 0.457\r",
      "Progress: 8.2% ... Training loss: 0.261 ... Validation loss: 0.439\r",
      "Progress: 8.2% ... Training loss: 0.270 ... Validation loss: 0.450\r",
      "Progress: 8.2% ... Training loss: 0.261 ... Validation loss: 0.440\r",
      "Progress: 8.2% ... Training loss: 0.262 ... Validation loss: 0.439\r",
      "Progress: 8.2% ... Training loss: 0.260 ... Validation loss: 0.441\r",
      "Progress: 8.2% ... Training loss: 0.262 ... Validation loss: 0.445\r",
      "Progress: 8.2% ... Training loss: 0.259 ... Validation loss: 0.433\r",
      "Progress: 8.2% ... Training loss: 0.258 ... Validation loss: 0.434\r",
      "Progress: 8.3% ... Training loss: 0.271 ... Validation loss: 0.453\r",
      "Progress: 8.3% ... Training loss: 0.265 ... Validation loss: 0.434\r",
      "Progress: 8.3% ... Training loss: 0.263 ... Validation loss: 0.444\r",
      "Progress: 8.3% ... Training loss: 0.263 ... Validation loss: 0.442\r",
      "Progress: 8.3% ... Training loss: 0.259 ... Validation loss: 0.433\r",
      "Progress: 8.3% ... Training loss: 0.258 ... Validation loss: 0.431\r",
      "Progress: 8.3% ... Training loss: 0.261 ... Validation loss: 0.431\r",
      "Progress: 8.3% ... Training loss: 0.275 ... Validation loss: 0.445\r",
      "Progress: 8.4% ... Training loss: 0.259 ... Validation loss: 0.424\r",
      "Progress: 8.4% ... Training loss: 0.273 ... Validation loss: 0.441\r",
      "Progress: 8.4% ... Training loss: 0.283 ... Validation loss: 0.443\r",
      "Progress: 8.4% ... Training loss: 0.334 ... Validation loss: 0.481\r",
      "Progress: 8.4% ... Training loss: 0.328 ... Validation loss: 0.495\r",
      "Progress: 8.4% ... Training loss: 0.309 ... Validation loss: 0.467\r",
      "Progress: 8.4% ... Training loss: 0.279 ... Validation loss: 0.439\r",
      "Progress: 8.4% ... Training loss: 0.271 ... Validation loss: 0.442\r",
      "Progress: 8.5% ... Training loss: 0.258 ... Validation loss: 0.437\r",
      "Progress: 8.5% ... Training loss: 0.257 ... Validation loss: 0.438\r",
      "Progress: 8.5% ... Training loss: 0.274 ... Validation loss: 0.454\r",
      "Progress: 8.5% ... Training loss: 0.276 ... Validation loss: 0.460\r",
      "Progress: 8.5% ... Training loss: 0.259 ... Validation loss: 0.441\r",
      "Progress: 8.5% ... Training loss: 0.259 ... Validation loss: 0.444\r",
      "Progress: 8.5% ... Training loss: 0.257 ... Validation loss: 0.443\r",
      "Progress: 8.6% ... Training loss: 0.267 ... Validation loss: 0.439\r",
      "Progress: 8.6% ... Training loss: 0.257 ... Validation loss: 0.436\r",
      "Progress: 8.6% ... Training loss: 0.262 ... Validation loss: 0.440\r",
      "Progress: 8.6% ... Training loss: 0.265 ... Validation loss: 0.438\r",
      "Progress: 8.6% ... Training loss: 0.263 ... Validation loss: 0.437\r",
      "Progress: 8.6% ... Training loss: 0.282 ... Validation loss: 0.440\r",
      "Progress: 8.6% ... Training loss: 0.283 ... Validation loss: 0.451\r",
      "Progress: 8.6% ... Training loss: 0.288 ... Validation loss: 0.463\r",
      "Progress: 8.7% ... Training loss: 0.257 ... Validation loss: 0.432\r",
      "Progress: 8.7% ... Training loss: 0.260 ... Validation loss: 0.430\r",
      "Progress: 8.7% ... Training loss: 0.261 ... Validation loss: 0.439\r",
      "Progress: 8.7% ... Training loss: 0.261 ... Validation loss: 0.442\r",
      "Progress: 8.7% ... Training loss: 0.269 ... Validation loss: 0.444\r",
      "Progress: 8.7% ... Training loss: 0.280 ... Validation loss: 0.454\r",
      "Progress: 8.7% ... Training loss: 0.256 ... Validation loss: 0.434\r",
      "Progress: 8.7% ... Training loss: 0.268 ... Validation loss: 0.447\r",
      "Progress: 8.8% ... Training loss: 0.255 ... Validation loss: 0.436\r",
      "Progress: 8.8% ... Training loss: 0.268 ... Validation loss: 0.442\r",
      "Progress: 8.8% ... Training loss: 0.265 ... Validation loss: 0.453\r",
      "Progress: 8.8% ... Training loss: 0.260 ... Validation loss: 0.433\r",
      "Progress: 8.8% ... Training loss: 0.256 ... Validation loss: 0.439\r",
      "Progress: 8.8% ... Training loss: 0.255 ... Validation loss: 0.434\r",
      "Progress: 8.8% ... Training loss: 0.264 ... Validation loss: 0.444\r",
      "Progress: 8.8% ... Training loss: 0.260 ... Validation loss: 0.452\r",
      "Progress: 8.8% ... Training loss: 0.256 ... Validation loss: 0.438\r",
      "Progress: 8.9% ... Training loss: 0.264 ... Validation loss: 0.439\r",
      "Progress: 8.9% ... Training loss: 0.265 ... Validation loss: 0.438\r",
      "Progress: 8.9% ... Training loss: 0.266 ... Validation loss: 0.440\r",
      "Progress: 8.9% ... Training loss: 0.274 ... Validation loss: 0.442\r",
      "Progress: 8.9% ... Training loss: 0.261 ... Validation loss: 0.439\r",
      "Progress: 8.9% ... Training loss: 0.272 ... Validation loss: 0.437\r",
      "Progress: 8.9% ... Training loss: 0.279 ... Validation loss: 0.461\r",
      "Progress: 8.9% ... Training loss: 0.279 ... Validation loss: 0.454\r",
      "Progress: 9.0% ... Training loss: 0.255 ... Validation loss: 0.432\r",
      "Progress: 9.0% ... Training loss: 0.266 ... Validation loss: 0.445\r",
      "Progress: 9.0% ... Training loss: 0.260 ... Validation loss: 0.446\r",
      "Progress: 9.0% ... Training loss: 0.256 ... Validation loss: 0.443\r",
      "Progress: 9.0% ... Training loss: 0.258 ... Validation loss: 0.442\r",
      "Progress: 9.0% ... Training loss: 0.275 ... Validation loss: 0.442\r",
      "Progress: 9.0% ... Training loss: 0.272 ... Validation loss: 0.449\r",
      "Progress: 9.1% ... Training loss: 0.273 ... Validation loss: 0.440\r",
      "Progress: 9.1% ... Training loss: 0.255 ... Validation loss: 0.435\r",
      "Progress: 9.1% ... Training loss: 0.263 ... Validation loss: 0.441\r",
      "Progress: 9.1% ... Training loss: 0.274 ... Validation loss: 0.430\r",
      "Progress: 9.1% ... Training loss: 0.294 ... Validation loss: 0.469\r",
      "Progress: 9.1% ... Training loss: 0.253 ... Validation loss: 0.430\r",
      "Progress: 9.1% ... Training loss: 0.265 ... Validation loss: 0.434\r",
      "Progress: 9.1% ... Training loss: 0.258 ... Validation loss: 0.443\r",
      "Progress: 9.2% ... Training loss: 0.254 ... Validation loss: 0.436\r",
      "Progress: 9.2% ... Training loss: 0.253 ... Validation loss: 0.437\r",
      "Progress: 9.2% ... Training loss: 0.255 ... Validation loss: 0.439\r",
      "Progress: 9.2% ... Training loss: 0.272 ... Validation loss: 0.456\r",
      "Progress: 9.2% ... Training loss: 0.312 ... Validation loss: 0.469\r",
      "Progress: 9.2% ... Training loss: 0.265 ... Validation loss: 0.442\r",
      "Progress: 9.2% ... Training loss: 0.258 ... Validation loss: 0.439\r",
      "Progress: 9.2% ... Training loss: 0.259 ... Validation loss: 0.437\r",
      "Progress: 9.2% ... Training loss: 0.259 ... Validation loss: 0.438\r",
      "Progress: 9.3% ... Training loss: 0.263 ... Validation loss: 0.447\r",
      "Progress: 9.3% ... Training loss: 0.260 ... Validation loss: 0.436\r",
      "Progress: 9.3% ... Training loss: 0.267 ... Validation loss: 0.449\r",
      "Progress: 9.3% ... Training loss: 0.262 ... Validation loss: 0.439\r",
      "Progress: 9.3% ... Training loss: 0.268 ... Validation loss: 0.450\r",
      "Progress: 9.3% ... Training loss: 0.255 ... Validation loss: 0.437\r",
      "Progress: 9.3% ... Training loss: 0.278 ... Validation loss: 0.445\r",
      "Progress: 9.3% ... Training loss: 0.283 ... Validation loss: 0.461\r",
      "Progress: 9.4% ... Training loss: 0.252 ... Validation loss: 0.427\r",
      "Progress: 9.4% ... Training loss: 0.261 ... Validation loss: 0.436\r",
      "Progress: 9.4% ... Training loss: 0.251 ... Validation loss: 0.429\r",
      "Progress: 9.4% ... Training loss: 0.257 ... Validation loss: 0.427\r",
      "Progress: 9.4% ... Training loss: 0.277 ... Validation loss: 0.447\r",
      "Progress: 9.4% ... Training loss: 0.274 ... Validation loss: 0.445\r",
      "Progress: 9.4% ... Training loss: 0.263 ... Validation loss: 0.440\r",
      "Progress: 9.4% ... Training loss: 0.271 ... Validation loss: 0.438\r",
      "Progress: 9.5% ... Training loss: 0.266 ... Validation loss: 0.439\r",
      "Progress: 9.5% ... Training loss: 0.251 ... Validation loss: 0.430\r",
      "Progress: 9.5% ... Training loss: 0.253 ... Validation loss: 0.427\r",
      "Progress: 9.5% ... Training loss: 0.253 ... Validation loss: 0.431\r",
      "Progress: 9.5% ... Training loss: 0.251 ... Validation loss: 0.430\r",
      "Progress: 9.5% ... Training loss: 0.251 ... Validation loss: 0.433\r",
      "Progress: 9.5% ... Training loss: 0.253 ... Validation loss: 0.438\r",
      "Progress: 9.6% ... Training loss: 0.249 ... Validation loss: 0.433\r",
      "Progress: 9.6% ... Training loss: 0.254 ... Validation loss: 0.435\r",
      "Progress: 9.6% ... Training loss: 0.258 ... Validation loss: 0.444\r",
      "Progress: 9.6% ... Training loss: 0.282 ... Validation loss: 0.464\r",
      "Progress: 9.6% ... Training loss: 0.270 ... Validation loss: 0.451\r",
      "Progress: 9.6% ... Training loss: 0.253 ... Validation loss: 0.436\r",
      "Progress: 9.6% ... Training loss: 0.251 ... Validation loss: 0.431\r",
      "Progress: 9.6% ... Training loss: 0.250 ... Validation loss: 0.430\r",
      "Progress: 9.7% ... Training loss: 0.255 ... Validation loss: 0.431\r",
      "Progress: 9.7% ... Training loss: 0.270 ... Validation loss: 0.452\r",
      "Progress: 9.7% ... Training loss: 0.272 ... Validation loss: 0.440\r",
      "Progress: 9.7% ... Training loss: 0.279 ... Validation loss: 0.451\r",
      "Progress: 9.7% ... Training loss: 0.307 ... Validation loss: 0.443\r",
      "Progress: 9.7% ... Training loss: 0.259 ... Validation loss: 0.432\r",
      "Progress: 9.7% ... Training loss: 0.249 ... Validation loss: 0.427\r",
      "Progress: 9.7% ... Training loss: 0.257 ... Validation loss: 0.436\r",
      "Progress: 9.8% ... Training loss: 0.248 ... Validation loss: 0.430\r",
      "Progress: 9.8% ... Training loss: 0.252 ... Validation loss: 0.428\r",
      "Progress: 9.8% ... Training loss: 0.250 ... Validation loss: 0.431\r",
      "Progress: 9.8% ... Training loss: 0.251 ... Validation loss: 0.434\r",
      "Progress: 9.8% ... Training loss: 0.250 ... Validation loss: 0.431\r",
      "Progress: 9.8% ... Training loss: 0.250 ... Validation loss: 0.435\r",
      "Progress: 9.8% ... Training loss: 0.258 ... Validation loss: 0.445\r",
      "Progress: 9.8% ... Training loss: 0.264 ... Validation loss: 0.431\r",
      "Progress: 9.8% ... Training loss: 0.249 ... Validation loss: 0.427\r",
      "Progress: 9.9% ... Training loss: 0.250 ... Validation loss: 0.428\r",
      "Progress: 9.9% ... Training loss: 0.253 ... Validation loss: 0.429\r",
      "Progress: 9.9% ... Training loss: 0.249 ... Validation loss: 0.432\r",
      "Progress: 9.9% ... Training loss: 0.258 ... Validation loss: 0.429\r",
      "Progress: 9.9% ... Training loss: 0.247 ... Validation loss: 0.428\r",
      "Progress: 9.9% ... Training loss: 0.251 ... Validation loss: 0.430\r",
      "Progress: 9.9% ... Training loss: 0.275 ... Validation loss: 0.458\r",
      "Progress: 9.9% ... Training loss: 0.374 ... Validation loss: 0.502\r",
      "Progress: 10.0% ... Training loss: 0.321 ... Validation loss: 0.488\r",
      "Progress: 10.0% ... Training loss: 0.305 ... Validation loss: 0.463\r",
      "Progress: 10.0% ... Training loss: 0.248 ... Validation loss: 0.423\r",
      "Progress: 10.0% ... Training loss: 0.269 ... Validation loss: 0.446\r",
      "Progress: 10.0% ... Training loss: 0.252 ... Validation loss: 0.432\r",
      "Progress: 10.0% ... Training loss: 0.248 ... Validation loss: 0.428\r",
      "Progress: 10.0% ... Training loss: 0.252 ... Validation loss: 0.426\r",
      "Progress: 10.1% ... Training loss: 0.248 ... Validation loss: 0.425\r",
      "Progress: 10.1% ... Training loss: 0.255 ... Validation loss: 0.429\r",
      "Progress: 10.1% ... Training loss: 0.251 ... Validation loss: 0.423\r",
      "Progress: 10.1% ... Training loss: 0.256 ... Validation loss: 0.421\r",
      "Progress: 10.1% ... Training loss: 0.249 ... Validation loss: 0.430\r",
      "Progress: 10.1% ... Training loss: 0.246 ... Validation loss: 0.423\r",
      "Progress: 10.1% ... Training loss: 0.261 ... Validation loss: 0.435\r",
      "Progress: 10.1% ... Training loss: 0.246 ... Validation loss: 0.424\r",
      "Progress: 10.2% ... Training loss: 0.246 ... Validation loss: 0.420\r",
      "Progress: 10.2% ... Training loss: 0.248 ... Validation loss: 0.419\r",
      "Progress: 10.2% ... Training loss: 0.297 ... Validation loss: 0.470\r",
      "Progress: 10.2% ... Training loss: 0.274 ... Validation loss: 0.447\r",
      "Progress: 10.2% ... Training loss: 0.287 ... Validation loss: 0.453\r",
      "Progress: 10.2% ... Training loss: 0.252 ... Validation loss: 0.424\r",
      "Progress: 10.2% ... Training loss: 0.274 ... Validation loss: 0.447\r",
      "Progress: 10.2% ... Training loss: 0.246 ... Validation loss: 0.423\r",
      "Progress: 10.2% ... Training loss: 0.245 ... Validation loss: 0.425\r",
      "Progress: 10.3% ... Training loss: 0.257 ... Validation loss: 0.438\r",
      "Progress: 10.3% ... Training loss: 0.250 ... Validation loss: 0.423\r",
      "Progress: 10.3% ... Training loss: 0.247 ... Validation loss: 0.422\r",
      "Progress: 10.3% ... Training loss: 0.245 ... Validation loss: 0.423\r",
      "Progress: 10.3% ... Training loss: 0.244 ... Validation loss: 0.423\r",
      "Progress: 10.3% ... Training loss: 0.246 ... Validation loss: 0.421\r",
      "Progress: 10.3% ... Training loss: 0.247 ... Validation loss: 0.424\r",
      "Progress: 10.3% ... Training loss: 0.254 ... Validation loss: 0.423\r",
      "Progress: 10.4% ... Training loss: 0.252 ... Validation loss: 0.421\r",
      "Progress: 10.4% ... Training loss: 0.263 ... Validation loss: 0.443\r",
      "Progress: 10.4% ... Training loss: 0.260 ... Validation loss: 0.427\r",
      "Progress: 10.4% ... Training loss: 0.249 ... Validation loss: 0.426\r",
      "Progress: 10.4% ... Training loss: 0.278 ... Validation loss: 0.471\r",
      "Progress: 10.4% ... Training loss: 0.288 ... Validation loss: 0.443\r",
      "Progress: 10.4% ... Training loss: 0.249 ... Validation loss: 0.446\r",
      "Progress: 10.4% ... Training loss: 0.263 ... Validation loss: 0.438\r",
      "Progress: 10.5% ... Training loss: 0.269 ... Validation loss: 0.452\r",
      "Progress: 10.5% ... Training loss: 0.257 ... Validation loss: 0.435\r",
      "Progress: 10.5% ... Training loss: 0.244 ... Validation loss: 0.435\r",
      "Progress: 10.5% ... Training loss: 0.252 ... Validation loss: 0.429\r",
      "Progress: 10.5% ... Training loss: 0.293 ... Validation loss: 0.484\r",
      "Progress: 10.5% ... Training loss: 0.256 ... Validation loss: 0.429\r",
      "Progress: 10.5% ... Training loss: 0.259 ... Validation loss: 0.430\r",
      "Progress: 10.6% ... Training loss: 0.249 ... Validation loss: 0.419\r",
      "Progress: 10.6% ... Training loss: 0.244 ... Validation loss: 0.422\r",
      "Progress: 10.6% ... Training loss: 0.250 ... Validation loss: 0.418\r",
      "Progress: 10.6% ... Training loss: 0.245 ... Validation loss: 0.421\r",
      "Progress: 10.6% ... Training loss: 0.245 ... Validation loss: 0.419\r",
      "Progress: 10.6% ... Training loss: 0.243 ... Validation loss: 0.416\r",
      "Progress: 10.6% ... Training loss: 0.252 ... Validation loss: 0.429\r",
      "Progress: 10.6% ... Training loss: 0.270 ... Validation loss: 0.434\r",
      "Progress: 10.7% ... Training loss: 0.280 ... Validation loss: 0.442\r",
      "Progress: 10.7% ... Training loss: 0.284 ... Validation loss: 0.447\r",
      "Progress: 10.7% ... Training loss: 0.258 ... Validation loss: 0.431\r",
      "Progress: 10.7% ... Training loss: 0.272 ... Validation loss: 0.434\r",
      "Progress: 10.7% ... Training loss: 0.243 ... Validation loss: 0.416\r",
      "Progress: 10.7% ... Training loss: 0.246 ... Validation loss: 0.421\r",
      "Progress: 10.7% ... Training loss: 0.248 ... Validation loss: 0.427\r",
      "Progress: 10.7% ... Training loss: 0.244 ... Validation loss: 0.420\r",
      "Progress: 10.8% ... Training loss: 0.243 ... Validation loss: 0.418\r",
      "Progress: 10.8% ... Training loss: 0.244 ... Validation loss: 0.420\r",
      "Progress: 10.8% ... Training loss: 0.258 ... Validation loss: 0.426\r",
      "Progress: 10.8% ... Training loss: 0.274 ... Validation loss: 0.433\r",
      "Progress: 10.8% ... Training loss: 0.278 ... Validation loss: 0.443\r",
      "Progress: 10.8% ... Training loss: 0.256 ... Validation loss: 0.415\r",
      "Progress: 10.8% ... Training loss: 0.244 ... Validation loss: 0.424\r",
      "Progress: 10.8% ... Training loss: 0.246 ... Validation loss: 0.424\r",
      "Progress: 10.8% ... Training loss: 0.243 ... Validation loss: 0.423\r",
      "Progress: 10.9% ... Training loss: 0.271 ... Validation loss: 0.434\r",
      "Progress: 10.9% ... Training loss: 0.305 ... Validation loss: 0.464\r",
      "Progress: 10.9% ... Training loss: 0.305 ... Validation loss: 0.461\r",
      "Progress: 10.9% ... Training loss: 0.285 ... Validation loss: 0.447\r",
      "Progress: 10.9% ... Training loss: 0.241 ... Validation loss: 0.423\r",
      "Progress: 10.9% ... Training loss: 0.240 ... Validation loss: 0.419\r",
      "Progress: 10.9% ... Training loss: 0.246 ... Validation loss: 0.429\r",
      "Progress: 10.9% ... Training loss: 0.286 ... Validation loss: 0.438\r",
      "Progress: 11.0% ... Training loss: 0.255 ... Validation loss: 0.436\r",
      "Progress: 11.0% ... Training loss: 0.240 ... Validation loss: 0.420\r",
      "Progress: 11.0% ... Training loss: 0.240 ... Validation loss: 0.415\r",
      "Progress: 11.0% ... Training loss: 0.240 ... Validation loss: 0.418\r",
      "Progress: 11.0% ... Training loss: 0.240 ... Validation loss: 0.413\r",
      "Progress: 11.0% ... Training loss: 0.240 ... Validation loss: 0.417\r",
      "Progress: 11.0% ... Training loss: 0.244 ... Validation loss: 0.415\r",
      "Progress: 11.1% ... Training loss: 0.240 ... Validation loss: 0.415\r",
      "Progress: 11.1% ... Training loss: 0.242 ... Validation loss: 0.412\r",
      "Progress: 11.1% ... Training loss: 0.241 ... Validation loss: 0.412\r",
      "Progress: 11.1% ... Training loss: 0.241 ... Validation loss: 0.414\r",
      "Progress: 11.1% ... Training loss: 0.242 ... Validation loss: 0.414\r",
      "Progress: 11.1% ... Training loss: 0.242 ... Validation loss: 0.409\r",
      "Progress: 11.1% ... Training loss: 0.264 ... Validation loss: 0.420\r",
      "Progress: 11.1% ... Training loss: 0.238 ... Validation loss: 0.411\r",
      "Progress: 11.2% ... Training loss: 0.240 ... Validation loss: 0.408\r",
      "Progress: 11.2% ... Training loss: 0.238 ... Validation loss: 0.410\r",
      "Progress: 11.2% ... Training loss: 0.269 ... Validation loss: 0.437\r",
      "Progress: 11.2% ... Training loss: 0.242 ... Validation loss: 0.408\r",
      "Progress: 11.2% ... Training loss: 0.249 ... Validation loss: 0.415\r",
      "Progress: 11.2% ... Training loss: 0.298 ... Validation loss: 0.458\r",
      "Progress: 11.2% ... Training loss: 0.257 ... Validation loss: 0.431\r",
      "Progress: 11.2% ... Training loss: 0.243 ... Validation loss: 0.423\r",
      "Progress: 11.2% ... Training loss: 0.245 ... Validation loss: 0.412\r",
      "Progress: 11.3% ... Training loss: 0.239 ... Validation loss: 0.409\r",
      "Progress: 11.3% ... Training loss: 0.241 ... Validation loss: 0.418\r",
      "Progress: 11.3% ... Training loss: 0.269 ... Validation loss: 0.415\r",
      "Progress: 11.3% ... Training loss: 0.260 ... Validation loss: 0.438\r",
      "Progress: 11.3% ... Training loss: 0.247 ... Validation loss: 0.415\r",
      "Progress: 11.3% ... Training loss: 0.253 ... Validation loss: 0.428\r",
      "Progress: 11.3% ... Training loss: 0.251 ... Validation loss: 0.425\r",
      "Progress: 11.3% ... Training loss: 0.241 ... Validation loss: 0.415\r",
      "Progress: 11.4% ... Training loss: 0.236 ... Validation loss: 0.412\r",
      "Progress: 11.4% ... Training loss: 0.247 ... Validation loss: 0.409\r",
      "Progress: 11.4% ... Training loss: 0.242 ... Validation loss: 0.418\r",
      "Progress: 11.4% ... Training loss: 0.237 ... Validation loss: 0.410\r",
      "Progress: 11.4% ... Training loss: 0.265 ... Validation loss: 0.444\r",
      "Progress: 11.4% ... Training loss: 0.241 ... Validation loss: 0.422\r",
      "Progress: 11.4% ... Training loss: 0.243 ... Validation loss: 0.418\r",
      "Progress: 11.4% ... Training loss: 0.246 ... Validation loss: 0.430\r",
      "Progress: 11.5% ... Training loss: 0.242 ... Validation loss: 0.415\r",
      "Progress: 11.5% ... Training loss: 0.240 ... Validation loss: 0.421\r",
      "Progress: 11.5% ... Training loss: 0.240 ... Validation loss: 0.412\r",
      "Progress: 11.5% ... Training loss: 0.237 ... Validation loss: 0.413\r",
      "Progress: 11.5% ... Training loss: 0.244 ... Validation loss: 0.424\r",
      "Progress: 11.5% ... Training loss: 0.245 ... Validation loss: 0.420\r",
      "Progress: 11.5% ... Training loss: 0.250 ... Validation loss: 0.428\r",
      "Progress: 11.6% ... Training loss: 0.239 ... Validation loss: 0.416\r",
      "Progress: 11.6% ... Training loss: 0.249 ... Validation loss: 0.430\r",
      "Progress: 11.6% ... Training loss: 0.236 ... Validation loss: 0.417\r",
      "Progress: 11.6% ... Training loss: 0.236 ... Validation loss: 0.416\r",
      "Progress: 11.6% ... Training loss: 0.250 ... Validation loss: 0.437\r",
      "Progress: 11.6% ... Training loss: 0.235 ... Validation loss: 0.420\r",
      "Progress: 11.6% ... Training loss: 0.235 ... Validation loss: 0.421\r",
      "Progress: 11.6% ... Training loss: 0.238 ... Validation loss: 0.426\r",
      "Progress: 11.7% ... Training loss: 0.237 ... Validation loss: 0.425\r",
      "Progress: 11.7% ... Training loss: 0.236 ... Validation loss: 0.420\r",
      "Progress: 11.7% ... Training loss: 0.256 ... Validation loss: 0.422\r",
      "Progress: 11.7% ... Training loss: 0.236 ... Validation loss: 0.416\r",
      "Progress: 11.7% ... Training loss: 0.235 ... Validation loss: 0.414\r",
      "Progress: 11.7% ... Training loss: 0.239 ... Validation loss: 0.421\r",
      "Progress: 11.7% ... Training loss: 0.241 ... Validation loss: 0.416\r",
      "Progress: 11.7% ... Training loss: 0.239 ... Validation loss: 0.418\r",
      "Progress: 11.8% ... Training loss: 0.244 ... Validation loss: 0.419\r",
      "Progress: 11.8% ... Training loss: 0.234 ... Validation loss: 0.410\r",
      "Progress: 11.8% ... Training loss: 0.244 ... Validation loss: 0.420\r",
      "Progress: 11.8% ... Training loss: 0.241 ... Validation loss: 0.425\r",
      "Progress: 11.8% ... Training loss: 0.244 ... Validation loss: 0.421\r",
      "Progress: 11.8% ... Training loss: 0.274 ... Validation loss: 0.439\r",
      "Progress: 11.8% ... Training loss: 0.279 ... Validation loss: 0.437\r",
      "Progress: 11.8% ... Training loss: 0.239 ... Validation loss: 0.433\r",
      "Progress: 11.8% ... Training loss: 0.248 ... Validation loss: 0.423\r",
      "Progress: 11.9% ... Training loss: 0.264 ... Validation loss: 0.453\r",
      "Progress: 11.9% ... Training loss: 0.256 ... Validation loss: 0.419\r",
      "Progress: 11.9% ... Training loss: 0.235 ... Validation loss: 0.419\r",
      "Progress: 11.9% ... Training loss: 0.236 ... Validation loss: 0.415\r",
      "Progress: 11.9% ... Training loss: 0.244 ... Validation loss: 0.426\r",
      "Progress: 11.9% ... Training loss: 0.239 ... Validation loss: 0.408\r",
      "Progress: 11.9% ... Training loss: 0.239 ... Validation loss: 0.429\r",
      "Progress: 11.9% ... Training loss: 0.254 ... Validation loss: 0.437\r",
      "Progress: 12.0% ... Training loss: 0.241 ... Validation loss: 0.406\r",
      "Progress: 12.0% ... Training loss: 0.232 ... Validation loss: 0.410\r",
      "Progress: 12.0% ... Training loss: 0.237 ... Validation loss: 0.407\r",
      "Progress: 12.0% ... Training loss: 0.245 ... Validation loss: 0.442\r",
      "Progress: 12.0% ... Training loss: 0.241 ... Validation loss: 0.418\r",
      "Progress: 12.0% ... Training loss: 0.236 ... Validation loss: 0.406\r",
      "Progress: 12.0% ... Training loss: 0.232 ... Validation loss: 0.417\r",
      "Progress: 12.1% ... Training loss: 0.233 ... Validation loss: 0.414\r",
      "Progress: 12.1% ... Training loss: 0.235 ... Validation loss: 0.411\r",
      "Progress: 12.1% ... Training loss: 0.233 ... Validation loss: 0.409\r",
      "Progress: 12.1% ... Training loss: 0.231 ... Validation loss: 0.407\r",
      "Progress: 12.1% ... Training loss: 0.235 ... Validation loss: 0.410\r",
      "Progress: 12.1% ... Training loss: 0.238 ... Validation loss: 0.427\r",
      "Progress: 12.1% ... Training loss: 0.233 ... Validation loss: 0.409\r",
      "Progress: 12.1% ... Training loss: 0.231 ... Validation loss: 0.412\r",
      "Progress: 12.2% ... Training loss: 0.232 ... Validation loss: 0.414\r",
      "Progress: 12.2% ... Training loss: 0.230 ... Validation loss: 0.407\r",
      "Progress: 12.2% ... Training loss: 0.231 ... Validation loss: 0.410\r",
      "Progress: 12.2% ... Training loss: 0.230 ... Validation loss: 0.409\r",
      "Progress: 12.2% ... Training loss: 0.231 ... Validation loss: 0.410\r",
      "Progress: 12.2% ... Training loss: 0.235 ... Validation loss: 0.414\r",
      "Progress: 12.2% ... Training loss: 0.263 ... Validation loss: 0.437\r",
      "Progress: 12.2% ... Training loss: 0.246 ... Validation loss: 0.415\r",
      "Progress: 12.2% ... Training loss: 0.260 ... Validation loss: 0.431\r",
      "Progress: 12.3% ... Training loss: 0.242 ... Validation loss: 0.410\r",
      "Progress: 12.3% ... Training loss: 0.237 ... Validation loss: 0.408\r",
      "Progress: 12.3% ... Training loss: 0.230 ... Validation loss: 0.402\r",
      "Progress: 12.3% ... Training loss: 0.233 ... Validation loss: 0.403\r",
      "Progress: 12.3% ... Training loss: 0.242 ... Validation loss: 0.408\r",
      "Progress: 12.3% ... Training loss: 0.229 ... Validation loss: 0.401\r",
      "Progress: 12.3% ... Training loss: 0.232 ... Validation loss: 0.403\r",
      "Progress: 12.3% ... Training loss: 0.250 ... Validation loss: 0.425\r",
      "Progress: 12.4% ... Training loss: 0.229 ... Validation loss: 0.400\r",
      "Progress: 12.4% ... Training loss: 0.243 ... Validation loss: 0.405\r",
      "Progress: 12.4% ... Training loss: 0.237 ... Validation loss: 0.403\r",
      "Progress: 12.4% ... Training loss: 0.237 ... Validation loss: 0.407\r",
      "Progress: 12.4% ... Training loss: 0.239 ... Validation loss: 0.409\r",
      "Progress: 12.4% ... Training loss: 0.234 ... Validation loss: 0.423\r",
      "Progress: 12.4% ... Training loss: 0.236 ... Validation loss: 0.408\r",
      "Progress: 12.4% ... Training loss: 0.232 ... Validation loss: 0.406\r",
      "Progress: 12.5% ... Training loss: 0.231 ... Validation loss: 0.407\r",
      "Progress: 12.5% ... Training loss: 0.228 ... Validation loss: 0.409\r",
      "Progress: 12.5% ... Training loss: 0.231 ... Validation loss: 0.411\r",
      "Progress: 12.5% ... Training loss: 0.233 ... Validation loss: 0.407\r",
      "Progress: 12.5% ... Training loss: 0.232 ... Validation loss: 0.409\r",
      "Progress: 12.5% ... Training loss: 0.232 ... Validation loss: 0.419\r",
      "Progress: 12.5% ... Training loss: 0.228 ... Validation loss: 0.408\r",
      "Progress: 12.6% ... Training loss: 0.230 ... Validation loss: 0.408\r",
      "Progress: 12.6% ... Training loss: 0.240 ... Validation loss: 0.410\r",
      "Progress: 12.6% ... Training loss: 0.229 ... Validation loss: 0.405\r",
      "Progress: 12.6% ... Training loss: 0.230 ... Validation loss: 0.403\r",
      "Progress: 12.6% ... Training loss: 0.237 ... Validation loss: 0.411\r",
      "Progress: 12.6% ... Training loss: 0.246 ... Validation loss: 0.421\r",
      "Progress: 12.6% ... Training loss: 0.227 ... Validation loss: 0.399\r",
      "Progress: 12.6% ... Training loss: 0.229 ... Validation loss: 0.401\r",
      "Progress: 12.7% ... Training loss: 0.238 ... Validation loss: 0.396\r",
      "Progress: 12.7% ... Training loss: 0.233 ... Validation loss: 0.407\r",
      "Progress: 12.7% ... Training loss: 0.226 ... Validation loss: 0.403\r",
      "Progress: 12.7% ... Training loss: 0.242 ... Validation loss: 0.412\r",
      "Progress: 12.7% ... Training loss: 0.258 ... Validation loss: 0.411\r",
      "Progress: 12.7% ... Training loss: 0.246 ... Validation loss: 0.413\r",
      "Progress: 12.7% ... Training loss: 0.232 ... Validation loss: 0.412\r",
      "Progress: 12.7% ... Training loss: 0.228 ... Validation loss: 0.399\r",
      "Progress: 12.8% ... Training loss: 0.230 ... Validation loss: 0.410\r",
      "Progress: 12.8% ... Training loss: 0.227 ... Validation loss: 0.397\r",
      "Progress: 12.8% ... Training loss: 0.227 ... Validation loss: 0.397\r",
      "Progress: 12.8% ... Training loss: 0.229 ... Validation loss: 0.406\r",
      "Progress: 12.8% ... Training loss: 0.228 ... Validation loss: 0.393\r",
      "Progress: 12.8% ... Training loss: 0.279 ... Validation loss: 0.422\r",
      "Progress: 12.8% ... Training loss: 0.259 ... Validation loss: 0.408\r",
      "Progress: 12.8% ... Training loss: 0.271 ... Validation loss: 0.455\r",
      "Progress: 12.8% ... Training loss: 0.231 ... Validation loss: 0.397\r",
      "Progress: 12.9% ... Training loss: 0.229 ... Validation loss: 0.396\r",
      "Progress: 12.9% ... Training loss: 0.238 ... Validation loss: 0.400\r",
      "Progress: 12.9% ... Training loss: 0.232 ... Validation loss: 0.388\r",
      "Progress: 12.9% ... Training loss: 0.231 ... Validation loss: 0.389\r",
      "Progress: 12.9% ... Training loss: 0.227 ... Validation loss: 0.389\r",
      "Progress: 12.9% ... Training loss: 0.229 ... Validation loss: 0.390\r",
      "Progress: 12.9% ... Training loss: 0.239 ... Validation loss: 0.394\r",
      "Progress: 12.9% ... Training loss: 0.231 ... Validation loss: 0.413\r",
      "Progress: 13.0% ... Training loss: 0.225 ... Validation loss: 0.392\r",
      "Progress: 13.0% ... Training loss: 0.227 ... Validation loss: 0.397\r",
      "Progress: 13.0% ... Training loss: 0.229 ... Validation loss: 0.397\r",
      "Progress: 13.0% ... Training loss: 0.227 ... Validation loss: 0.407\r",
      "Progress: 13.0% ... Training loss: 0.225 ... Validation loss: 0.405\r",
      "Progress: 13.0% ... Training loss: 0.229 ... Validation loss: 0.412\r",
      "Progress: 13.0% ... Training loss: 0.225 ... Validation loss: 0.396\r",
      "Progress: 13.1% ... Training loss: 0.228 ... Validation loss: 0.396\r",
      "Progress: 13.1% ... Training loss: 0.228 ... Validation loss: 0.396\r",
      "Progress: 13.1% ... Training loss: 0.225 ... Validation loss: 0.405\r",
      "Progress: 13.1% ... Training loss: 0.227 ... Validation loss: 0.402\r",
      "Progress: 13.1% ... Training loss: 0.235 ... Validation loss: 0.398\r",
      "Progress: 13.1% ... Training loss: 0.224 ... Validation loss: 0.399\r",
      "Progress: 13.1% ... Training loss: 0.227 ... Validation loss: 0.417\r",
      "Progress: 13.1% ... Training loss: 0.228 ... Validation loss: 0.405\r",
      "Progress: 13.2% ... Training loss: 0.225 ... Validation loss: 0.403\r",
      "Progress: 13.2% ... Training loss: 0.227 ... Validation loss: 0.419\r",
      "Progress: 13.2% ... Training loss: 0.259 ... Validation loss: 0.414\r",
      "Progress: 13.2% ... Training loss: 0.244 ... Validation loss: 0.441\r",
      "Progress: 13.2% ... Training loss: 0.224 ... Validation loss: 0.401\r",
      "Progress: 13.2% ... Training loss: 0.222 ... Validation loss: 0.402\r",
      "Progress: 13.2% ... Training loss: 0.238 ... Validation loss: 0.414\r",
      "Progress: 13.2% ... Training loss: 0.238 ... Validation loss: 0.405\r",
      "Progress: 13.2% ... Training loss: 0.222 ... Validation loss: 0.400\r",
      "Progress: 13.3% ... Training loss: 0.222 ... Validation loss: 0.404\r",
      "Progress: 13.3% ... Training loss: 0.231 ... Validation loss: 0.408\r",
      "Progress: 13.3% ... Training loss: 0.222 ... Validation loss: 0.410\r",
      "Progress: 13.3% ... Training loss: 0.224 ... Validation loss: 0.418\r",
      "Progress: 13.3% ... Training loss: 0.227 ... Validation loss: 0.414\r",
      "Progress: 13.3% ... Training loss: 0.222 ... Validation loss: 0.401\r",
      "Progress: 13.3% ... Training loss: 0.226 ... Validation loss: 0.405\r",
      "Progress: 13.3% ... Training loss: 0.221 ... Validation loss: 0.409\r",
      "Progress: 13.4% ... Training loss: 0.229 ... Validation loss: 0.410\r",
      "Progress: 13.4% ... Training loss: 0.221 ... Validation loss: 0.396\r",
      "Progress: 13.4% ... Training loss: 0.223 ... Validation loss: 0.403\r",
      "Progress: 13.4% ... Training loss: 0.221 ... Validation loss: 0.396\r",
      "Progress: 13.4% ... Training loss: 0.221 ... Validation loss: 0.399\r",
      "Progress: 13.4% ... Training loss: 0.223 ... Validation loss: 0.395\r",
      "Progress: 13.4% ... Training loss: 0.222 ... Validation loss: 0.397\r",
      "Progress: 13.4% ... Training loss: 0.228 ... Validation loss: 0.404\r",
      "Progress: 13.5% ... Training loss: 0.249 ... Validation loss: 0.406\r",
      "Progress: 13.5% ... Training loss: 0.229 ... Validation loss: 0.422\r",
      "Progress: 13.5% ... Training loss: 0.220 ... Validation loss: 0.399\r",
      "Progress: 13.5% ... Training loss: 0.238 ... Validation loss: 0.401\r",
      "Progress: 13.5% ... Training loss: 0.221 ... Validation loss: 0.414\r",
      "Progress: 13.5% ... Training loss: 0.223 ... Validation loss: 0.417\r",
      "Progress: 13.5% ... Training loss: 0.219 ... Validation loss: 0.405\r",
      "Progress: 13.6% ... Training loss: 0.230 ... Validation loss: 0.404\r",
      "Progress: 13.6% ... Training loss: 0.228 ... Validation loss: 0.419\r",
      "Progress: 13.6% ... Training loss: 0.224 ... Validation loss: 0.397\r",
      "Progress: 13.6% ... Training loss: 0.224 ... Validation loss: 0.402\r",
      "Progress: 13.6% ... Training loss: 0.218 ... Validation loss: 0.393\r",
      "Progress: 13.6% ... Training loss: 0.218 ... Validation loss: 0.400\r",
      "Progress: 13.6% ... Training loss: 0.275 ... Validation loss: 0.455\r",
      "Progress: 13.6% ... Training loss: 0.224 ... Validation loss: 0.390\r",
      "Progress: 13.7% ... Training loss: 0.232 ... Validation loss: 0.402\r",
      "Progress: 13.7% ... Training loss: 0.242 ... Validation loss: 0.396\r",
      "Progress: 13.7% ... Training loss: 0.217 ... Validation loss: 0.390\r",
      "Progress: 13.7% ... Training loss: 0.217 ... Validation loss: 0.396\r",
      "Progress: 13.7% ... Training loss: 0.220 ... Validation loss: 0.399\r",
      "Progress: 13.7% ... Training loss: 0.218 ... Validation loss: 0.399\r",
      "Progress: 13.7% ... Training loss: 0.226 ... Validation loss: 0.396\r",
      "Progress: 13.7% ... Training loss: 0.220 ... Validation loss: 0.397\r",
      "Progress: 13.8% ... Training loss: 0.217 ... Validation loss: 0.394\r",
      "Progress: 13.8% ... Training loss: 0.220 ... Validation loss: 0.396\r",
      "Progress: 13.8% ... Training loss: 0.216 ... Validation loss: 0.391\r",
      "Progress: 13.8% ... Training loss: 0.219 ... Validation loss: 0.396\r",
      "Progress: 13.8% ... Training loss: 0.221 ... Validation loss: 0.397\r",
      "Progress: 13.8% ... Training loss: 0.219 ... Validation loss: 0.386\r",
      "Progress: 13.8% ... Training loss: 0.233 ... Validation loss: 0.399\r",
      "Progress: 13.8% ... Training loss: 0.249 ... Validation loss: 0.397\r",
      "Progress: 13.8% ... Training loss: 0.249 ... Validation loss: 0.439\r",
      "Progress: 13.9% ... Training loss: 0.234 ... Validation loss: 0.393\r",
      "Progress: 13.9% ... Training loss: 0.216 ... Validation loss: 0.386\r",
      "Progress: 13.9% ... Training loss: 0.229 ... Validation loss: 0.390\r",
      "Progress: 13.9% ... Training loss: 0.225 ... Validation loss: 0.386\r",
      "Progress: 13.9% ... Training loss: 0.217 ... Validation loss: 0.385\r",
      "Progress: 13.9% ... Training loss: 0.217 ... Validation loss: 0.392\r",
      "Progress: 13.9% ... Training loss: 0.219 ... Validation loss: 0.387\r",
      "Progress: 13.9% ... Training loss: 0.214 ... Validation loss: 0.389\r",
      "Progress: 14.0% ... Training loss: 0.217 ... Validation loss: 0.392\r",
      "Progress: 14.0% ... Training loss: 0.216 ... Validation loss: 0.394\r",
      "Progress: 14.0% ... Training loss: 0.257 ... Validation loss: 0.422\r",
      "Progress: 14.0% ... Training loss: 0.217 ... Validation loss: 0.393\r",
      "Progress: 14.0% ... Training loss: 0.228 ... Validation loss: 0.405\r",
      "Progress: 14.0% ... Training loss: 0.219 ... Validation loss: 0.393\r",
      "Progress: 14.0% ... Training loss: 0.216 ... Validation loss: 0.396\r",
      "Progress: 14.1% ... Training loss: 0.224 ... Validation loss: 0.403\r",
      "Progress: 14.1% ... Training loss: 0.228 ... Validation loss: 0.394\r",
      "Progress: 14.1% ... Training loss: 0.218 ... Validation loss: 0.390\r",
      "Progress: 14.1% ... Training loss: 0.230 ... Validation loss: 0.408\r",
      "Progress: 14.1% ... Training loss: 0.272 ... Validation loss: 0.429\r",
      "Progress: 14.1% ... Training loss: 0.238 ... Validation loss: 0.409\r",
      "Progress: 14.1% ... Training loss: 0.217 ... Validation loss: 0.396\r",
      "Progress: 14.1% ... Training loss: 0.214 ... Validation loss: 0.393\r",
      "Progress: 14.2% ... Training loss: 0.218 ... Validation loss: 0.397\r",
      "Progress: 14.2% ... Training loss: 0.217 ... Validation loss: 0.397\r",
      "Progress: 14.2% ... Training loss: 0.231 ... Validation loss: 0.405\r",
      "Progress: 14.2% ... Training loss: 0.275 ... Validation loss: 0.443\r",
      "Progress: 14.2% ... Training loss: 0.244 ... Validation loss: 0.406\r",
      "Progress: 14.2% ... Training loss: 0.213 ... Validation loss: 0.396\r",
      "Progress: 14.2% ... Training loss: 0.214 ... Validation loss: 0.401\r",
      "Progress: 14.2% ... Training loss: 0.215 ... Validation loss: 0.398\r",
      "Progress: 14.2% ... Training loss: 0.230 ... Validation loss: 0.416\r",
      "Progress: 14.3% ... Training loss: 0.219 ... Validation loss: 0.402\r",
      "Progress: 14.3% ... Training loss: 0.215 ... Validation loss: 0.410\r",
      "Progress: 14.3% ... Training loss: 0.213 ... Validation loss: 0.398\r",
      "Progress: 14.3% ... Training loss: 0.214 ... Validation loss: 0.399\r",
      "Progress: 14.3% ... Training loss: 0.214 ... Validation loss: 0.399\r",
      "Progress: 14.3% ... Training loss: 0.233 ... Validation loss: 0.394\r",
      "Progress: 14.3% ... Training loss: 0.221 ... Validation loss: 0.407\r",
      "Progress: 14.3% ... Training loss: 0.212 ... Validation loss: 0.387\r",
      "Progress: 14.4% ... Training loss: 0.221 ... Validation loss: 0.398\r",
      "Progress: 14.4% ... Training loss: 0.222 ... Validation loss: 0.389\r",
      "Progress: 14.4% ... Training loss: 0.234 ... Validation loss: 0.426\r",
      "Progress: 14.4% ... Training loss: 0.230 ... Validation loss: 0.392\r",
      "Progress: 14.4% ... Training loss: 0.216 ... Validation loss: 0.401\r",
      "Progress: 14.4% ... Training loss: 0.220 ... Validation loss: 0.390\r",
      "Progress: 14.4% ... Training loss: 0.216 ... Validation loss: 0.397\r",
      "Progress: 14.4% ... Training loss: 0.240 ... Validation loss: 0.396\r",
      "Progress: 14.5% ... Training loss: 0.217 ... Validation loss: 0.409\r",
      "Progress: 14.5% ... Training loss: 0.212 ... Validation loss: 0.396\r",
      "Progress: 14.5% ... Training loss: 0.214 ... Validation loss: 0.387\r",
      "Progress: 14.5% ... Training loss: 0.239 ... Validation loss: 0.427\r",
      "Progress: 14.5% ... Training loss: 0.233 ... Validation loss: 0.388\r",
      "Progress: 14.5% ... Training loss: 0.238 ... Validation loss: 0.445\r",
      "Progress: 14.5% ... Training loss: 0.218 ... Validation loss: 0.384\r",
      "Progress: 14.6% ... Training loss: 0.212 ... Validation loss: 0.392\r",
      "Progress: 14.6% ... Training loss: 0.214 ... Validation loss: 0.384\r",
      "Progress: 14.6% ... Training loss: 0.228 ... Validation loss: 0.404\r",
      "Progress: 14.6% ... Training loss: 0.214 ... Validation loss: 0.384\r",
      "Progress: 14.6% ... Training loss: 0.244 ... Validation loss: 0.426\r",
      "Progress: 14.6% ... Training loss: 0.219 ... Validation loss: 0.383\r",
      "Progress: 14.6% ... Training loss: 0.211 ... Validation loss: 0.398\r",
      "Progress: 14.6% ... Training loss: 0.226 ... Validation loss: 0.385\r",
      "Progress: 14.7% ... Training loss: 0.217 ... Validation loss: 0.408\r",
      "Progress: 14.7% ... Training loss: 0.238 ... Validation loss: 0.395\r",
      "Progress: 14.7% ... Training loss: 0.224 ... Validation loss: 0.422\r",
      "Progress: 14.7% ... Training loss: 0.228 ... Validation loss: 0.394\r",
      "Progress: 14.7% ... Training loss: 0.211 ... Validation loss: 0.402\r",
      "Progress: 14.7% ... Training loss: 0.217 ... Validation loss: 0.411\r",
      "Progress: 14.7% ... Training loss: 0.219 ... Validation loss: 0.387\r",
      "Progress: 14.7% ... Training loss: 0.209 ... Validation loss: 0.389\r",
      "Progress: 14.8% ... Training loss: 0.211 ... Validation loss: 0.391\r",
      "Progress: 14.8% ... Training loss: 0.210 ... Validation loss: 0.394\r",
      "Progress: 14.8% ... Training loss: 0.216 ... Validation loss: 0.400\r",
      "Progress: 14.8% ... Training loss: 0.214 ... Validation loss: 0.383\r",
      "Progress: 14.8% ... Training loss: 0.209 ... Validation loss: 0.383\r",
      "Progress: 14.8% ... Training loss: 0.211 ... Validation loss: 0.386\r",
      "Progress: 14.8% ... Training loss: 0.216 ... Validation loss: 0.387\r",
      "Progress: 14.8% ... Training loss: 0.211 ... Validation loss: 0.401\r",
      "Progress: 14.8% ... Training loss: 0.209 ... Validation loss: 0.387\r",
      "Progress: 14.9% ... Training loss: 0.210 ... Validation loss: 0.401\r",
      "Progress: 14.9% ... Training loss: 0.209 ... Validation loss: 0.394\r",
      "Progress: 14.9% ... Training loss: 0.209 ... Validation loss: 0.397\r",
      "Progress: 14.9% ... Training loss: 0.213 ... Validation loss: 0.387\r",
      "Progress: 14.9% ... Training loss: 0.209 ... Validation loss: 0.386\r",
      "Progress: 14.9% ... Training loss: 0.208 ... Validation loss: 0.388\r",
      "Progress: 14.9% ... Training loss: 0.211 ... Validation loss: 0.396\r",
      "Progress: 14.9% ... Training loss: 0.220 ... Validation loss: 0.391\r",
      "Progress: 15.0% ... Training loss: 0.207 ... Validation loss: 0.392\r",
      "Progress: 15.0% ... Training loss: 0.232 ... Validation loss: 0.399\r",
      "Progress: 15.0% ... Training loss: 0.214 ... Validation loss: 0.398\r",
      "Progress: 15.0% ... Training loss: 0.218 ... Validation loss: 0.393\r",
      "Progress: 15.0% ... Training loss: 0.222 ... Validation loss: 0.410\r",
      "Progress: 15.0% ... Training loss: 0.208 ... Validation loss: 0.390\r",
      "Progress: 15.0% ... Training loss: 0.214 ... Validation loss: 0.409\r",
      "Progress: 15.1% ... Training loss: 0.208 ... Validation loss: 0.400\r",
      "Progress: 15.1% ... Training loss: 0.211 ... Validation loss: 0.404\r",
      "Progress: 15.1% ... Training loss: 0.210 ... Validation loss: 0.403\r",
      "Progress: 15.1% ... Training loss: 0.210 ... Validation loss: 0.394\r",
      "Progress: 15.1% ... Training loss: 0.214 ... Validation loss: 0.418\r",
      "Progress: 15.1% ... Training loss: 0.207 ... Validation loss: 0.389\r",
      "Progress: 15.1% ... Training loss: 0.213 ... Validation loss: 0.400\r",
      "Progress: 15.1% ... Training loss: 0.209 ... Validation loss: 0.388\r",
      "Progress: 15.2% ... Training loss: 0.210 ... Validation loss: 0.392\r",
      "Progress: 15.2% ... Training loss: 0.216 ... Validation loss: 0.394\r",
      "Progress: 15.2% ... Training loss: 0.230 ... Validation loss: 0.427\r",
      "Progress: 15.2% ... Training loss: 0.208 ... Validation loss: 0.387\r",
      "Progress: 15.2% ... Training loss: 0.209 ... Validation loss: 0.391\r",
      "Progress: 15.2% ... Training loss: 0.213 ... Validation loss: 0.379\r",
      "Progress: 15.2% ... Training loss: 0.219 ... Validation loss: 0.404\r",
      "Progress: 15.2% ... Training loss: 0.212 ... Validation loss: 0.376\r",
      "Progress: 15.2% ... Training loss: 0.228 ... Validation loss: 0.418\r",
      "Progress: 15.3% ... Training loss: 0.207 ... Validation loss: 0.380\r",
      "Progress: 15.3% ... Training loss: 0.207 ... Validation loss: 0.382\r",
      "Progress: 15.3% ... Training loss: 0.214 ... Validation loss: 0.385\r",
      "Progress: 15.3% ... Training loss: 0.207 ... Validation loss: 0.383\r",
      "Progress: 15.3% ... Training loss: 0.216 ... Validation loss: 0.405\r",
      "Progress: 15.3% ... Training loss: 0.238 ... Validation loss: 0.390\r",
      "Progress: 15.3% ... Training loss: 0.226 ... Validation loss: 0.409\r",
      "Progress: 15.3% ... Training loss: 0.207 ... Validation loss: 0.378\r",
      "Progress: 15.4% ... Training loss: 0.230 ... Validation loss: 0.411\r",
      "Progress: 15.4% ... Training loss: 0.216 ... Validation loss: 0.373\r",
      "Progress: 15.4% ... Training loss: 0.209 ... Validation loss: 0.376\r",
      "Progress: 15.4% ... Training loss: 0.213 ... Validation loss: 0.376\r",
      "Progress: 15.4% ... Training loss: 0.206 ... Validation loss: 0.385\r",
      "Progress: 15.4% ... Training loss: 0.205 ... Validation loss: 0.380\r",
      "Progress: 15.4% ... Training loss: 0.219 ... Validation loss: 0.407\r",
      "Progress: 15.4% ... Training loss: 0.211 ... Validation loss: 0.397\r",
      "Progress: 15.5% ... Training loss: 0.209 ... Validation loss: 0.375\r",
      "Progress: 15.5% ... Training loss: 0.208 ... Validation loss: 0.383\r",
      "Progress: 15.5% ... Training loss: 0.227 ... Validation loss: 0.382\r",
      "Progress: 15.5% ... Training loss: 0.209 ... Validation loss: 0.392\r",
      "Progress: 15.5% ... Training loss: 0.205 ... Validation loss: 0.376\r",
      "Progress: 15.5% ... Training loss: 0.205 ... Validation loss: 0.375\r",
      "Progress: 15.5% ... Training loss: 0.204 ... Validation loss: 0.379\r",
      "Progress: 15.6% ... Training loss: 0.203 ... Validation loss: 0.384\r",
      "Progress: 15.6% ... Training loss: 0.205 ... Validation loss: 0.392\r",
      "Progress: 15.6% ... Training loss: 0.205 ... Validation loss: 0.395\r",
      "Progress: 15.6% ... Training loss: 0.208 ... Validation loss: 0.404\r",
      "Progress: 15.6% ... Training loss: 0.203 ... Validation loss: 0.396\r",
      "Progress: 15.6% ... Training loss: 0.201 ... Validation loss: 0.389\r",
      "Progress: 15.6% ... Training loss: 0.201 ... Validation loss: 0.389\r",
      "Progress: 15.6% ... Training loss: 0.201 ... Validation loss: 0.387\r",
      "Progress: 15.7% ... Training loss: 0.204 ... Validation loss: 0.402\r",
      "Progress: 15.7% ... Training loss: 0.203 ... Validation loss: 0.388\r",
      "Progress: 15.7% ... Training loss: 0.204 ... Validation loss: 0.387\r",
      "Progress: 15.7% ... Training loss: 0.203 ... Validation loss: 0.378\r",
      "Progress: 15.7% ... Training loss: 0.206 ... Validation loss: 0.392\r",
      "Progress: 15.7% ... Training loss: 0.204 ... Validation loss: 0.384\r",
      "Progress: 15.7% ... Training loss: 0.201 ... Validation loss: 0.387\r",
      "Progress: 15.7% ... Training loss: 0.204 ... Validation loss: 0.406\r",
      "Progress: 15.8% ... Training loss: 0.201 ... Validation loss: 0.385\r",
      "Progress: 15.8% ... Training loss: 0.222 ... Validation loss: 0.432\r",
      "Progress: 15.8% ... Training loss: 0.201 ... Validation loss: 0.391\r",
      "Progress: 15.8% ... Training loss: 0.205 ... Validation loss: 0.411\r",
      "Progress: 15.8% ... Training loss: 0.201 ... Validation loss: 0.400\r",
      "Progress: 15.8% ... Training loss: 0.203 ... Validation loss: 0.385\r",
      "Progress: 15.8% ... Training loss: 0.201 ... Validation loss: 0.394\r",
      "Progress: 15.8% ... Training loss: 0.201 ... Validation loss: 0.386\r",
      "Progress: 15.8% ... Training loss: 0.201 ... Validation loss: 0.394\r",
      "Progress: 15.9% ... Training loss: 0.200 ... Validation loss: 0.384\r",
      "Progress: 15.9% ... Training loss: 0.209 ... Validation loss: 0.404\r",
      "Progress: 15.9% ... Training loss: 0.204 ... Validation loss: 0.390\r",
      "Progress: 15.9% ... Training loss: 0.224 ... Validation loss: 0.372\r",
      "Progress: 15.9% ... Training loss: 0.220 ... Validation loss: 0.412\r",
      "Progress: 15.9% ... Training loss: 0.202 ... Validation loss: 0.375\r",
      "Progress: 15.9% ... Training loss: 0.203 ... Validation loss: 0.365\r",
      "Progress: 15.9% ... Training loss: 0.200 ... Validation loss: 0.374\r",
      "Progress: 16.0% ... Training loss: 0.199 ... Validation loss: 0.370\r",
      "Progress: 16.0% ... Training loss: 0.200 ... Validation loss: 0.380\r",
      "Progress: 16.0% ... Training loss: 0.202 ... Validation loss: 0.371\r",
      "Progress: 16.0% ... Training loss: 0.201 ... Validation loss: 0.375\r",
      "Progress: 16.0% ... Training loss: 0.202 ... Validation loss: 0.378\r",
      "Progress: 16.0% ... Training loss: 0.209 ... Validation loss: 0.397\r",
      "Progress: 16.0% ... Training loss: 0.210 ... Validation loss: 0.375\r",
      "Progress: 16.1% ... Training loss: 0.214 ... Validation loss: 0.396\r",
      "Progress: 16.1% ... Training loss: 0.211 ... Validation loss: 0.372\r",
      "Progress: 16.1% ... Training loss: 0.200 ... Validation loss: 0.370\r",
      "Progress: 16.1% ... Training loss: 0.223 ... Validation loss: 0.388\r",
      "Progress: 16.1% ... Training loss: 0.218 ... Validation loss: 0.372\r",
      "Progress: 16.1% ... Training loss: 0.207 ... Validation loss: 0.381\r",
      "Progress: 16.1% ... Training loss: 0.221 ... Validation loss: 0.378\r",
      "Progress: 16.1% ... Training loss: 0.198 ... Validation loss: 0.375\r",
      "Progress: 16.1% ... Training loss: 0.199 ... Validation loss: 0.369\r",
      "Progress: 16.2% ... Training loss: 0.200 ... Validation loss: 0.367\r",
      "Progress: 16.2% ... Training loss: 0.199 ... Validation loss: 0.373\r",
      "Progress: 16.2% ... Training loss: 0.201 ... Validation loss: 0.374\r",
      "Progress: 16.2% ... Training loss: 0.225 ... Validation loss: 0.380\r",
      "Progress: 16.2% ... Training loss: 0.210 ... Validation loss: 0.386\r",
      "Progress: 16.2% ... Training loss: 0.201 ... Validation loss: 0.370\r",
      "Progress: 16.2% ... Training loss: 0.201 ... Validation loss: 0.377\r",
      "Progress: 16.2% ... Training loss: 0.209 ... Validation loss: 0.376\r",
      "Progress: 16.3% ... Training loss: 0.197 ... Validation loss: 0.374\r",
      "Progress: 16.3% ... Training loss: 0.205 ... Validation loss: 0.386\r",
      "Progress: 16.3% ... Training loss: 0.202 ... Validation loss: 0.372\r",
      "Progress: 16.3% ... Training loss: 0.211 ... Validation loss: 0.394\r",
      "Progress: 16.3% ... Training loss: 0.201 ... Validation loss: 0.367\r",
      "Progress: 16.3% ... Training loss: 0.200 ... Validation loss: 0.376\r",
      "Progress: 16.3% ... Training loss: 0.207 ... Validation loss: 0.387\r",
      "Progress: 16.4% ... Training loss: 0.197 ... Validation loss: 0.373\r",
      "Progress: 16.4% ... Training loss: 0.200 ... Validation loss: 0.383\r",
      "Progress: 16.4% ... Training loss: 0.202 ... Validation loss: 0.373\r",
      "Progress: 16.4% ... Training loss: 0.228 ... Validation loss: 0.408\r",
      "Progress: 16.4% ... Training loss: 0.207 ... Validation loss: 0.367\r",
      "Progress: 16.4% ... Training loss: 0.199 ... Validation loss: 0.382\r",
      "Progress: 16.4% ... Training loss: 0.196 ... Validation loss: 0.373\r",
      "Progress: 16.4% ... Training loss: 0.197 ... Validation loss: 0.372\r",
      "Progress: 16.4% ... Training loss: 0.201 ... Validation loss: 0.377\r",
      "Progress: 16.5% ... Training loss: 0.204 ... Validation loss: 0.370\r",
      "Progress: 16.5% ... Training loss: 0.197 ... Validation loss: 0.376\r",
      "Progress: 16.5% ... Training loss: 0.215 ... Validation loss: 0.375\r",
      "Progress: 16.5% ... Training loss: 0.210 ... Validation loss: 0.392\r",
      "Progress: 16.5% ... Training loss: 0.226 ... Validation loss: 0.378\r",
      "Progress: 16.5% ... Training loss: 0.243 ... Validation loss: 0.399\r",
      "Progress: 16.5% ... Training loss: 0.218 ... Validation loss: 0.376\r",
      "Progress: 16.6% ... Training loss: 0.196 ... Validation loss: 0.369\r",
      "Progress: 16.6% ... Training loss: 0.198 ... Validation loss: 0.367\r",
      "Progress: 16.6% ... Training loss: 0.195 ... Validation loss: 0.373\r",
      "Progress: 16.6% ... Training loss: 0.203 ... Validation loss: 0.380\r",
      "Progress: 16.6% ... Training loss: 0.197 ... Validation loss: 0.371\r",
      "Progress: 16.6% ... Training loss: 0.195 ... Validation loss: 0.368\r",
      "Progress: 16.6% ... Training loss: 0.196 ... Validation loss: 0.378\r",
      "Progress: 16.6% ... Training loss: 0.201 ... Validation loss: 0.366\r",
      "Progress: 16.6% ... Training loss: 0.201 ... Validation loss: 0.385\r",
      "Progress: 16.7% ... Training loss: 0.232 ... Validation loss: 0.379\r",
      "Progress: 16.7% ... Training loss: 0.195 ... Validation loss: 0.377\r",
      "Progress: 16.7% ... Training loss: 0.194 ... Validation loss: 0.367\r",
      "Progress: 16.7% ... Training loss: 0.196 ... Validation loss: 0.366\r",
      "Progress: 16.7% ... Training loss: 0.198 ... Validation loss: 0.368\r",
      "Progress: 16.7% ... Training loss: 0.198 ... Validation loss: 0.371\r",
      "Progress: 16.7% ... Training loss: 0.193 ... Validation loss: 0.365\r",
      "Progress: 16.8% ... Training loss: 0.194 ... Validation loss: 0.365\r",
      "Progress: 16.8% ... Training loss: 0.196 ... Validation loss: 0.365\r",
      "Progress: 16.8% ... Training loss: 0.202 ... Validation loss: 0.367\r",
      "Progress: 16.8% ... Training loss: 0.194 ... Validation loss: 0.374\r",
      "Progress: 16.8% ... Training loss: 0.194 ... Validation loss: 0.366\r",
      "Progress: 16.8% ... Training loss: 0.197 ... Validation loss: 0.362\r",
      "Progress: 16.8% ... Training loss: 0.196 ... Validation loss: 0.379\r",
      "Progress: 16.8% ... Training loss: 0.196 ... Validation loss: 0.363\r",
      "Progress: 16.9% ... Training loss: 0.193 ... Validation loss: 0.363\r",
      "Progress: 16.9% ... Training loss: 0.198 ... Validation loss: 0.371\r",
      "Progress: 16.9% ... Training loss: 0.197 ... Validation loss: 0.366\r",
      "Progress: 16.9% ... Training loss: 0.196 ... Validation loss: 0.375\r",
      "Progress: 16.9% ... Training loss: 0.196 ... Validation loss: 0.363\r",
      "Progress: 16.9% ... Training loss: 0.193 ... Validation loss: 0.369\r",
      "Progress: 16.9% ... Training loss: 0.198 ... Validation loss: 0.369\r",
      "Progress: 16.9% ... Training loss: 0.209 ... Validation loss: 0.363\r",
      "Progress: 16.9% ... Training loss: 0.209 ... Validation loss: 0.383\r",
      "Progress: 17.0% ... Training loss: 0.205 ... Validation loss: 0.363\r",
      "Progress: 17.0% ... Training loss: 0.194 ... Validation loss: 0.367\r",
      "Progress: 17.0% ... Training loss: 0.194 ... Validation loss: 0.363\r",
      "Progress: 17.0% ... Training loss: 0.194 ... Validation loss: 0.358\r",
      "Progress: 17.0% ... Training loss: 0.203 ... Validation loss: 0.365\r",
      "Progress: 17.0% ... Training loss: 0.192 ... Validation loss: 0.367\r",
      "Progress: 17.0% ... Training loss: 0.201 ... Validation loss: 0.366\r",
      "Progress: 17.1% ... Training loss: 0.194 ... Validation loss: 0.368\r",
      "Progress: 17.1% ... Training loss: 0.195 ... Validation loss: 0.384\r",
      "Progress: 17.1% ... Training loss: 0.203 ... Validation loss: 0.371\r",
      "Progress: 17.1% ... Training loss: 0.212 ... Validation loss: 0.401\r",
      "Progress: 17.1% ... Training loss: 0.197 ... Validation loss: 0.361\r",
      "Progress: 17.1% ... Training loss: 0.197 ... Validation loss: 0.382\r",
      "Progress: 17.1% ... Training loss: 0.200 ... Validation loss: 0.367\r",
      "Progress: 17.1% ... Training loss: 0.193 ... Validation loss: 0.375\r",
      "Progress: 17.1% ... Training loss: 0.198 ... Validation loss: 0.363\r",
      "Progress: 17.2% ... Training loss: 0.191 ... Validation loss: 0.372\r",
      "Progress: 17.2% ... Training loss: 0.198 ... Validation loss: 0.365\r",
      "Progress: 17.2% ... Training loss: 0.194 ... Validation loss: 0.380\r",
      "Progress: 17.2% ... Training loss: 0.191 ... Validation loss: 0.365\r",
      "Progress: 17.2% ... Training loss: 0.194 ... Validation loss: 0.362\r",
      "Progress: 17.2% ... Training loss: 0.190 ... Validation loss: 0.363\r",
      "Progress: 17.2% ... Training loss: 0.193 ... Validation loss: 0.364\r",
      "Progress: 17.2% ... Training loss: 0.212 ... Validation loss: 0.400\r",
      "Progress: 17.3% ... Training loss: 0.191 ... Validation loss: 0.364\r",
      "Progress: 17.3% ... Training loss: 0.190 ... Validation loss: 0.360\r",
      "Progress: 17.3% ... Training loss: 0.192 ... Validation loss: 0.373\r",
      "Progress: 17.3% ... Training loss: 0.193 ... Validation loss: 0.357\r",
      "Progress: 17.3% ... Training loss: 0.197 ... Validation loss: 0.359\r",
      "Progress: 17.3% ... Training loss: 0.200 ... Validation loss: 0.382\r",
      "Progress: 17.3% ... Training loss: 0.205 ... Validation loss: 0.362\r",
      "Progress: 17.4% ... Training loss: 0.195 ... Validation loss: 0.373\r",
      "Progress: 17.4% ... Training loss: 0.191 ... Validation loss: 0.352\r",
      "Progress: 17.4% ... Training loss: 0.190 ... Validation loss: 0.351\r",
      "Progress: 17.4% ... Training loss: 0.190 ... Validation loss: 0.352\r",
      "Progress: 17.4% ... Training loss: 0.197 ... Validation loss: 0.369\r",
      "Progress: 17.4% ... Training loss: 0.189 ... Validation loss: 0.356\r",
      "Progress: 17.4% ... Training loss: 0.190 ... Validation loss: 0.354\r",
      "Progress: 17.4% ... Training loss: 0.197 ... Validation loss: 0.354\r",
      "Progress: 17.4% ... Training loss: 0.204 ... Validation loss: 0.380\r",
      "Progress: 17.5% ... Training loss: 0.227 ... Validation loss: 0.370\r",
      "Progress: 17.5% ... Training loss: 0.206 ... Validation loss: 0.370\r",
      "Progress: 17.5% ... Training loss: 0.191 ... Validation loss: 0.350\r",
      "Progress: 17.5% ... Training loss: 0.188 ... Validation loss: 0.357\r",
      "Progress: 17.5% ... Training loss: 0.188 ... Validation loss: 0.352\r",
      "Progress: 17.5% ... Training loss: 0.189 ... Validation loss: 0.360\r",
      "Progress: 17.5% ... Training loss: 0.191 ... Validation loss: 0.355\r",
      "Progress: 17.6% ... Training loss: 0.188 ... Validation loss: 0.354\r",
      "Progress: 17.6% ... Training loss: 0.192 ... Validation loss: 0.353\r",
      "Progress: 17.6% ... Training loss: 0.201 ... Validation loss: 0.363\r",
      "Progress: 17.6% ... Training loss: 0.188 ... Validation loss: 0.359\r",
      "Progress: 17.6% ... Training loss: 0.187 ... Validation loss: 0.361\r",
      "Progress: 17.6% ... Training loss: 0.188 ... Validation loss: 0.354\r",
      "Progress: 17.6% ... Training loss: 0.187 ... Validation loss: 0.354\r",
      "Progress: 17.6% ... Training loss: 0.204 ... Validation loss: 0.393\r",
      "Progress: 17.6% ... Training loss: 0.187 ... Validation loss: 0.367\r",
      "Progress: 17.7% ... Training loss: 0.187 ... Validation loss: 0.360\r",
      "Progress: 17.7% ... Training loss: 0.186 ... Validation loss: 0.362\r",
      "Progress: 17.7% ... Training loss: 0.189 ... Validation loss: 0.377\r",
      "Progress: 17.7% ... Training loss: 0.189 ... Validation loss: 0.364\r",
      "Progress: 17.7% ... Training loss: 0.187 ... Validation loss: 0.374\r",
      "Progress: 17.7% ... Training loss: 0.186 ... Validation loss: 0.364\r",
      "Progress: 17.7% ... Training loss: 0.189 ... Validation loss: 0.363\r",
      "Progress: 17.8% ... Training loss: 0.201 ... Validation loss: 0.382\r",
      "Progress: 17.8% ... Training loss: 0.187 ... Validation loss: 0.360\r",
      "Progress: 17.8% ... Training loss: 0.190 ... Validation loss: 0.387\r",
      "Progress: 17.8% ... Training loss: 0.197 ... Validation loss: 0.354\r",
      "Progress: 17.8% ... Training loss: 0.188 ... Validation loss: 0.374\r",
      "Progress: 17.8% ... Training loss: 0.188 ... Validation loss: 0.367\r",
      "Progress: 17.8% ... Training loss: 0.189 ... Validation loss: 0.361\r",
      "Progress: 17.8% ... Training loss: 0.187 ... Validation loss: 0.377\r",
      "Progress: 17.9% ... Training loss: 0.188 ... Validation loss: 0.375\r",
      "Progress: 17.9% ... Training loss: 0.188 ... Validation loss: 0.367\r",
      "Progress: 17.9% ... Training loss: 0.198 ... Validation loss: 0.351\r",
      "Progress: 17.9% ... Training loss: 0.196 ... Validation loss: 0.360\r",
      "Progress: 17.9% ... Training loss: 0.199 ... Validation loss: 0.396\r",
      "Progress: 17.9% ... Training loss: 0.186 ... Validation loss: 0.367\r",
      "Progress: 17.9% ... Training loss: 0.185 ... Validation loss: 0.359\r",
      "Progress: 17.9% ... Training loss: 0.190 ... Validation loss: 0.379\r",
      "Progress: 17.9% ... Training loss: 0.187 ... Validation loss: 0.368\r",
      "Progress: 18.0% ... Training loss: 0.186 ... Validation loss: 0.366\r",
      "Progress: 18.0% ... Training loss: 0.186 ... Validation loss: 0.350\r",
      "Progress: 18.0% ... Training loss: 0.185 ... Validation loss: 0.367\r",
      "Progress: 18.0% ... Training loss: 0.184 ... Validation loss: 0.354\r",
      "Progress: 18.0% ... Training loss: 0.184 ... Validation loss: 0.360\r",
      "Progress: 18.0% ... Training loss: 0.187 ... Validation loss: 0.366\r",
      "Progress: 18.0% ... Training loss: 0.188 ... Validation loss: 0.349\r",
      "Progress: 18.1% ... Training loss: 0.185 ... Validation loss: 0.365\r",
      "Progress: 18.1% ... Training loss: 0.184 ... Validation loss: 0.359\r",
      "Progress: 18.1% ... Training loss: 0.183 ... Validation loss: 0.355\r",
      "Progress: 18.1% ... Training loss: 0.202 ... Validation loss: 0.383\r",
      "Progress: 18.1% ... Training loss: 0.183 ... Validation loss: 0.351\r",
      "Progress: 18.1% ... Training loss: 0.193 ... Validation loss: 0.381\r",
      "Progress: 18.1% ... Training loss: 0.191 ... Validation loss: 0.344\r",
      "Progress: 18.1% ... Training loss: 0.186 ... Validation loss: 0.354\r",
      "Progress: 18.1% ... Training loss: 0.184 ... Validation loss: 0.343\r",
      "Progress: 18.2% ... Training loss: 0.182 ... Validation loss: 0.351\r",
      "Progress: 18.2% ... Training loss: 0.191 ... Validation loss: 0.343\r",
      "Progress: 18.2% ... Training loss: 0.187 ... Validation loss: 0.367\r",
      "Progress: 18.2% ... Training loss: 0.184 ... Validation loss: 0.343\r",
      "Progress: 18.2% ... Training loss: 0.183 ... Validation loss: 0.361\r",
      "Progress: 18.2% ... Training loss: 0.183 ... Validation loss: 0.354\r",
      "Progress: 18.2% ... Training loss: 0.185 ... Validation loss: 0.358\r",
      "Progress: 18.2% ... Training loss: 0.198 ... Validation loss: 0.345\r",
      "Progress: 18.3% ... Training loss: 0.183 ... Validation loss: 0.358\r",
      "Progress: 18.3% ... Training loss: 0.188 ... Validation loss: 0.353\r",
      "Progress: 18.3% ... Training loss: 0.194 ... Validation loss: 0.355\r",
      "Progress: 18.3% ... Training loss: 0.186 ... Validation loss: 0.369\r",
      "Progress: 18.3% ... Training loss: 0.188 ... Validation loss: 0.356\r",
      "Progress: 18.3% ... Training loss: 0.185 ... Validation loss: 0.371\r",
      "Progress: 18.3% ... Training loss: 0.207 ... Validation loss: 0.351\r",
      "Progress: 18.4% ... Training loss: 0.252 ... Validation loss: 0.423\r",
      "Progress: 18.4% ... Training loss: 0.198 ... Validation loss: 0.348\r",
      "Progress: 18.4% ... Training loss: 0.226 ... Validation loss: 0.423\r",
      "Progress: 18.4% ... Training loss: 0.184 ... Validation loss: 0.354\r",
      "Progress: 18.4% ... Training loss: 0.186 ... Validation loss: 0.362\r",
      "Progress: 18.4% ... Training loss: 0.185 ... Validation loss: 0.368\r",
      "Progress: 18.4% ... Training loss: 0.186 ... Validation loss: 0.371\r",
      "Progress: 18.4% ... Training loss: 0.186 ... Validation loss: 0.346\r",
      "Progress: 18.4% ... Training loss: 0.191 ... Validation loss: 0.380\r",
      "Progress: 18.5% ... Training loss: 0.192 ... Validation loss: 0.351\r",
      "Progress: 18.5% ... Training loss: 0.186 ... Validation loss: 0.369\r",
      "Progress: 18.5% ... Training loss: 0.184 ... Validation loss: 0.385\r",
      "Progress: 18.5% ... Training loss: 0.184 ... Validation loss: 0.355\r",
      "Progress: 18.5% ... Training loss: 0.182 ... Validation loss: 0.354\r",
      "Progress: 18.5% ... Training loss: 0.181 ... Validation loss: 0.355\r",
      "Progress: 18.5% ... Training loss: 0.186 ... Validation loss: 0.379\r",
      "Progress: 18.6% ... Training loss: 0.182 ... Validation loss: 0.370\r",
      "Progress: 18.6% ... Training loss: 0.184 ... Validation loss: 0.356\r",
      "Progress: 18.6% ... Training loss: 0.182 ... Validation loss: 0.370\r",
      "Progress: 18.6% ... Training loss: 0.182 ... Validation loss: 0.348\r",
      "Progress: 18.6% ... Training loss: 0.180 ... Validation loss: 0.360\r",
      "Progress: 18.6% ... Training loss: 0.181 ... Validation loss: 0.351\r",
      "Progress: 18.6% ... Training loss: 0.183 ... Validation loss: 0.342\r",
      "Progress: 18.6% ... Training loss: 0.180 ... Validation loss: 0.352\r",
      "Progress: 18.6% ... Training loss: 0.185 ... Validation loss: 0.369\r",
      "Progress: 18.7% ... Training loss: 0.179 ... Validation loss: 0.359\r",
      "Progress: 18.7% ... Training loss: 0.180 ... Validation loss: 0.351\r",
      "Progress: 18.7% ... Training loss: 0.200 ... Validation loss: 0.392\r",
      "Progress: 18.7% ... Training loss: 0.190 ... Validation loss: 0.342\r",
      "Progress: 18.7% ... Training loss: 0.179 ... Validation loss: 0.358\r",
      "Progress: 18.7% ... Training loss: 0.184 ... Validation loss: 0.369\r",
      "Progress: 18.7% ... Training loss: 0.179 ... Validation loss: 0.353\r",
      "Progress: 18.8% ... Training loss: 0.181 ... Validation loss: 0.348\r",
      "Progress: 18.8% ... Training loss: 0.184 ... Validation loss: 0.356\r",
      "Progress: 18.8% ... Training loss: 0.180 ... Validation loss: 0.345\r",
      "Progress: 18.8% ... Training loss: 0.184 ... Validation loss: 0.343\r",
      "Progress: 18.8% ... Training loss: 0.181 ... Validation loss: 0.361\r",
      "Progress: 18.8% ... Training loss: 0.184 ... Validation loss: 0.347\r",
      "Progress: 18.8% ... Training loss: 0.180 ... Validation loss: 0.357\r",
      "Progress: 18.8% ... Training loss: 0.179 ... Validation loss: 0.359\r",
      "Progress: 18.9% ... Training loss: 0.182 ... Validation loss: 0.344\r",
      "Progress: 18.9% ... Training loss: 0.180 ... Validation loss: 0.365\r",
      "Progress: 18.9% ... Training loss: 0.178 ... Validation loss: 0.346\r",
      "Progress: 18.9% ... Training loss: 0.195 ... Validation loss: 0.377\r",
      "Progress: 18.9% ... Training loss: 0.183 ... Validation loss: 0.341\r",
      "Progress: 18.9% ... Training loss: 0.184 ... Validation loss: 0.363\r",
      "Progress: 18.9% ... Training loss: 0.182 ... Validation loss: 0.340\r",
      "Progress: 18.9% ... Training loss: 0.185 ... Validation loss: 0.349\r",
      "Progress: 18.9% ... Training loss: 0.193 ... Validation loss: 0.362\r",
      "Progress: 19.0% ... Training loss: 0.183 ... Validation loss: 0.332\r",
      "Progress: 19.0% ... Training loss: 0.179 ... Validation loss: 0.342\r",
      "Progress: 19.0% ... Training loss: 0.177 ... Validation loss: 0.345\r",
      "Progress: 19.0% ... Training loss: 0.181 ... Validation loss: 0.356\r",
      "Progress: 19.0% ... Training loss: 0.188 ... Validation loss: 0.340\r",
      "Progress: 19.0% ... Training loss: 0.187 ... Validation loss: 0.373\r",
      "Progress: 19.0% ... Training loss: 0.182 ... Validation loss: 0.340\r",
      "Progress: 19.1% ... Training loss: 0.177 ... Validation loss: 0.355\r",
      "Progress: 19.1% ... Training loss: 0.176 ... Validation loss: 0.356\r",
      "Progress: 19.1% ... Training loss: 0.186 ... Validation loss: 0.342\r",
      "Progress: 19.1% ... Training loss: 0.191 ... Validation loss: 0.396\r",
      "Progress: 19.1% ... Training loss: 0.180 ... Validation loss: 0.340\r",
      "Progress: 19.1% ... Training loss: 0.202 ... Validation loss: 0.387\r",
      "Progress: 19.1% ... Training loss: 0.179 ... Validation loss: 0.337\r",
      "Progress: 19.1% ... Training loss: 0.197 ... Validation loss: 0.392\r",
      "Progress: 19.1% ... Training loss: 0.183 ... Validation loss: 0.335\r",
      "Progress: 19.2% ... Training loss: 0.178 ... Validation loss: 0.359\r",
      "Progress: 19.2% ... Training loss: 0.180 ... Validation loss: 0.347\r",
      "Progress: 19.2% ... Training loss: 0.180 ... Validation loss: 0.369\r",
      "Progress: 19.2% ... Training loss: 0.176 ... Validation loss: 0.357\r",
      "Progress: 19.2% ... Training loss: 0.177 ... Validation loss: 0.351\r",
      "Progress: 19.2% ... Training loss: 0.175 ... Validation loss: 0.346\r",
      "Progress: 19.2% ... Training loss: 0.175 ... Validation loss: 0.348\r",
      "Progress: 19.2% ... Training loss: 0.193 ... Validation loss: 0.361\r",
      "Progress: 19.3% ... Training loss: 0.212 ... Validation loss: 0.346\r",
      "Progress: 19.3% ... Training loss: 0.193 ... Validation loss: 0.369\r",
      "Progress: 19.3% ... Training loss: 0.210 ... Validation loss: 0.347\r",
      "Progress: 19.3% ... Training loss: 0.177 ... Validation loss: 0.361\r",
      "Progress: 19.3% ... Training loss: 0.183 ... Validation loss: 0.342\r",
      "Progress: 19.3% ... Training loss: 0.178 ... Validation loss: 0.369\r",
      "Progress: 19.3% ... Training loss: 0.181 ... Validation loss: 0.352\r",
      "Progress: 19.4% ... Training loss: 0.182 ... Validation loss: 0.364\r",
      "Progress: 19.4% ... Training loss: 0.183 ... Validation loss: 0.339\r",
      "Progress: 19.4% ... Training loss: 0.178 ... Validation loss: 0.357\r",
      "Progress: 19.4% ... Training loss: 0.175 ... Validation loss: 0.354\r",
      "Progress: 19.4% ... Training loss: 0.174 ... Validation loss: 0.371\r",
      "Progress: 19.4% ... Training loss: 0.176 ... Validation loss: 0.365\r",
      "Progress: 19.4% ... Training loss: 0.176 ... Validation loss: 0.359\r",
      "Progress: 19.4% ... Training loss: 0.174 ... Validation loss: 0.367\r",
      "Progress: 19.4% ... Training loss: 0.176 ... Validation loss: 0.366\r",
      "Progress: 19.5% ... Training loss: 0.179 ... Validation loss: 0.372\r",
      "Progress: 19.5% ... Training loss: 0.189 ... Validation loss: 0.346\r",
      "Progress: 19.5% ... Training loss: 0.178 ... Validation loss: 0.381\r",
      "Progress: 19.5% ... Training loss: 0.173 ... Validation loss: 0.349\r",
      "Progress: 19.5% ... Training loss: 0.186 ... Validation loss: 0.341\r",
      "Progress: 19.5% ... Training loss: 0.174 ... Validation loss: 0.354\r",
      "Progress: 19.5% ... Training loss: 0.174 ... Validation loss: 0.356\r",
      "Progress: 19.6% ... Training loss: 0.186 ... Validation loss: 0.361\r",
      "Progress: 19.6% ... Training loss: 0.179 ... Validation loss: 0.334\r",
      "Progress: 19.6% ... Training loss: 0.185 ... Validation loss: 0.373\r",
      "Progress: 19.6% ... Training loss: 0.177 ... Validation loss: 0.334\r",
      "Progress: 19.6% ... Training loss: 0.218 ... Validation loss: 0.393\r",
      "Progress: 19.6% ... Training loss: 0.174 ... Validation loss: 0.337\r",
      "Progress: 19.6% ... Training loss: 0.177 ... Validation loss: 0.335\r",
      "Progress: 19.6% ... Training loss: 0.177 ... Validation loss: 0.339\r",
      "Progress: 19.6% ... Training loss: 0.175 ... Validation loss: 0.340\r",
      "Progress: 19.7% ... Training loss: 0.179 ... Validation loss: 0.340\r",
      "Progress: 19.7% ... Training loss: 0.177 ... Validation loss: 0.359\r",
      "Progress: 19.7% ... Training loss: 0.181 ... Validation loss: 0.363\r",
      "Progress: 19.7% ... Training loss: 0.174 ... Validation loss: 0.341\r",
      "Progress: 19.7% ... Training loss: 0.181 ... Validation loss: 0.336\r",
      "Progress: 19.7% ... Training loss: 0.176 ... Validation loss: 0.371\r",
      "Progress: 19.7% ... Training loss: 0.172 ... Validation loss: 0.349\r",
      "Progress: 19.8% ... Training loss: 0.172 ... Validation loss: 0.346\r",
      "Progress: 19.8% ... Training loss: 0.191 ... Validation loss: 0.377\r",
      "Progress: 19.8% ... Training loss: 0.185 ... Validation loss: 0.345\r",
      "Progress: 19.8% ... Training loss: 0.215 ... Validation loss: 0.403\r",
      "Progress: 19.8% ... Training loss: 0.189 ... Validation loss: 0.339\r",
      "Progress: 19.8% ... Training loss: 0.181 ... Validation loss: 0.366\r",
      "Progress: 19.8% ... Training loss: 0.176 ... Validation loss: 0.328\r",
      "Progress: 19.8% ... Training loss: 0.226 ... Validation loss: 0.381\r",
      "Progress: 19.9% ... Training loss: 0.258 ... Validation loss: 0.370\r",
      "Progress: 19.9% ... Training loss: 0.230 ... Validation loss: 0.426\r",
      "Progress: 19.9% ... Training loss: 0.172 ... Validation loss: 0.346\r",
      "Progress: 19.9% ... Training loss: 0.171 ... Validation loss: 0.343\r",
      "Progress: 19.9% ... Training loss: 0.171 ... Validation loss: 0.339\r",
      "Progress: 19.9% ... Training loss: 0.170 ... Validation loss: 0.330\r",
      "Progress: 19.9% ... Training loss: 0.172 ... Validation loss: 0.342\r",
      "Progress: 19.9% ... Training loss: 0.192 ... Validation loss: 0.337\r",
      "Progress: 19.9% ... Training loss: 0.197 ... Validation loss: 0.378\r",
      "Progress: 20.0% ... Training loss: 0.181 ... Validation loss: 0.331\r",
      "Progress: 20.0% ... Training loss: 0.174 ... Validation loss: 0.348\r",
      "Progress: 20.0% ... Training loss: 0.186 ... Validation loss: 0.332\r",
      "Progress: 20.0% ... Training loss: 0.181 ... Validation loss: 0.369\r",
      "Progress: 20.0% ... Training loss: 0.169 ... Validation loss: 0.340\r",
      "Progress: 20.0% ... Training loss: 0.172 ... Validation loss: 0.361\r",
      "Progress: 20.0% ... Training loss: 0.170 ... Validation loss: 0.345\r",
      "Progress: 20.1% ... Training loss: 0.169 ... Validation loss: 0.340\r",
      "Progress: 20.1% ... Training loss: 0.180 ... Validation loss: 0.332\r",
      "Progress: 20.1% ... Training loss: 0.172 ... Validation loss: 0.350\r",
      "Progress: 20.1% ... Training loss: 0.172 ... Validation loss: 0.351\r",
      "Progress: 20.1% ... Training loss: 0.171 ... Validation loss: 0.339\r",
      "Progress: 20.1% ... Training loss: 0.170 ... Validation loss: 0.346\r",
      "Progress: 20.1% ... Training loss: 0.171 ... Validation loss: 0.356\r",
      "Progress: 20.1% ... Training loss: 0.170 ... Validation loss: 0.349\r",
      "Progress: 20.1% ... Training loss: 0.177 ... Validation loss: 0.337\r",
      "Progress: 20.2% ... Training loss: 0.169 ... Validation loss: 0.343\r",
      "Progress: 20.2% ... Training loss: 0.172 ... Validation loss: 0.352\r",
      "Progress: 20.2% ... Training loss: 0.168 ... Validation loss: 0.339\r",
      "Progress: 20.2% ... Training loss: 0.174 ... Validation loss: 0.335\r",
      "Progress: 20.2% ... Training loss: 0.168 ... Validation loss: 0.352\r",
      "Progress: 20.2% ... Training loss: 0.182 ... Validation loss: 0.334\r",
      "Progress: 20.2% ... Training loss: 0.175 ... Validation loss: 0.376\r",
      "Progress: 20.2% ... Training loss: 0.170 ... Validation loss: 0.361\r",
      "Progress: 20.3% ... Training loss: 0.170 ... Validation loss: 0.366\r",
      "Progress: 20.3% ... Training loss: 0.170 ... Validation loss: 0.348\r",
      "Progress: 20.3% ... Training loss: 0.168 ... Validation loss: 0.341\r",
      "Progress: 20.3% ... Training loss: 0.169 ... Validation loss: 0.344\r",
      "Progress: 20.3% ... Training loss: 0.169 ... Validation loss: 0.355\r",
      "Progress: 20.3% ... Training loss: 0.182 ... Validation loss: 0.327\r",
      "Progress: 20.3% ... Training loss: 0.217 ... Validation loss: 0.410\r",
      "Progress: 20.4% ... Training loss: 0.190 ... Validation loss: 0.325\r",
      "Progress: 20.4% ... Training loss: 0.176 ... Validation loss: 0.350\r",
      "Progress: 20.4% ... Training loss: 0.175 ... Validation loss: 0.331\r",
      "Progress: 20.4% ... Training loss: 0.168 ... Validation loss: 0.344\r",
      "Progress: 20.4% ... Training loss: 0.173 ... Validation loss: 0.335\r",
      "Progress: 20.4% ... Training loss: 0.188 ... Validation loss: 0.383\r",
      "Progress: 20.4% ... Training loss: 0.173 ... Validation loss: 0.327\r",
      "Progress: 20.4% ... Training loss: 0.175 ... Validation loss: 0.358\r",
      "Progress: 20.4% ... Training loss: 0.183 ... Validation loss: 0.329\r",
      "Progress: 20.5% ... Training loss: 0.192 ... Validation loss: 0.370\r",
      "Progress: 20.5% ... Training loss: 0.176 ... Validation loss: 0.324\r",
      "Progress: 20.5% ... Training loss: 0.174 ... Validation loss: 0.347\r",
      "Progress: 20.5% ... Training loss: 0.169 ... Validation loss: 0.322\r",
      "Progress: 20.5% ... Training loss: 0.172 ... Validation loss: 0.342\r",
      "Progress: 20.5% ... Training loss: 0.179 ... Validation loss: 0.325\r",
      "Progress: 20.5% ... Training loss: 0.176 ... Validation loss: 0.342\r",
      "Progress: 20.6% ... Training loss: 0.174 ... Validation loss: 0.327\r",
      "Progress: 20.6% ... Training loss: 0.166 ... Validation loss: 0.339\r",
      "Progress: 20.6% ... Training loss: 0.165 ... Validation loss: 0.339\r",
      "Progress: 20.6% ... Training loss: 0.167 ... Validation loss: 0.334\r",
      "Progress: 20.6% ... Training loss: 0.165 ... Validation loss: 0.332\r",
      "Progress: 20.6% ... Training loss: 0.165 ... Validation loss: 0.324\r",
      "Progress: 20.6% ... Training loss: 0.173 ... Validation loss: 0.325\r",
      "Progress: 20.6% ... Training loss: 0.165 ... Validation loss: 0.326\r",
      "Progress: 20.6% ... Training loss: 0.175 ... Validation loss: 0.351\r",
      "Progress: 20.7% ... Training loss: 0.175 ... Validation loss: 0.329\r",
      "Progress: 20.7% ... Training loss: 0.166 ... Validation loss: 0.330\r",
      "Progress: 20.7% ... Training loss: 0.174 ... Validation loss: 0.330\r",
      "Progress: 20.7% ... Training loss: 0.165 ... Validation loss: 0.336\r",
      "Progress: 20.7% ... Training loss: 0.165 ... Validation loss: 0.337\r",
      "Progress: 20.7% ... Training loss: 0.175 ... Validation loss: 0.336\r",
      "Progress: 20.7% ... Training loss: 0.167 ... Validation loss: 0.346\r",
      "Progress: 20.8% ... Training loss: 0.170 ... Validation loss: 0.345\r",
      "Progress: 20.8% ... Training loss: 0.172 ... Validation loss: 0.325\r",
      "Progress: 20.8% ... Training loss: 0.167 ... Validation loss: 0.343\r",
      "Progress: 20.8% ... Training loss: 0.174 ... Validation loss: 0.325\r",
      "Progress: 20.8% ... Training loss: 0.175 ... Validation loss: 0.361\r",
      "Progress: 20.8% ... Training loss: 0.174 ... Validation loss: 0.329\r",
      "Progress: 20.8% ... Training loss: 0.179 ... Validation loss: 0.396\r",
      "Progress: 20.8% ... Training loss: 0.182 ... Validation loss: 0.321\r",
      "Progress: 20.9% ... Training loss: 0.183 ... Validation loss: 0.367\r",
      "Progress: 20.9% ... Training loss: 0.177 ... Validation loss: 0.325\r",
      "Progress: 20.9% ... Training loss: 0.187 ... Validation loss: 0.381\r",
      "Progress: 20.9% ... Training loss: 0.165 ... Validation loss: 0.325\r",
      "Progress: 20.9% ... Training loss: 0.164 ... Validation loss: 0.327\r",
      "Progress: 20.9% ... Training loss: 0.164 ... Validation loss: 0.325\r",
      "Progress: 20.9% ... Training loss: 0.165 ... Validation loss: 0.344\r",
      "Progress: 20.9% ... Training loss: 0.164 ... Validation loss: 0.344\r",
      "Progress: 20.9% ... Training loss: 0.171 ... Validation loss: 0.342\r",
      "Progress: 21.0% ... Training loss: 0.166 ... Validation loss: 0.361\r",
      "Progress: 21.0% ... Training loss: 0.172 ... Validation loss: 0.358\r",
      "Progress: 21.0% ... Training loss: 0.164 ... Validation loss: 0.318\r",
      "Progress: 21.0% ... Training loss: 0.165 ... Validation loss: 0.323\r",
      "Progress: 21.0% ... Training loss: 0.162 ... Validation loss: 0.329\r",
      "Progress: 21.0% ... Training loss: 0.162 ... Validation loss: 0.343\r",
      "Progress: 21.0% ... Training loss: 0.161 ... Validation loss: 0.331\r",
      "Progress: 21.1% ... Training loss: 0.162 ... Validation loss: 0.328\r",
      "Progress: 21.1% ... Training loss: 0.163 ... Validation loss: 0.321\r",
      "Progress: 21.1% ... Training loss: 0.161 ... Validation loss: 0.335\r",
      "Progress: 21.1% ... Training loss: 0.161 ... Validation loss: 0.343\r",
      "Progress: 21.1% ... Training loss: 0.163 ... Validation loss: 0.352\r",
      "Progress: 21.1% ... Training loss: 0.170 ... Validation loss: 0.324\r",
      "Progress: 21.1% ... Training loss: 0.161 ... Validation loss: 0.347\r",
      "Progress: 21.1% ... Training loss: 0.166 ... Validation loss: 0.325\r",
      "Progress: 21.1% ... Training loss: 0.160 ... Validation loss: 0.329\r",
      "Progress: 21.2% ... Training loss: 0.162 ... Validation loss: 0.330\r",
      "Progress: 21.2% ... Training loss: 0.165 ... Validation loss: 0.344\r",
      "Progress: 21.2% ... Training loss: 0.160 ... Validation loss: 0.326\r",
      "Progress: 21.2% ... Training loss: 0.160 ... Validation loss: 0.334\r",
      "Progress: 21.2% ... Training loss: 0.162 ... Validation loss: 0.335\r",
      "Progress: 21.2% ... Training loss: 0.163 ... Validation loss: 0.329\r",
      "Progress: 21.2% ... Training loss: 0.160 ... Validation loss: 0.341\r",
      "Progress: 21.2% ... Training loss: 0.161 ... Validation loss: 0.333\r",
      "Progress: 21.3% ... Training loss: 0.168 ... Validation loss: 0.371\r",
      "Progress: 21.3% ... Training loss: 0.159 ... Validation loss: 0.331\r",
      "Progress: 21.3% ... Training loss: 0.160 ... Validation loss: 0.347\r",
      "Progress: 21.3% ... Training loss: 0.160 ... Validation loss: 0.353\r",
      "Progress: 21.3% ... Training loss: 0.160 ... Validation loss: 0.344\r",
      "Progress: 21.3% ... Training loss: 0.163 ... Validation loss: 0.360\r",
      "Progress: 21.3% ... Training loss: 0.164 ... Validation loss: 0.335\r",
      "Progress: 21.4% ... Training loss: 0.164 ... Validation loss: 0.329\r",
      "Progress: 21.4% ... Training loss: 0.160 ... Validation loss: 0.349\r",
      "Progress: 21.4% ... Training loss: 0.162 ... Validation loss: 0.325\r",
      "Progress: 21.4% ... Training loss: 0.169 ... Validation loss: 0.321\r",
      "Progress: 21.4% ... Training loss: 0.163 ... Validation loss: 0.344\r",
      "Progress: 21.4% ... Training loss: 0.163 ... Validation loss: 0.338\r",
      "Progress: 21.4% ... Training loss: 0.204 ... Validation loss: 0.322\r",
      "Progress: 21.4% ... Training loss: 0.190 ... Validation loss: 0.368\r",
      "Progress: 21.4% ... Training loss: 0.163 ... Validation loss: 0.313\r",
      "Progress: 21.5% ... Training loss: 0.160 ... Validation loss: 0.319\r",
      "Progress: 21.5% ... Training loss: 0.165 ... Validation loss: 0.346\r",
      "Progress: 21.5% ... Training loss: 0.164 ... Validation loss: 0.321\r",
      "Progress: 21.5% ... Training loss: 0.160 ... Validation loss: 0.348\r",
      "Progress: 21.5% ... Training loss: 0.162 ... Validation loss: 0.334\r",
      "Progress: 21.5% ... Training loss: 0.160 ... Validation loss: 0.354\r",
      "Progress: 21.5% ... Training loss: 0.160 ... Validation loss: 0.354\r",
      "Progress: 21.6% ... Training loss: 0.158 ... Validation loss: 0.340\r",
      "Progress: 21.6% ... Training loss: 0.165 ... Validation loss: 0.350\r",
      "Progress: 21.6% ... Training loss: 0.169 ... Validation loss: 0.311\r",
      "Progress: 21.6% ... Training loss: 0.158 ... Validation loss: 0.352\r",
      "Progress: 21.6% ... Training loss: 0.160 ... Validation loss: 0.335\r",
      "Progress: 21.6% ... Training loss: 0.158 ... Validation loss: 0.349\r",
      "Progress: 21.6% ... Training loss: 0.157 ... Validation loss: 0.338\r",
      "Progress: 21.6% ... Training loss: 0.183 ... Validation loss: 0.397\r",
      "Progress: 21.6% ... Training loss: 0.188 ... Validation loss: 0.317\r",
      "Progress: 21.7% ... Training loss: 0.161 ... Validation loss: 0.361\r",
      "Progress: 21.7% ... Training loss: 0.157 ... Validation loss: 0.328\r",
      "Progress: 21.7% ... Training loss: 0.158 ... Validation loss: 0.318\r",
      "Progress: 21.7% ... Training loss: 0.158 ... Validation loss: 0.318\r",
      "Progress: 21.7% ... Training loss: 0.161 ... Validation loss: 0.319\r",
      "Progress: 21.7% ... Training loss: 0.161 ... Validation loss: 0.346\r",
      "Progress: 21.7% ... Training loss: 0.158 ... Validation loss: 0.339\r",
      "Progress: 21.8% ... Training loss: 0.157 ... Validation loss: 0.325\r",
      "Progress: 21.8% ... Training loss: 0.164 ... Validation loss: 0.378\r",
      "Progress: 21.8% ... Training loss: 0.157 ... Validation loss: 0.336\r",
      "Progress: 21.8% ... Training loss: 0.156 ... Validation loss: 0.328\r",
      "Progress: 21.8% ... Training loss: 0.156 ... Validation loss: 0.331\r",
      "Progress: 21.8% ... Training loss: 0.157 ... Validation loss: 0.322\r",
      "Progress: 21.8% ... Training loss: 0.156 ... Validation loss: 0.315\r",
      "Progress: 21.8% ... Training loss: 0.158 ... Validation loss: 0.334\r",
      "Progress: 21.9% ... Training loss: 0.156 ... Validation loss: 0.328\r",
      "Progress: 21.9% ... Training loss: 0.155 ... Validation loss: 0.323\r",
      "Progress: 21.9% ... Training loss: 0.158 ... Validation loss: 0.324\r",
      "Progress: 21.9% ... Training loss: 0.157 ... Validation loss: 0.328\r",
      "Progress: 21.9% ... Training loss: 0.157 ... Validation loss: 0.328\r",
      "Progress: 21.9% ... Training loss: 0.157 ... Validation loss: 0.319\r",
      "Progress: 21.9% ... Training loss: 0.161 ... Validation loss: 0.326\r",
      "Progress: 21.9% ... Training loss: 0.160 ... Validation loss: 0.318\r",
      "Progress: 21.9% ... Training loss: 0.158 ... Validation loss: 0.337\r",
      "Progress: 22.0% ... Training loss: 0.158 ... Validation loss: 0.331\r",
      "Progress: 22.0% ... Training loss: 0.155 ... Validation loss: 0.326\r",
      "Progress: 22.0% ... Training loss: 0.157 ... Validation loss: 0.347\r",
      "Progress: 22.0% ... Training loss: 0.156 ... Validation loss: 0.331\r",
      "Progress: 22.0% ... Training loss: 0.156 ... Validation loss: 0.349\r",
      "Progress: 22.0% ... Training loss: 0.156 ... Validation loss: 0.351\r",
      "Progress: 22.0% ... Training loss: 0.156 ... Validation loss: 0.328\r",
      "Progress: 22.1% ... Training loss: 0.156 ... Validation loss: 0.335\r",
      "Progress: 22.1% ... Training loss: 0.161 ... Validation loss: 0.326\r",
      "Progress: 22.1% ... Training loss: 0.155 ... Validation loss: 0.349\r",
      "Progress: 22.1% ... Training loss: 0.154 ... Validation loss: 0.332\r",
      "Progress: 22.1% ... Training loss: 0.155 ... Validation loss: 0.336\r",
      "Progress: 22.1% ... Training loss: 0.154 ... Validation loss: 0.330\r",
      "Progress: 22.1% ... Training loss: 0.157 ... Validation loss: 0.351\r",
      "Progress: 22.1% ... Training loss: 0.173 ... Validation loss: 0.312\r",
      "Progress: 22.1% ... Training loss: 0.172 ... Validation loss: 0.365\r",
      "Progress: 22.2% ... Training loss: 0.172 ... Validation loss: 0.319\r",
      "Progress: 22.2% ... Training loss: 0.154 ... Validation loss: 0.339\r",
      "Progress: 22.2% ... Training loss: 0.155 ... Validation loss: 0.335\r",
      "Progress: 22.2% ... Training loss: 0.154 ... Validation loss: 0.338\r",
      "Progress: 22.2% ... Training loss: 0.156 ... Validation loss: 0.325\r",
      "Progress: 22.2% ... Training loss: 0.154 ... Validation loss: 0.331\r",
      "Progress: 22.2% ... Training loss: 0.156 ... Validation loss: 0.325\r",
      "Progress: 22.2% ... Training loss: 0.154 ... Validation loss: 0.350\r",
      "Progress: 22.3% ... Training loss: 0.159 ... Validation loss: 0.322\r",
      "Progress: 22.3% ... Training loss: 0.155 ... Validation loss: 0.348\r",
      "Progress: 22.3% ... Training loss: 0.154 ... Validation loss: 0.319\r",
      "Progress: 22.3% ... Training loss: 0.153 ... Validation loss: 0.321\r",
      "Progress: 22.3% ... Training loss: 0.153 ... Validation loss: 0.327\r",
      "Progress: 22.3% ... Training loss: 0.152 ... Validation loss: 0.325\r",
      "Progress: 22.3% ... Training loss: 0.157 ... Validation loss: 0.347\r",
      "Progress: 22.4% ... Training loss: 0.157 ... Validation loss: 0.345\r",
      "Progress: 22.4% ... Training loss: 0.163 ... Validation loss: 0.319\r",
      "Progress: 22.4% ... Training loss: 0.187 ... Validation loss: 0.393\r",
      "Progress: 22.4% ... Training loss: 0.161 ... Validation loss: 0.314\r",
      "Progress: 22.4% ... Training loss: 0.156 ... Validation loss: 0.328\r",
      "Progress: 22.4% ... Training loss: 0.154 ... Validation loss: 0.316\r",
      "Progress: 22.4% ... Training loss: 0.154 ... Validation loss: 0.312\r",
      "Progress: 22.4% ... Training loss: 0.168 ... Validation loss: 0.343\r",
      "Progress: 22.4% ... Training loss: 0.159 ... Validation loss: 0.315\r",
      "Progress: 22.5% ... Training loss: 0.156 ... Validation loss: 0.339\r",
      "Progress: 22.5% ... Training loss: 0.153 ... Validation loss: 0.333\r",
      "Progress: 22.5% ... Training loss: 0.152 ... Validation loss: 0.320\r",
      "Progress: 22.5% ... Training loss: 0.151 ... Validation loss: 0.325\r",
      "Progress: 22.5% ... Training loss: 0.157 ... Validation loss: 0.337\r",
      "Progress: 22.5% ... Training loss: 0.157 ... Validation loss: 0.297\r",
      "Progress: 22.5% ... Training loss: 0.164 ... Validation loss: 0.334\r",
      "Progress: 22.6% ... Training loss: 0.164 ... Validation loss: 0.300\r",
      "Progress: 22.6% ... Training loss: 0.162 ... Validation loss: 0.336\r",
      "Progress: 22.6% ... Training loss: 0.165 ... Validation loss: 0.302\r",
      "Progress: 22.6% ... Training loss: 0.168 ... Validation loss: 0.355\r",
      "Progress: 22.6% ... Training loss: 0.166 ... Validation loss: 0.302\r",
      "Progress: 22.6% ... Training loss: 0.219 ... Validation loss: 0.391\r",
      "Progress: 22.6% ... Training loss: 0.184 ... Validation loss: 0.303\r",
      "Progress: 22.6% ... Training loss: 0.150 ... Validation loss: 0.321\r",
      "Progress: 22.6% ... Training loss: 0.151 ... Validation loss: 0.329\r",
      "Progress: 22.7% ... Training loss: 0.153 ... Validation loss: 0.324\r",
      "Progress: 22.7% ... Training loss: 0.150 ... Validation loss: 0.329\r",
      "Progress: 22.7% ... Training loss: 0.156 ... Validation loss: 0.336\r",
      "Progress: 22.7% ... Training loss: 0.152 ... Validation loss: 0.319\r",
      "Progress: 22.7% ... Training loss: 0.151 ... Validation loss: 0.340\r",
      "Progress: 22.7% ... Training loss: 0.152 ... Validation loss: 0.323\r",
      "Progress: 22.7% ... Training loss: 0.150 ... Validation loss: 0.324\r",
      "Progress: 22.8% ... Training loss: 0.163 ... Validation loss: 0.356\r",
      "Progress: 22.8% ... Training loss: 0.166 ... Validation loss: 0.313\r",
      "Progress: 22.8% ... Training loss: 0.151 ... Validation loss: 0.313\r",
      "Progress: 22.8% ... Training loss: 0.158 ... Validation loss: 0.342\r",
      "Progress: 22.8% ... Training loss: 0.163 ... Validation loss: 0.306\r",
      "Progress: 22.8% ... Training loss: 0.154 ... Validation loss: 0.331\r",
      "Progress: 22.8% ... Training loss: 0.152 ... Validation loss: 0.312\r",
      "Progress: 22.8% ... Training loss: 0.153 ... Validation loss: 0.315\r",
      "Progress: 22.9% ... Training loss: 0.152 ... Validation loss: 0.335\r",
      "Progress: 22.9% ... Training loss: 0.163 ... Validation loss: 0.313\r",
      "Progress: 22.9% ... Training loss: 0.157 ... Validation loss: 0.358\r",
      "Progress: 22.9% ... Training loss: 0.150 ... Validation loss: 0.313\r",
      "Progress: 22.9% ... Training loss: 0.148 ... Validation loss: 0.322\r",
      "Progress: 22.9% ... Training loss: 0.148 ... Validation loss: 0.328\r",
      "Progress: 22.9% ... Training loss: 0.149 ... Validation loss: 0.343\r",
      "Progress: 22.9% ... Training loss: 0.150 ... Validation loss: 0.356\r",
      "Progress: 22.9% ... Training loss: 0.157 ... Validation loss: 0.366\r",
      "Progress: 23.0% ... Training loss: 0.174 ... Validation loss: 0.316\r",
      "Progress: 23.0% ... Training loss: 0.152 ... Validation loss: 0.354\r",
      "Progress: 23.0% ... Training loss: 0.148 ... Validation loss: 0.329\r",
      "Progress: 23.0% ... Training loss: 0.148 ... Validation loss: 0.331\r",
      "Progress: 23.0% ... Training loss: 0.160 ... Validation loss: 0.318\r",
      "Progress: 23.0% ... Training loss: 0.155 ... Validation loss: 0.339\r",
      "Progress: 23.0% ... Training loss: 0.149 ... Validation loss: 0.336\r",
      "Progress: 23.1% ... Training loss: 0.150 ... Validation loss: 0.348\r",
      "Progress: 23.1% ... Training loss: 0.147 ... Validation loss: 0.326\r",
      "Progress: 23.1% ... Training loss: 0.150 ... Validation loss: 0.352\r",
      "Progress: 23.1% ... Training loss: 0.152 ... Validation loss: 0.321\r",
      "Progress: 23.1% ... Training loss: 0.149 ... Validation loss: 0.351\r",
      "Progress: 23.1% ... Training loss: 0.173 ... Validation loss: 0.320\r",
      "Progress: 23.1% ... Training loss: 0.161 ... Validation loss: 0.389\r",
      "Progress: 23.1% ... Training loss: 0.147 ... Validation loss: 0.325\r",
      "Progress: 23.1% ... Training loss: 0.154 ... Validation loss: 0.360\r",
      "Progress: 23.2% ... Training loss: 0.146 ... Validation loss: 0.333\r",
      "Progress: 23.2% ... Training loss: 0.146 ... Validation loss: 0.320\r",
      "Progress: 23.2% ... Training loss: 0.147 ... Validation loss: 0.335\r",
      "Progress: 23.2% ... Training loss: 0.151 ... Validation loss: 0.358\r",
      "Progress: 23.2% ... Training loss: 0.150 ... Validation loss: 0.332\r",
      "Progress: 23.2% ... Training loss: 0.146 ... Validation loss: 0.330\r",
      "Progress: 23.2% ... Training loss: 0.145 ... Validation loss: 0.316\r",
      "Progress: 23.2% ... Training loss: 0.150 ... Validation loss: 0.329\r",
      "Progress: 23.3% ... Training loss: 0.145 ... Validation loss: 0.309\r",
      "Progress: 23.3% ... Training loss: 0.159 ... Validation loss: 0.350\r",
      "Progress: 23.3% ... Training loss: 0.152 ... Validation loss: 0.306\r",
      "Progress: 23.3% ... Training loss: 0.165 ... Validation loss: 0.370\r",
      "Progress: 23.3% ... Training loss: 0.173 ... Validation loss: 0.304\r",
      "Progress: 23.3% ... Training loss: 0.154 ... Validation loss: 0.353\r",
      "Progress: 23.3% ... Training loss: 0.146 ... Validation loss: 0.322\r",
      "Progress: 23.4% ... Training loss: 0.152 ... Validation loss: 0.346\r",
      "Progress: 23.4% ... Training loss: 0.150 ... Validation loss: 0.305\r",
      "Progress: 23.4% ... Training loss: 0.146 ... Validation loss: 0.340\r",
      "Progress: 23.4% ... Training loss: 0.145 ... Validation loss: 0.328\r",
      "Progress: 23.4% ... Training loss: 0.145 ... Validation loss: 0.329\r",
      "Progress: 23.4% ... Training loss: 0.161 ... Validation loss: 0.301\r",
      "Progress: 23.4% ... Training loss: 0.156 ... Validation loss: 0.360\r",
      "Progress: 23.4% ... Training loss: 0.152 ... Validation loss: 0.301\r",
      "Progress: 23.4% ... Training loss: 0.145 ... Validation loss: 0.327\r",
      "Progress: 23.5% ... Training loss: 0.146 ... Validation loss: 0.331\r",
      "Progress: 23.5% ... Training loss: 0.150 ... Validation loss: 0.335\r",
      "Progress: 23.5% ... Training loss: 0.144 ... Validation loss: 0.304\r",
      "Progress: 23.5% ... Training loss: 0.143 ... Validation loss: 0.300\r",
      "Progress: 23.5% ... Training loss: 0.144 ... Validation loss: 0.302\r",
      "Progress: 23.5% ... Training loss: 0.162 ... Validation loss: 0.302\r",
      "Progress: 23.5% ... Training loss: 0.155 ... Validation loss: 0.344\r",
      "Progress: 23.6% ... Training loss: 0.159 ... Validation loss: 0.303\r",
      "Progress: 23.6% ... Training loss: 0.144 ... Validation loss: 0.304\r",
      "Progress: 23.6% ... Training loss: 0.145 ... Validation loss: 0.312\r",
      "Progress: 23.6% ... Training loss: 0.143 ... Validation loss: 0.312\r",
      "Progress: 23.6% ... Training loss: 0.146 ... Validation loss: 0.327\r",
      "Progress: 23.6% ... Training loss: 0.155 ... Validation loss: 0.297\r",
      "Progress: 23.6% ... Training loss: 0.166 ... Validation loss: 0.356\r",
      "Progress: 23.6% ... Training loss: 0.168 ... Validation loss: 0.298\r",
      "Progress: 23.6% ... Training loss: 0.160 ... Validation loss: 0.365\r",
      "Progress: 23.7% ... Training loss: 0.146 ... Validation loss: 0.307\r",
      "Progress: 23.7% ... Training loss: 0.143 ... Validation loss: 0.304\r",
      "Progress: 23.7% ... Training loss: 0.143 ... Validation loss: 0.321\r",
      "Progress: 23.7% ... Training loss: 0.143 ... Validation loss: 0.318\r",
      "Progress: 23.7% ... Training loss: 0.143 ... Validation loss: 0.320\r",
      "Progress: 23.7% ... Training loss: 0.143 ... Validation loss: 0.329\r",
      "Progress: 23.7% ... Training loss: 0.150 ... Validation loss: 0.309\r",
      "Progress: 23.8% ... Training loss: 0.149 ... Validation loss: 0.344\r",
      "Progress: 23.8% ... Training loss: 0.154 ... Validation loss: 0.299\r",
      "Progress: 23.8% ... Training loss: 0.152 ... Validation loss: 0.343\r",
      "Progress: 23.8% ... Training loss: 0.167 ... Validation loss: 0.297\r",
      "Progress: 23.8% ... Training loss: 0.198 ... Validation loss: 0.391\r",
      "Progress: 23.8% ... Training loss: 0.159 ... Validation loss: 0.292\r",
      "Progress: 23.8% ... Training loss: 0.145 ... Validation loss: 0.334\r",
      "Progress: 23.8% ... Training loss: 0.169 ... Validation loss: 0.299\r",
      "Progress: 23.9% ... Training loss: 0.159 ... Validation loss: 0.364\r",
      "Progress: 23.9% ... Training loss: 0.146 ... Validation loss: 0.311\r",
      "Progress: 23.9% ... Training loss: 0.144 ... Validation loss: 0.318\r",
      "Progress: 23.9% ... Training loss: 0.143 ... Validation loss: 0.323\r",
      "Progress: 23.9% ... Training loss: 0.141 ... Validation loss: 0.321\r",
      "Progress: 23.9% ... Training loss: 0.147 ... Validation loss: 0.301\r",
      "Progress: 23.9% ... Training loss: 0.142 ... Validation loss: 0.317\r",
      "Progress: 23.9% ... Training loss: 0.141 ... Validation loss: 0.308\r",
      "Progress: 23.9% ... Training loss: 0.146 ... Validation loss: 0.304\r",
      "Progress: 24.0% ... Training loss: 0.146 ... Validation loss: 0.352\r",
      "Progress: 24.0% ... Training loss: 0.141 ... Validation loss: 0.302\r",
      "Progress: 24.0% ... Training loss: 0.145 ... Validation loss: 0.320\r",
      "Progress: 24.0% ... Training loss: 0.146 ... Validation loss: 0.289\r",
      "Progress: 24.0% ... Training loss: 0.147 ... Validation loss: 0.309\r",
      "Progress: 24.0% ... Training loss: 0.142 ... Validation loss: 0.284\r",
      "Progress: 24.0% ... Training loss: 0.148 ... Validation loss: 0.280\r",
      "Progress: 24.1% ... Training loss: 0.144 ... Validation loss: 0.286\r",
      "Progress: 24.1% ... Training loss: 0.152 ... Validation loss: 0.329\r",
      "Progress: 24.1% ... Training loss: 0.141 ... Validation loss: 0.307\r",
      "Progress: 24.1% ... Training loss: 0.141 ... Validation loss: 0.289\r",
      "Progress: 24.1% ... Training loss: 0.140 ... Validation loss: 0.289\r",
      "Progress: 24.1% ... Training loss: 0.140 ... Validation loss: 0.289\r",
      "Progress: 24.1% ... Training loss: 0.141 ... Validation loss: 0.293\r",
      "Progress: 24.1% ... Training loss: 0.141 ... Validation loss: 0.294\r",
      "Progress: 24.1% ... Training loss: 0.141 ... Validation loss: 0.296\r",
      "Progress: 24.2% ... Training loss: 0.142 ... Validation loss: 0.300\r",
      "Progress: 24.2% ... Training loss: 0.142 ... Validation loss: 0.314\r",
      "Progress: 24.2% ... Training loss: 0.145 ... Validation loss: 0.284\r",
      "Progress: 24.2% ... Training loss: 0.143 ... Validation loss: 0.315\r",
      "Progress: 24.2% ... Training loss: 0.139 ... Validation loss: 0.296\r",
      "Progress: 24.2% ... Training loss: 0.144 ... Validation loss: 0.308\r",
      "Progress: 24.2% ... Training loss: 0.143 ... Validation loss: 0.283\r",
      "Progress: 24.2% ... Training loss: 0.138 ... Validation loss: 0.300\r",
      "Progress: 24.3% ... Training loss: 0.138 ... Validation loss: 0.305\r",
      "Progress: 24.3% ... Training loss: 0.141 ... Validation loss: 0.316\r",
      "Progress: 24.3% ... Training loss: 0.143 ... Validation loss: 0.289\r",
      "Progress: 24.3% ... Training loss: 0.151 ... Validation loss: 0.319\r",
      "Progress: 24.3% ... Training loss: 0.140 ... Validation loss: 0.281\r",
      "Progress: 24.3% ... Training loss: 0.140 ... Validation loss: 0.308\r",
      "Progress: 24.3% ... Training loss: 0.144 ... Validation loss: 0.283\r",
      "Progress: 24.4% ... Training loss: 0.137 ... Validation loss: 0.307\r",
      "Progress: 24.4% ... Training loss: 0.137 ... Validation loss: 0.297\r",
      "Progress: 24.4% ... Training loss: 0.138 ... Validation loss: 0.304\r",
      "Progress: 24.4% ... Training loss: 0.138 ... Validation loss: 0.295\r",
      "Progress: 24.4% ... Training loss: 0.137 ... Validation loss: 0.305\r",
      "Progress: 24.4% ... Training loss: 0.140 ... Validation loss: 0.322\r",
      "Progress: 24.4% ... Training loss: 0.140 ... Validation loss: 0.304\r",
      "Progress: 24.4% ... Training loss: 0.141 ... Validation loss: 0.315\r",
      "Progress: 24.4% ... Training loss: 0.180 ... Validation loss: 0.284\r",
      "Progress: 24.5% ... Training loss: 0.163 ... Validation loss: 0.392\r",
      "Progress: 24.5% ... Training loss: 0.146 ... Validation loss: 0.291\r",
      "Progress: 24.5% ... Training loss: 0.151 ... Validation loss: 0.348\r",
      "Progress: 24.5% ... Training loss: 0.138 ... Validation loss: 0.296\r",
      "Progress: 24.5% ... Training loss: 0.136 ... Validation loss: 0.310\r",
      "Progress: 24.5% ... Training loss: 0.136 ... Validation loss: 0.302\r",
      "Progress: 24.5% ... Training loss: 0.136 ... Validation loss: 0.309\r",
      "Progress: 24.6% ... Training loss: 0.144 ... Validation loss: 0.343\r",
      "Progress: 24.6% ... Training loss: 0.137 ... Validation loss: 0.326\r",
      "Progress: 24.6% ... Training loss: 0.137 ... Validation loss: 0.305\r",
      "Progress: 24.6% ... Training loss: 0.137 ... Validation loss: 0.299\r",
      "Progress: 24.6% ... Training loss: 0.140 ... Validation loss: 0.327\r",
      "Progress: 24.6% ... Training loss: 0.138 ... Validation loss: 0.289\r",
      "Progress: 24.6% ... Training loss: 0.136 ... Validation loss: 0.294\r",
      "Progress: 24.6% ... Training loss: 0.140 ... Validation loss: 0.312\r",
      "Progress: 24.6% ... Training loss: 0.139 ... Validation loss: 0.289\r",
      "Progress: 24.7% ... Training loss: 0.136 ... Validation loss: 0.307\r",
      "Progress: 24.7% ... Training loss: 0.146 ... Validation loss: 0.323\r",
      "Progress: 24.7% ... Training loss: 0.151 ... Validation loss: 0.281\r",
      "Progress: 24.7% ... Training loss: 0.142 ... Validation loss: 0.328\r",
      "Progress: 24.7% ... Training loss: 0.137 ... Validation loss: 0.306\r",
      "Progress: 24.7% ... Training loss: 0.143 ... Validation loss: 0.289\r",
      "Progress: 24.7% ... Training loss: 0.136 ... Validation loss: 0.298\r",
      "Progress: 24.8% ... Training loss: 0.145 ... Validation loss: 0.338\r",
      "Progress: 24.8% ... Training loss: 0.153 ... Validation loss: 0.279\r",
      "Progress: 24.8% ... Training loss: 0.135 ... Validation loss: 0.306\r",
      "Progress: 24.8% ... Training loss: 0.141 ... Validation loss: 0.291\r",
      "Progress: 24.8% ... Training loss: 0.140 ... Validation loss: 0.338\r",
      "Progress: 24.8% ... Training loss: 0.137 ... Validation loss: 0.309\r",
      "Progress: 24.8% ... Training loss: 0.141 ... Validation loss: 0.339\r",
      "Progress: 24.8% ... Training loss: 0.139 ... Validation loss: 0.307\r",
      "Progress: 24.9% ... Training loss: 0.167 ... Validation loss: 0.376\r",
      "Progress: 24.9% ... Training loss: 0.163 ... Validation loss: 0.274\r",
      "Progress: 24.9% ... Training loss: 0.166 ... Validation loss: 0.365\r",
      "Progress: 24.9% ... Training loss: 0.133 ... Validation loss: 0.304\r",
      "Progress: 24.9% ... Training loss: 0.133 ... Validation loss: 0.301\r",
      "Progress: 24.9% ... Training loss: 0.135 ... Validation loss: 0.316\r",
      "Progress: 24.9% ... Training loss: 0.134 ... Validation loss: 0.282\r",
      "Progress: 24.9% ... Training loss: 0.134 ... Validation loss: 0.283\r",
      "Progress: 24.9% ... Training loss: 0.134 ... Validation loss: 0.278\r",
      "Progress: 25.0% ... Training loss: 0.133 ... Validation loss: 0.291\r",
      "Progress: 25.0% ... Training loss: 0.133 ... Validation loss: 0.289\r",
      "Progress: 25.0% ... Training loss: 0.134 ... Validation loss: 0.310\r",
      "Progress: 25.0% ... Training loss: 0.134 ... Validation loss: 0.289\r",
      "Progress: 25.0% ... Training loss: 0.132 ... Validation loss: 0.298\r",
      "Progress: 25.0% ... Training loss: 0.137 ... Validation loss: 0.309\r",
      "Progress: 25.0% ... Training loss: 0.143 ... Validation loss: 0.271\r",
      "Progress: 25.1% ... Training loss: 0.133 ... Validation loss: 0.291\r",
      "Progress: 25.1% ... Training loss: 0.136 ... Validation loss: 0.283\r",
      "Progress: 25.1% ... Training loss: 0.132 ... Validation loss: 0.299\r",
      "Progress: 25.1% ... Training loss: 0.133 ... Validation loss: 0.280\r",
      "Progress: 25.1% ... Training loss: 0.137 ... Validation loss: 0.280\r",
      "Progress: 25.1% ... Training loss: 0.139 ... Validation loss: 0.279\r",
      "Progress: 25.1% ... Training loss: 0.134 ... Validation loss: 0.307\r",
      "Progress: 25.1% ... Training loss: 0.135 ... Validation loss: 0.294\r",
      "Progress: 25.1% ... Training loss: 0.132 ... Validation loss: 0.313\r",
      "Progress: 25.2% ... Training loss: 0.132 ... Validation loss: 0.316\r",
      "Progress: 25.2% ... Training loss: 0.132 ... Validation loss: 0.296\r",
      "Progress: 25.2% ... Training loss: 0.138 ... Validation loss: 0.280\r",
      "Progress: 25.2% ... Training loss: 0.131 ... Validation loss: 0.302\r",
      "Progress: 25.2% ... Training loss: 0.133 ... Validation loss: 0.296\r",
      "Progress: 25.2% ... Training loss: 0.132 ... Validation loss: 0.309\r",
      "Progress: 25.2% ... Training loss: 0.131 ... Validation loss: 0.306\r",
      "Progress: 25.2% ... Training loss: 0.150 ... Validation loss: 0.280\r",
      "Progress: 25.3% ... Training loss: 0.137 ... Validation loss: 0.313\r",
      "Progress: 25.3% ... Training loss: 0.145 ... Validation loss: 0.289\r",
      "Progress: 25.3% ... Training loss: 0.148 ... Validation loss: 0.356\r",
      "Progress: 25.3% ... Training loss: 0.130 ... Validation loss: 0.301\r",
      "Progress: 25.3% ... Training loss: 0.130 ... Validation loss: 0.302\r",
      "Progress: 25.3% ... Training loss: 0.131 ... Validation loss: 0.310\r",
      "Progress: 25.3% ... Training loss: 0.136 ... Validation loss: 0.287\r",
      "Progress: 25.4% ... Training loss: 0.136 ... Validation loss: 0.333\r",
      "Progress: 25.4% ... Training loss: 0.135 ... Validation loss: 0.290\r",
      "Progress: 25.4% ... Training loss: 0.154 ... Validation loss: 0.368\r",
      "Progress: 25.4% ... Training loss: 0.130 ... Validation loss: 0.298\r",
      "Progress: 25.4% ... Training loss: 0.134 ... Validation loss: 0.280\r",
      "Progress: 25.4% ... Training loss: 0.131 ... Validation loss: 0.305\r",
      "Progress: 25.4% ... Training loss: 0.134 ... Validation loss: 0.294\r",
      "Progress: 25.4% ... Training loss: 0.143 ... Validation loss: 0.328\r",
      "Progress: 25.4% ... Training loss: 0.146 ... Validation loss: 0.282\r",
      "Progress: 25.5% ... Training loss: 0.150 ... Validation loss: 0.330\r",
      "Progress: 25.5% ... Training loss: 0.153 ... Validation loss: 0.282\r",
      "Progress: 25.5% ... Training loss: 0.136 ... Validation loss: 0.334\r",
      "Progress: 25.5% ... Training loss: 0.138 ... Validation loss: 0.285\r",
      "Progress: 25.5% ... Training loss: 0.139 ... Validation loss: 0.329\r",
      "Progress: 25.5% ... Training loss: 0.141 ... Validation loss: 0.273\r",
      "Progress: 25.5% ... Training loss: 0.131 ... Validation loss: 0.293\r",
      "Progress: 25.6% ... Training loss: 0.130 ... Validation loss: 0.278\r",
      "Progress: 25.6% ... Training loss: 0.129 ... Validation loss: 0.285\r",
      "Progress: 25.6% ... Training loss: 0.129 ... Validation loss: 0.291\r",
      "Progress: 25.6% ... Training loss: 0.131 ... Validation loss: 0.304\r",
      "Progress: 25.6% ... Training loss: 0.137 ... Validation loss: 0.278\r",
      "Progress: 25.6% ... Training loss: 0.130 ... Validation loss: 0.284\r",
      "Progress: 25.6% ... Training loss: 0.133 ... Validation loss: 0.297\r",
      "Progress: 25.6% ... Training loss: 0.136 ... Validation loss: 0.274\r",
      "Progress: 25.6% ... Training loss: 0.144 ... Validation loss: 0.321\r",
      "Progress: 25.7% ... Training loss: 0.128 ... Validation loss: 0.287\r",
      "Progress: 25.7% ... Training loss: 0.129 ... Validation loss: 0.286\r",
      "Progress: 25.7% ... Training loss: 0.134 ... Validation loss: 0.273\r",
      "Progress: 25.7% ... Training loss: 0.166 ... Validation loss: 0.365\r",
      "Progress: 25.7% ... Training loss: 0.143 ... Validation loss: 0.265\r",
      "Progress: 25.7% ... Training loss: 0.128 ... Validation loss: 0.283\r",
      "Progress: 25.7% ... Training loss: 0.130 ... Validation loss: 0.280\r",
      "Progress: 25.8% ... Training loss: 0.158 ... Validation loss: 0.338\r",
      "Progress: 25.8% ... Training loss: 0.178 ... Validation loss: 0.280\r",
      "Progress: 25.8% ... Training loss: 0.136 ... Validation loss: 0.352\r",
      "Progress: 25.8% ... Training loss: 0.128 ... Validation loss: 0.315\r",
      "Progress: 25.8% ... Training loss: 0.130 ... Validation loss: 0.313\r",
      "Progress: 25.8% ... Training loss: 0.129 ... Validation loss: 0.305\r",
      "Progress: 25.8% ... Training loss: 0.129 ... Validation loss: 0.298\r",
      "Progress: 25.8% ... Training loss: 0.129 ... Validation loss: 0.296\r",
      "Progress: 25.9% ... Training loss: 0.146 ... Validation loss: 0.289\r",
      "Progress: 25.9% ... Training loss: 0.128 ... Validation loss: 0.313\r",
      "Progress: 25.9% ... Training loss: 0.130 ... Validation loss: 0.278\r",
      "Progress: 25.9% ... Training loss: 0.130 ... Validation loss: 0.300\r",
      "Progress: 25.9% ... Training loss: 0.145 ... Validation loss: 0.271\r",
      "Progress: 25.9% ... Training loss: 0.128 ... Validation loss: 0.288\r",
      "Progress: 25.9% ... Training loss: 0.129 ... Validation loss: 0.288\r",
      "Progress: 25.9% ... Training loss: 0.133 ... Validation loss: 0.273\r",
      "Progress: 25.9% ... Training loss: 0.128 ... Validation loss: 0.314\r",
      "Progress: 26.0% ... Training loss: 0.141 ... Validation loss: 0.281\r",
      "Progress: 26.0% ... Training loss: 0.135 ... Validation loss: 0.328\r",
      "Progress: 26.0% ... Training loss: 0.135 ... Validation loss: 0.271\r",
      "Progress: 26.0% ... Training loss: 0.128 ... Validation loss: 0.311\r",
      "Progress: 26.0% ... Training loss: 0.126 ... Validation loss: 0.304\r",
      "Progress: 26.0% ... Training loss: 0.127 ... Validation loss: 0.297\r",
      "Progress: 26.0% ... Training loss: 0.128 ... Validation loss: 0.325\r",
      "Progress: 26.1% ... Training loss: 0.127 ... Validation loss: 0.312\r",
      "Progress: 26.1% ... Training loss: 0.127 ... Validation loss: 0.317\r",
      "Progress: 26.1% ... Training loss: 0.126 ... Validation loss: 0.310\r",
      "Progress: 26.1% ... Training loss: 0.127 ... Validation loss: 0.292\r",
      "Progress: 26.1% ... Training loss: 0.125 ... Validation loss: 0.295\r",
      "Progress: 26.1% ... Training loss: 0.125 ... Validation loss: 0.301\r",
      "Progress: 26.1% ... Training loss: 0.126 ... Validation loss: 0.293\r",
      "Progress: 26.1% ... Training loss: 0.126 ... Validation loss: 0.311\r",
      "Progress: 26.1% ... Training loss: 0.140 ... Validation loss: 0.282\r",
      "Progress: 26.2% ... Training loss: 0.127 ... Validation loss: 0.328\r",
      "Progress: 26.2% ... Training loss: 0.125 ... Validation loss: 0.296\r",
      "Progress: 26.2% ... Training loss: 0.130 ... Validation loss: 0.295\r",
      "Progress: 26.2% ... Training loss: 0.127 ... Validation loss: 0.272\r",
      "Progress: 26.2% ... Training loss: 0.126 ... Validation loss: 0.277\r",
      "Progress: 26.2% ... Training loss: 0.132 ... Validation loss: 0.269\r",
      "Progress: 26.2% ... Training loss: 0.130 ... Validation loss: 0.297\r",
      "Progress: 26.2% ... Training loss: 0.126 ... Validation loss: 0.279\r",
      "Progress: 26.3% ... Training loss: 0.132 ... Validation loss: 0.326\r",
      "Progress: 26.3% ... Training loss: 0.129 ... Validation loss: 0.282\r",
      "Progress: 26.3% ... Training loss: 0.135 ... Validation loss: 0.322\r",
      "Progress: 26.3% ... Training loss: 0.125 ... Validation loss: 0.288\r",
      "Progress: 26.3% ... Training loss: 0.130 ... Validation loss: 0.322\r",
      "Progress: 26.3% ... Training loss: 0.138 ... Validation loss: 0.268\r",
      "Progress: 26.3% ... Training loss: 0.126 ... Validation loss: 0.300\r",
      "Progress: 26.4% ... Training loss: 0.123 ... Validation loss: 0.279\r",
      "Progress: 26.4% ... Training loss: 0.134 ... Validation loss: 0.306\r",
      "Progress: 26.4% ... Training loss: 0.133 ... Validation loss: 0.266\r",
      "Progress: 26.4% ... Training loss: 0.124 ... Validation loss: 0.283\r",
      "Progress: 26.4% ... Training loss: 0.126 ... Validation loss: 0.274\r",
      "Progress: 26.4% ... Training loss: 0.128 ... Validation loss: 0.303\r",
      "Progress: 26.4% ... Training loss: 0.133 ... Validation loss: 0.280\r",
      "Progress: 26.4% ... Training loss: 0.136 ... Validation loss: 0.332\r",
      "Progress: 26.4% ... Training loss: 0.130 ... Validation loss: 0.275\r",
      "Progress: 26.5% ... Training loss: 0.125 ... Validation loss: 0.301\r",
      "Progress: 26.5% ... Training loss: 0.123 ... Validation loss: 0.300\r",
      "Progress: 26.5% ... Training loss: 0.135 ... Validation loss: 0.278\r",
      "Progress: 26.5% ... Training loss: 0.124 ... Validation loss: 0.289\r",
      "Progress: 26.5% ... Training loss: 0.126 ... Validation loss: 0.286\r",
      "Progress: 26.5% ... Training loss: 0.130 ... Validation loss: 0.318\r",
      "Progress: 26.5% ... Training loss: 0.140 ... Validation loss: 0.325\r",
      "Progress: 26.6% ... Training loss: 0.158 ... Validation loss: 0.272\r",
      "Progress: 26.6% ... Training loss: 0.144 ... Validation loss: 0.321\r",
      "Progress: 26.6% ... Training loss: 0.140 ... Validation loss: 0.267\r",
      "Progress: 26.6% ... Training loss: 0.147 ... Validation loss: 0.303\r",
      "Progress: 26.6% ... Training loss: 0.127 ... Validation loss: 0.263\r",
      "Progress: 26.6% ... Training loss: 0.125 ... Validation loss: 0.272\r",
      "Progress: 26.6% ... Training loss: 0.122 ... Validation loss: 0.282\r",
      "Progress: 26.6% ... Training loss: 0.124 ... Validation loss: 0.293\r",
      "Progress: 26.6% ... Training loss: 0.142 ... Validation loss: 0.269\r",
      "Progress: 26.7% ... Training loss: 0.150 ... Validation loss: 0.350\r",
      "Progress: 26.7% ... Training loss: 0.129 ... Validation loss: 0.273\r",
      "Progress: 26.7% ... Training loss: 0.127 ... Validation loss: 0.303\r",
      "Progress: 26.7% ... Training loss: 0.129 ... Validation loss: 0.273\r",
      "Progress: 26.7% ... Training loss: 0.126 ... Validation loss: 0.316\r",
      "Progress: 26.7% ... Training loss: 0.124 ... Validation loss: 0.302\r",
      "Progress: 26.7% ... Training loss: 0.122 ... Validation loss: 0.284\r",
      "Progress: 26.8% ... Training loss: 0.121 ... Validation loss: 0.284\r",
      "Progress: 26.8% ... Training loss: 0.138 ... Validation loss: 0.272\r",
      "Progress: 26.8% ... Training loss: 0.124 ... Validation loss: 0.316\r",
      "Progress: 26.8% ... Training loss: 0.125 ... Validation loss: 0.277\r",
      "Progress: 26.8% ... Training loss: 0.122 ... Validation loss: 0.309\r",
      "Progress: 26.8% ... Training loss: 0.123 ... Validation loss: 0.307\r",
      "Progress: 26.8% ... Training loss: 0.122 ... Validation loss: 0.308\r",
      "Progress: 26.8% ... Training loss: 0.120 ... Validation loss: 0.278\r",
      "Progress: 26.9% ... Training loss: 0.124 ... Validation loss: 0.315\r",
      "Progress: 26.9% ... Training loss: 0.122 ... Validation loss: 0.280\r",
      "Progress: 26.9% ... Training loss: 0.125 ... Validation loss: 0.318\r",
      "Progress: 26.9% ... Training loss: 0.123 ... Validation loss: 0.289\r",
      "Progress: 26.9% ... Training loss: 0.126 ... Validation loss: 0.282\r",
      "Progress: 26.9% ... Training loss: 0.123 ... Validation loss: 0.281\r",
      "Progress: 26.9% ... Training loss: 0.120 ... Validation loss: 0.299\r",
      "Progress: 26.9% ... Training loss: 0.121 ... Validation loss: 0.283\r",
      "Progress: 26.9% ... Training loss: 0.121 ... Validation loss: 0.292\r",
      "Progress: 27.0% ... Training loss: 0.131 ... Validation loss: 0.338\r",
      "Progress: 27.0% ... Training loss: 0.124 ... Validation loss: 0.280\r",
      "Progress: 27.0% ... Training loss: 0.121 ... Validation loss: 0.305\r",
      "Progress: 27.0% ... Training loss: 0.120 ... Validation loss: 0.307\r",
      "Progress: 27.0% ... Training loss: 0.124 ... Validation loss: 0.282\r",
      "Progress: 27.0% ... Training loss: 0.122 ... Validation loss: 0.299\r",
      "Progress: 27.0% ... Training loss: 0.123 ... Validation loss: 0.284\r",
      "Progress: 27.1% ... Training loss: 0.129 ... Validation loss: 0.323\r",
      "Progress: 27.1% ... Training loss: 0.121 ... Validation loss: 0.275\r",
      "Progress: 27.1% ... Training loss: 0.121 ... Validation loss: 0.285\r",
      "Progress: 27.1% ... Training loss: 0.120 ... Validation loss: 0.266\r",
      "Progress: 27.1% ... Training loss: 0.121 ... Validation loss: 0.286\r",
      "Progress: 27.1% ... Training loss: 0.120 ... Validation loss: 0.273\r",
      "Progress: 27.1% ... Training loss: 0.120 ... Validation loss: 0.286\r",
      "Progress: 27.1% ... Training loss: 0.122 ... Validation loss: 0.284\r",
      "Progress: 27.1% ... Training loss: 0.122 ... Validation loss: 0.256\r",
      "Progress: 27.2% ... Training loss: 0.120 ... Validation loss: 0.270\r",
      "Progress: 27.2% ... Training loss: 0.124 ... Validation loss: 0.303\r",
      "Progress: 27.2% ... Training loss: 0.122 ... Validation loss: 0.273\r",
      "Progress: 27.2% ... Training loss: 0.125 ... Validation loss: 0.314\r",
      "Progress: 27.2% ... Training loss: 0.132 ... Validation loss: 0.265\r",
      "Progress: 27.2% ... Training loss: 0.119 ... Validation loss: 0.290\r",
      "Progress: 27.2% ... Training loss: 0.126 ... Validation loss: 0.267\r",
      "Progress: 27.2% ... Training loss: 0.120 ... Validation loss: 0.288\r",
      "Progress: 27.3% ... Training loss: 0.119 ... Validation loss: 0.269\r",
      "Progress: 27.3% ... Training loss: 0.119 ... Validation loss: 0.275\r",
      "Progress: 27.3% ... Training loss: 0.119 ... Validation loss: 0.270\r",
      "Progress: 27.3% ... Training loss: 0.119 ... Validation loss: 0.279\r",
      "Progress: 27.3% ... Training loss: 0.127 ... Validation loss: 0.295\r",
      "Progress: 27.3% ... Training loss: 0.123 ... Validation loss: 0.271\r",
      "Progress: 27.3% ... Training loss: 0.118 ... Validation loss: 0.291\r",
      "Progress: 27.4% ... Training loss: 0.118 ... Validation loss: 0.280\r",
      "Progress: 27.4% ... Training loss: 0.122 ... Validation loss: 0.274\r",
      "Progress: 27.4% ... Training loss: 0.129 ... Validation loss: 0.333\r",
      "Progress: 27.4% ... Training loss: 0.131 ... Validation loss: 0.273\r",
      "Progress: 27.4% ... Training loss: 0.139 ... Validation loss: 0.350\r",
      "Progress: 27.4% ... Training loss: 0.133 ... Validation loss: 0.265\r",
      "Progress: 27.4% ... Training loss: 0.139 ... Validation loss: 0.341\r",
      "Progress: 27.4% ... Training loss: 0.126 ... Validation loss: 0.267\r",
      "Progress: 27.4% ... Training loss: 0.124 ... Validation loss: 0.327\r",
      "Progress: 27.5% ... Training loss: 0.118 ... Validation loss: 0.280\r",
      "Progress: 27.5% ... Training loss: 0.118 ... Validation loss: 0.299\r",
      "Progress: 27.5% ... Training loss: 0.125 ... Validation loss: 0.308\r",
      "Progress: 27.5% ... Training loss: 0.121 ... Validation loss: 0.283\r",
      "Progress: 27.5% ... Training loss: 0.145 ... Validation loss: 0.343\r",
      "Progress: 27.5% ... Training loss: 0.189 ... Validation loss: 0.265\r",
      "Progress: 27.5% ... Training loss: 0.164 ... Validation loss: 0.427\r",
      "Progress: 27.6% ... Training loss: 0.159 ... Validation loss: 0.265\r",
      "Progress: 27.6% ... Training loss: 0.154 ... Validation loss: 0.396\r",
      "Progress: 27.6% ... Training loss: 0.119 ... Validation loss: 0.278\r",
      "Progress: 27.6% ... Training loss: 0.142 ... Validation loss: 0.375\r",
      "Progress: 27.6% ... Training loss: 0.136 ... Validation loss: 0.273\r",
      "Progress: 27.6% ... Training loss: 0.130 ... Validation loss: 0.379\r",
      "Progress: 27.6% ... Training loss: 0.129 ... Validation loss: 0.268\r",
      "Progress: 27.6% ... Training loss: 0.130 ... Validation loss: 0.355\r",
      "Progress: 27.6% ... Training loss: 0.139 ... Validation loss: 0.269\r",
      "Progress: 27.7% ... Training loss: 0.134 ... Validation loss: 0.360\r",
      "Progress: 27.7% ... Training loss: 0.119 ... Validation loss: 0.290\r",
      "Progress: 27.7% ... Training loss: 0.117 ... Validation loss: 0.302\r",
      "Progress: 27.7% ... Training loss: 0.124 ... Validation loss: 0.271\r",
      "Progress: 27.7% ... Training loss: 0.119 ... Validation loss: 0.305\r",
      "Progress: 27.7% ... Training loss: 0.117 ... Validation loss: 0.295\r",
      "Progress: 27.7% ... Training loss: 0.116 ... Validation loss: 0.303\r",
      "Progress: 27.8% ... Training loss: 0.116 ... Validation loss: 0.274\r",
      "Progress: 27.8% ... Training loss: 0.116 ... Validation loss: 0.273\r",
      "Progress: 27.8% ... Training loss: 0.115 ... Validation loss: 0.283\r",
      "Progress: 27.8% ... Training loss: 0.115 ... Validation loss: 0.279\r",
      "Progress: 27.8% ... Training loss: 0.118 ... Validation loss: 0.266\r",
      "Progress: 27.8% ... Training loss: 0.121 ... Validation loss: 0.297\r",
      "Progress: 27.8% ... Training loss: 0.128 ... Validation loss: 0.253\r",
      "Progress: 27.8% ... Training loss: 0.117 ... Validation loss: 0.296\r",
      "Progress: 27.9% ... Training loss: 0.116 ... Validation loss: 0.284\r",
      "Progress: 27.9% ... Training loss: 0.118 ... Validation loss: 0.270\r",
      "Progress: 27.9% ... Training loss: 0.117 ... Validation loss: 0.279\r",
      "Progress: 27.9% ... Training loss: 0.114 ... Validation loss: 0.297\r",
      "Progress: 27.9% ... Training loss: 0.116 ... Validation loss: 0.282\r",
      "Progress: 27.9% ... Training loss: 0.119 ... Validation loss: 0.274\r",
      "Progress: 27.9% ... Training loss: 0.118 ... Validation loss: 0.313\r",
      "Progress: 27.9% ... Training loss: 0.115 ... Validation loss: 0.297\r",
      "Progress: 27.9% ... Training loss: 0.115 ... Validation loss: 0.284\r",
      "Progress: 28.0% ... Training loss: 0.114 ... Validation loss: 0.293\r",
      "Progress: 28.0% ... Training loss: 0.114 ... Validation loss: 0.282\r",
      "Progress: 28.0% ... Training loss: 0.116 ... Validation loss: 0.288\r",
      "Progress: 28.0% ... Training loss: 0.117 ... Validation loss: 0.272\r",
      "Progress: 28.0% ... Training loss: 0.115 ... Validation loss: 0.274\r",
      "Progress: 28.0% ... Training loss: 0.118 ... Validation loss: 0.301\r",
      "Progress: 28.0% ... Training loss: 0.113 ... Validation loss: 0.302\r",
      "Progress: 28.1% ... Training loss: 0.114 ... Validation loss: 0.306\r",
      "Progress: 28.1% ... Training loss: 0.119 ... Validation loss: 0.269\r",
      "Progress: 28.1% ... Training loss: 0.121 ... Validation loss: 0.303\r",
      "Progress: 28.1% ... Training loss: 0.112 ... Validation loss: 0.284\r",
      "Progress: 28.1% ... Training loss: 0.114 ... Validation loss: 0.287\r",
      "Progress: 28.1% ... Training loss: 0.113 ... Validation loss: 0.290\r",
      "Progress: 28.1% ... Training loss: 0.113 ... Validation loss: 0.283\r",
      "Progress: 28.1% ... Training loss: 0.116 ... Validation loss: 0.272\r",
      "Progress: 28.1% ... Training loss: 0.114 ... Validation loss: 0.305\r",
      "Progress: 28.2% ... Training loss: 0.115 ... Validation loss: 0.268\r",
      "Progress: 28.2% ... Training loss: 0.113 ... Validation loss: 0.297\r",
      "Progress: 28.2% ... Training loss: 0.125 ... Validation loss: 0.272\r",
      "Progress: 28.2% ... Training loss: 0.157 ... Validation loss: 0.370\r",
      "Progress: 28.2% ... Training loss: 0.137 ... Validation loss: 0.255\r",
      "Progress: 28.2% ... Training loss: 0.130 ... Validation loss: 0.306\r",
      "Progress: 28.2% ... Training loss: 0.136 ... Validation loss: 0.253\r",
      "Progress: 28.2% ... Training loss: 0.130 ... Validation loss: 0.330\r",
      "Progress: 28.3% ... Training loss: 0.123 ... Validation loss: 0.255\r",
      "Progress: 28.3% ... Training loss: 0.115 ... Validation loss: 0.299\r",
      "Progress: 28.3% ... Training loss: 0.127 ... Validation loss: 0.253\r",
      "Progress: 28.3% ... Training loss: 0.122 ... Validation loss: 0.300\r",
      "Progress: 28.3% ... Training loss: 0.112 ... Validation loss: 0.283\r",
      "Progress: 28.3% ... Training loss: 0.119 ... Validation loss: 0.316\r",
      "Progress: 28.3% ... Training loss: 0.120 ... Validation loss: 0.270\r",
      "Progress: 28.4% ... Training loss: 0.132 ... Validation loss: 0.328\r",
      "Progress: 28.4% ... Training loss: 0.112 ... Validation loss: 0.284\r",
      "Progress: 28.4% ... Training loss: 0.116 ... Validation loss: 0.305\r",
      "Progress: 28.4% ... Training loss: 0.113 ... Validation loss: 0.287\r",
      "Progress: 28.4% ... Training loss: 0.111 ... Validation loss: 0.273\r",
      "Progress: 28.4% ... Training loss: 0.113 ... Validation loss: 0.277\r",
      "Progress: 28.4% ... Training loss: 0.114 ... Validation loss: 0.292\r",
      "Progress: 28.4% ... Training loss: 0.114 ... Validation loss: 0.272\r",
      "Progress: 28.4% ... Training loss: 0.125 ... Validation loss: 0.326\r",
      "Progress: 28.5% ... Training loss: 0.114 ... Validation loss: 0.263\r",
      "Progress: 28.5% ... Training loss: 0.112 ... Validation loss: 0.296\r",
      "Progress: 28.5% ... Training loss: 0.111 ... Validation loss: 0.288\r",
      "Progress: 28.5% ... Training loss: 0.112 ... Validation loss: 0.277\r",
      "Progress: 28.5% ... Training loss: 0.111 ... Validation loss: 0.272\r",
      "Progress: 28.5% ... Training loss: 0.112 ... Validation loss: 0.288\r",
      "Progress: 28.5% ... Training loss: 0.118 ... Validation loss: 0.268\r",
      "Progress: 28.6% ... Training loss: 0.111 ... Validation loss: 0.273\r",
      "Progress: 28.6% ... Training loss: 0.116 ... Validation loss: 0.269\r",
      "Progress: 28.6% ... Training loss: 0.111 ... Validation loss: 0.282\r",
      "Progress: 28.6% ... Training loss: 0.112 ... Validation loss: 0.287\r",
      "Progress: 28.6% ... Training loss: 0.113 ... Validation loss: 0.269\r",
      "Progress: 28.6% ... Training loss: 0.111 ... Validation loss: 0.287\r",
      "Progress: 28.6% ... Training loss: 0.116 ... Validation loss: 0.255\r",
      "Progress: 28.6% ... Training loss: 0.118 ... Validation loss: 0.284\r",
      "Progress: 28.6% ... Training loss: 0.113 ... Validation loss: 0.256\r",
      "Progress: 28.7% ... Training loss: 0.111 ... Validation loss: 0.278\r",
      "Progress: 28.7% ... Training loss: 0.112 ... Validation loss: 0.274\r",
      "Progress: 28.7% ... Training loss: 0.110 ... Validation loss: 0.270\r",
      "Progress: 28.7% ... Training loss: 0.114 ... Validation loss: 0.277\r",
      "Progress: 28.7% ... Training loss: 0.115 ... Validation loss: 0.252\r",
      "Progress: 28.7% ... Training loss: 0.109 ... Validation loss: 0.272\r",
      "Progress: 28.7% ... Training loss: 0.113 ... Validation loss: 0.267\r",
      "Progress: 28.8% ... Training loss: 0.120 ... Validation loss: 0.308\r",
      "Progress: 28.8% ... Training loss: 0.112 ... Validation loss: 0.255\r",
      "Progress: 28.8% ... Training loss: 0.110 ... Validation loss: 0.266\r",
      "Progress: 28.8% ... Training loss: 0.111 ... Validation loss: 0.283\r",
      "Progress: 28.8% ... Training loss: 0.109 ... Validation loss: 0.272\r",
      "Progress: 28.8% ... Training loss: 0.109 ... Validation loss: 0.263\r",
      "Progress: 28.8% ... Training loss: 0.111 ... Validation loss: 0.267\r",
      "Progress: 28.8% ... Training loss: 0.110 ... Validation loss: 0.291\r",
      "Progress: 28.9% ... Training loss: 0.110 ... Validation loss: 0.260\r",
      "Progress: 28.9% ... Training loss: 0.117 ... Validation loss: 0.298\r",
      "Progress: 28.9% ... Training loss: 0.109 ... Validation loss: 0.282\r",
      "Progress: 28.9% ... Training loss: 0.108 ... Validation loss: 0.265\r",
      "Progress: 28.9% ... Training loss: 0.112 ... Validation loss: 0.271\r",
      "Progress: 28.9% ... Training loss: 0.111 ... Validation loss: 0.304\r",
      "Progress: 28.9% ... Training loss: 0.119 ... Validation loss: 0.266\r",
      "Progress: 28.9% ... Training loss: 0.133 ... Validation loss: 0.346\r",
      "Progress: 28.9% ... Training loss: 0.109 ... Validation loss: 0.268\r",
      "Progress: 29.0% ... Training loss: 0.114 ... Validation loss: 0.293\r",
      "Progress: 29.0% ... Training loss: 0.109 ... Validation loss: 0.261\r",
      "Progress: 29.0% ... Training loss: 0.109 ... Validation loss: 0.281\r",
      "Progress: 29.0% ... Training loss: 0.108 ... Validation loss: 0.278\r",
      "Progress: 29.0% ... Training loss: 0.108 ... Validation loss: 0.275\r",
      "Progress: 29.0% ... Training loss: 0.112 ... Validation loss: 0.277\r",
      "Progress: 29.0% ... Training loss: 0.119 ... Validation loss: 0.313\r",
      "Progress: 29.1% ... Training loss: 0.107 ... Validation loss: 0.273\r",
      "Progress: 29.1% ... Training loss: 0.117 ... Validation loss: 0.259\r",
      "Progress: 29.1% ... Training loss: 0.110 ... Validation loss: 0.286\r",
      "Progress: 29.1% ... Training loss: 0.112 ... Validation loss: 0.269\r",
      "Progress: 29.1% ... Training loss: 0.111 ... Validation loss: 0.249\r",
      "Progress: 29.1% ... Training loss: 0.120 ... Validation loss: 0.294\r",
      "Progress: 29.1% ... Training loss: 0.130 ... Validation loss: 0.237\r",
      "Progress: 29.1% ... Training loss: 0.117 ... Validation loss: 0.303\r",
      "Progress: 29.1% ... Training loss: 0.109 ... Validation loss: 0.251\r",
      "Progress: 29.2% ... Training loss: 0.111 ... Validation loss: 0.275\r",
      "Progress: 29.2% ... Training loss: 0.108 ... Validation loss: 0.250\r",
      "Progress: 29.2% ... Training loss: 0.107 ... Validation loss: 0.270\r",
      "Progress: 29.2% ... Training loss: 0.107 ... Validation loss: 0.272\r",
      "Progress: 29.2% ... Training loss: 0.107 ... Validation loss: 0.262\r",
      "Progress: 29.2% ... Training loss: 0.107 ... Validation loss: 0.258\r",
      "Progress: 29.2% ... Training loss: 0.110 ... Validation loss: 0.284\r",
      "Progress: 29.2% ... Training loss: 0.111 ... Validation loss: 0.260\r",
      "Progress: 29.3% ... Training loss: 0.107 ... Validation loss: 0.286\r",
      "Progress: 29.3% ... Training loss: 0.113 ... Validation loss: 0.279\r",
      "Progress: 29.3% ... Training loss: 0.115 ... Validation loss: 0.297\r",
      "Progress: 29.3% ... Training loss: 0.113 ... Validation loss: 0.245\r",
      "Progress: 29.3% ... Training loss: 0.110 ... Validation loss: 0.283\r",
      "Progress: 29.3% ... Training loss: 0.110 ... Validation loss: 0.260\r",
      "Progress: 29.3% ... Training loss: 0.106 ... Validation loss: 0.260\r",
      "Progress: 29.4% ... Training loss: 0.114 ... Validation loss: 0.284\r",
      "Progress: 29.4% ... Training loss: 0.121 ... Validation loss: 0.241\r",
      "Progress: 29.4% ... Training loss: 0.123 ... Validation loss: 0.317\r",
      "Progress: 29.4% ... Training loss: 0.112 ... Validation loss: 0.259\r",
      "Progress: 29.4% ... Training loss: 0.105 ... Validation loss: 0.267\r",
      "Progress: 29.4% ... Training loss: 0.107 ... Validation loss: 0.276\r",
      "Progress: 29.4% ... Training loss: 0.115 ... Validation loss: 0.299\r",
      "Progress: 29.4% ... Training loss: 0.113 ... Validation loss: 0.258\r",
      "Progress: 29.4% ... Training loss: 0.115 ... Validation loss: 0.303\r",
      "Progress: 29.5% ... Training loss: 0.107 ... Validation loss: 0.254\r",
      "Progress: 29.5% ... Training loss: 0.105 ... Validation loss: 0.277\r",
      "Progress: 29.5% ... Training loss: 0.107 ... Validation loss: 0.265\r",
      "Progress: 29.5% ... Training loss: 0.109 ... Validation loss: 0.283\r",
      "Progress: 29.5% ... Training loss: 0.108 ... Validation loss: 0.292\r",
      "Progress: 29.5% ... Training loss: 0.105 ... Validation loss: 0.278\r",
      "Progress: 29.5% ... Training loss: 0.110 ... Validation loss: 0.264\r",
      "Progress: 29.6% ... Training loss: 0.141 ... Validation loss: 0.360\r",
      "Progress: 29.6% ... Training loss: 0.108 ... Validation loss: 0.259\r",
      "Progress: 29.6% ... Training loss: 0.114 ... Validation loss: 0.247\r",
      "Progress: 29.6% ... Training loss: 0.113 ... Validation loss: 0.318\r",
      "Progress: 29.6% ... Training loss: 0.107 ... Validation loss: 0.269\r",
      "Progress: 29.6% ... Training loss: 0.106 ... Validation loss: 0.269\r",
      "Progress: 29.6% ... Training loss: 0.107 ... Validation loss: 0.256\r",
      "Progress: 29.6% ... Training loss: 0.113 ... Validation loss: 0.266\r",
      "Progress: 29.6% ... Training loss: 0.107 ... Validation loss: 0.270\r",
      "Progress: 29.7% ... Training loss: 0.111 ... Validation loss: 0.258\r",
      "Progress: 29.7% ... Training loss: 0.109 ... Validation loss: 0.284\r",
      "Progress: 29.7% ... Training loss: 0.110 ... Validation loss: 0.240\r",
      "Progress: 29.7% ... Training loss: 0.107 ... Validation loss: 0.275\r",
      "Progress: 29.7% ... Training loss: 0.105 ... Validation loss: 0.269\r",
      "Progress: 29.7% ... Training loss: 0.104 ... Validation loss: 0.264\r",
      "Progress: 29.7% ... Training loss: 0.113 ... Validation loss: 0.297\r",
      "Progress: 29.8% ... Training loss: 0.128 ... Validation loss: 0.250\r",
      "Progress: 29.8% ... Training loss: 0.136 ... Validation loss: 0.368\r",
      "Progress: 29.8% ... Training loss: 0.144 ... Validation loss: 0.244\r",
      "Progress: 29.8% ... Training loss: 0.108 ... Validation loss: 0.295\r",
      "Progress: 29.8% ... Training loss: 0.105 ... Validation loss: 0.278\r",
      "Progress: 29.8% ... Training loss: 0.107 ... Validation loss: 0.302\r",
      "Progress: 29.8% ... Training loss: 0.108 ... Validation loss: 0.287\r",
      "Progress: 29.8% ... Training loss: 0.106 ... Validation loss: 0.255\r",
      "Progress: 29.9% ... Training loss: 0.108 ... Validation loss: 0.288\r",
      "Progress: 29.9% ... Training loss: 0.106 ... Validation loss: 0.265\r",
      "Progress: 29.9% ... Training loss: 0.104 ... Validation loss: 0.278\r",
      "Progress: 29.9% ... Training loss: 0.109 ... Validation loss: 0.268\r",
      "Progress: 29.9% ... Training loss: 0.105 ... Validation loss: 0.279\r",
      "Progress: 29.9% ... Training loss: 0.107 ... Validation loss: 0.300\r",
      "Progress: 29.9% ... Training loss: 0.107 ... Validation loss: 0.295\r",
      "Progress: 29.9% ... Training loss: 0.103 ... Validation loss: 0.258\r",
      "Progress: 29.9% ... Training loss: 0.120 ... Validation loss: 0.311\r",
      "Progress: 30.0% ... Training loss: 0.126 ... Validation loss: 0.240\r",
      "Progress: 30.0% ... Training loss: 0.111 ... Validation loss: 0.279\r",
      "Progress: 30.0% ... Training loss: 0.128 ... Validation loss: 0.243\r",
      "Progress: 30.0% ... Training loss: 0.111 ... Validation loss: 0.281\r",
      "Progress: 30.0% ... Training loss: 0.104 ... Validation loss: 0.263\r",
      "Progress: 30.0% ... Training loss: 0.106 ... Validation loss: 0.275\r",
      "Progress: 30.0% ... Training loss: 0.108 ... Validation loss: 0.252\r",
      "Progress: 30.1% ... Training loss: 0.114 ... Validation loss: 0.289\r",
      "Progress: 30.1% ... Training loss: 0.103 ... Validation loss: 0.251\r",
      "Progress: 30.1% ... Training loss: 0.104 ... Validation loss: 0.259\r",
      "Progress: 30.1% ... Training loss: 0.105 ... Validation loss: 0.249\r",
      "Progress: 30.1% ... Training loss: 0.117 ... Validation loss: 0.281\r",
      "Progress: 30.1% ... Training loss: 0.103 ... Validation loss: 0.240\r",
      "Progress: 30.1% ... Training loss: 0.102 ... Validation loss: 0.250\r",
      "Progress: 30.1% ... Training loss: 0.103 ... Validation loss: 0.246\r",
      "Progress: 30.1% ... Training loss: 0.102 ... Validation loss: 0.267\r",
      "Progress: 30.2% ... Training loss: 0.114 ... Validation loss: 0.249\r",
      "Progress: 30.2% ... Training loss: 0.108 ... Validation loss: 0.310\r",
      "Progress: 30.2% ... Training loss: 0.109 ... Validation loss: 0.244\r",
      "Progress: 30.2% ... Training loss: 0.108 ... Validation loss: 0.304\r",
      "Progress: 30.2% ... Training loss: 0.107 ... Validation loss: 0.286\r",
      "Progress: 30.2% ... Training loss: 0.103 ... Validation loss: 0.263\r",
      "Progress: 30.2% ... Training loss: 0.102 ... Validation loss: 0.273\r",
      "Progress: 30.2% ... Training loss: 0.109 ... Validation loss: 0.277\r",
      "Progress: 30.3% ... Training loss: 0.103 ... Validation loss: 0.256\r",
      "Progress: 30.3% ... Training loss: 0.103 ... Validation loss: 0.269\r",
      "Progress: 30.3% ... Training loss: 0.103 ... Validation loss: 0.258\r",
      "Progress: 30.3% ... Training loss: 0.107 ... Validation loss: 0.301\r",
      "Progress: 30.3% ... Training loss: 0.104 ... Validation loss: 0.270\r",
      "Progress: 30.3% ... Training loss: 0.109 ... Validation loss: 0.298\r",
      "Progress: 30.3% ... Training loss: 0.102 ... Validation loss: 0.265\r",
      "Progress: 30.4% ... Training loss: 0.102 ... Validation loss: 0.284\r",
      "Progress: 30.4% ... Training loss: 0.101 ... Validation loss: 0.258\r",
      "Progress: 30.4% ... Training loss: 0.101 ... Validation loss: 0.262\r",
      "Progress: 30.4% ... Training loss: 0.101 ... Validation loss: 0.272\r",
      "Progress: 30.4% ... Training loss: 0.102 ... Validation loss: 0.268\r",
      "Progress: 30.4% ... Training loss: 0.102 ... Validation loss: 0.291\r",
      "Progress: 30.4% ... Training loss: 0.104 ... Validation loss: 0.305\r",
      "Progress: 30.4% ... Training loss: 0.122 ... Validation loss: 0.240\r",
      "Progress: 30.4% ... Training loss: 0.178 ... Validation loss: 0.385\r",
      "Progress: 30.5% ... Training loss: 0.128 ... Validation loss: 0.238\r",
      "Progress: 30.5% ... Training loss: 0.176 ... Validation loss: 0.371\r",
      "Progress: 30.5% ... Training loss: 0.178 ... Validation loss: 0.242\r",
      "Progress: 30.5% ... Training loss: 0.159 ... Validation loss: 0.374\r",
      "Progress: 30.5% ... Training loss: 0.111 ... Validation loss: 0.242\r",
      "Progress: 30.5% ... Training loss: 0.102 ... Validation loss: 0.266\r",
      "Progress: 30.5% ... Training loss: 0.102 ... Validation loss: 0.296\r",
      "Progress: 30.6% ... Training loss: 0.108 ... Validation loss: 0.245\r",
      "Progress: 30.6% ... Training loss: 0.106 ... Validation loss: 0.294\r",
      "Progress: 30.6% ... Training loss: 0.123 ... Validation loss: 0.254\r",
      "Progress: 30.6% ... Training loss: 0.130 ... Validation loss: 0.370\r",
      "Progress: 30.6% ... Training loss: 0.129 ... Validation loss: 0.247\r",
      "Progress: 30.6% ... Training loss: 0.115 ... Validation loss: 0.304\r",
      "Progress: 30.6% ... Training loss: 0.105 ... Validation loss: 0.256\r",
      "Progress: 30.6% ... Training loss: 0.112 ... Validation loss: 0.309\r",
      "Progress: 30.6% ... Training loss: 0.118 ... Validation loss: 0.245\r",
      "Progress: 30.7% ... Training loss: 0.104 ... Validation loss: 0.294\r",
      "Progress: 30.7% ... Training loss: 0.103 ... Validation loss: 0.270\r",
      "Progress: 30.7% ... Training loss: 0.107 ... Validation loss: 0.308\r",
      "Progress: 30.7% ... Training loss: 0.138 ... Validation loss: 0.252\r",
      "Progress: 30.7% ... Training loss: 0.152 ... Validation loss: 0.376\r",
      "Progress: 30.7% ... Training loss: 0.118 ... Validation loss: 0.241\r",
      "Progress: 30.7% ... Training loss: 0.104 ... Validation loss: 0.291\r",
      "Progress: 30.8% ... Training loss: 0.105 ... Validation loss: 0.255\r",
      "Progress: 30.8% ... Training loss: 0.099 ... Validation loss: 0.267\r",
      "Progress: 30.8% ... Training loss: 0.099 ... Validation loss: 0.257\r",
      "Progress: 30.8% ... Training loss: 0.103 ... Validation loss: 0.268\r",
      "Progress: 30.8% ... Training loss: 0.105 ... Validation loss: 0.238\r",
      "Progress: 30.8% ... Training loss: 0.103 ... Validation loss: 0.279\r",
      "Progress: 30.8% ... Training loss: 0.100 ... Validation loss: 0.255\r",
      "Progress: 30.8% ... Training loss: 0.103 ... Validation loss: 0.284\r",
      "Progress: 30.9% ... Training loss: 0.111 ... Validation loss: 0.245\r",
      "Progress: 30.9% ... Training loss: 0.105 ... Validation loss: 0.303\r",
      "Progress: 30.9% ... Training loss: 0.103 ... Validation loss: 0.249\r",
      "Progress: 30.9% ... Training loss: 0.098 ... Validation loss: 0.270\r",
      "Progress: 30.9% ... Training loss: 0.100 ... Validation loss: 0.270\r",
      "Progress: 30.9% ... Training loss: 0.102 ... Validation loss: 0.272\r",
      "Progress: 30.9% ... Training loss: 0.100 ... Validation loss: 0.252\r",
      "Progress: 30.9% ... Training loss: 0.101 ... Validation loss: 0.261\r",
      "Progress: 30.9% ... Training loss: 0.098 ... Validation loss: 0.258\r",
      "Progress: 31.0% ... Training loss: 0.099 ... Validation loss: 0.264\r",
      "Progress: 31.0% ... Training loss: 0.121 ... Validation loss: 0.232\r",
      "Progress: 31.0% ... Training loss: 0.113 ... Validation loss: 0.281\r",
      "Progress: 31.0% ... Training loss: 0.101 ... Validation loss: 0.236\r",
      "Progress: 31.0% ... Training loss: 0.103 ... Validation loss: 0.255\r",
      "Progress: 31.0% ... Training loss: 0.103 ... Validation loss: 0.238\r",
      "Progress: 31.0% ... Training loss: 0.102 ... Validation loss: 0.278\r",
      "Progress: 31.1% ... Training loss: 0.108 ... Validation loss: 0.241\r",
      "Progress: 31.1% ... Training loss: 0.112 ... Validation loss: 0.297\r",
      "Progress: 31.1% ... Training loss: 0.126 ... Validation loss: 0.242\r",
      "Progress: 31.1% ... Training loss: 0.128 ... Validation loss: 0.310\r",
      "Progress: 31.1% ... Training loss: 0.128 ... Validation loss: 0.238\r",
      "Progress: 31.1% ... Training loss: 0.123 ... Validation loss: 0.329\r",
      "Progress: 31.1% ... Training loss: 0.106 ... Validation loss: 0.259\r",
      "Progress: 31.1% ... Training loss: 0.141 ... Validation loss: 0.337\r",
      "Progress: 31.1% ... Training loss: 0.111 ... Validation loss: 0.246\r",
      "Progress: 31.2% ... Training loss: 0.127 ... Validation loss: 0.323\r",
      "Progress: 31.2% ... Training loss: 0.130 ... Validation loss: 0.237\r",
      "Progress: 31.2% ... Training loss: 0.134 ... Validation loss: 0.343\r",
      "Progress: 31.2% ... Training loss: 0.103 ... Validation loss: 0.253\r",
      "Progress: 31.2% ... Training loss: 0.103 ... Validation loss: 0.303\r",
      "Progress: 31.2% ... Training loss: 0.100 ... Validation loss: 0.307\r",
      "Progress: 31.2% ... Training loss: 0.099 ... Validation loss: 0.269\r",
      "Progress: 31.2% ... Training loss: 0.101 ... Validation loss: 0.252\r",
      "Progress: 31.3% ... Training loss: 0.101 ... Validation loss: 0.273\r",
      "Progress: 31.3% ... Training loss: 0.097 ... Validation loss: 0.270\r",
      "Progress: 31.3% ... Training loss: 0.099 ... Validation loss: 0.295\r",
      "Progress: 31.3% ... Training loss: 0.097 ... Validation loss: 0.251\r",
      "Progress: 31.3% ... Training loss: 0.097 ... Validation loss: 0.261\r",
      "Progress: 31.3% ... Training loss: 0.098 ... Validation loss: 0.247\r",
      "Progress: 31.3% ... Training loss: 0.100 ... Validation loss: 0.252\r",
      "Progress: 31.4% ... Training loss: 0.101 ... Validation loss: 0.291\r",
      "Progress: 31.4% ... Training loss: 0.108 ... Validation loss: 0.246\r",
      "Progress: 31.4% ... Training loss: 0.119 ... Validation loss: 0.319\r",
      "Progress: 31.4% ... Training loss: 0.109 ... Validation loss: 0.230\r",
      "Progress: 31.4% ... Training loss: 0.099 ... Validation loss: 0.275\r",
      "Progress: 31.4% ... Training loss: 0.100 ... Validation loss: 0.254\r",
      "Progress: 31.4% ... Training loss: 0.104 ... Validation loss: 0.285\r",
      "Progress: 31.4% ... Training loss: 0.098 ... Validation loss: 0.242\r",
      "Progress: 31.4% ... Training loss: 0.103 ... Validation loss: 0.281\r",
      "Progress: 31.5% ... Training loss: 0.154 ... Validation loss: 0.241\r",
      "Progress: 31.5% ... Training loss: 0.147 ... Validation loss: 0.375\r",
      "Progress: 31.5% ... Training loss: 0.120 ... Validation loss: 0.232\r",
      "Progress: 31.5% ... Training loss: 0.099 ... Validation loss: 0.278\r",
      "Progress: 31.5% ... Training loss: 0.097 ... Validation loss: 0.251\r",
      "Progress: 31.5% ... Training loss: 0.107 ... Validation loss: 0.292\r",
      "Progress: 31.5% ... Training loss: 0.116 ... Validation loss: 0.241\r",
      "Progress: 31.6% ... Training loss: 0.098 ... Validation loss: 0.291\r",
      "Progress: 31.6% ... Training loss: 0.105 ... Validation loss: 0.256\r",
      "Progress: 31.6% ... Training loss: 0.099 ... Validation loss: 0.291\r",
      "Progress: 31.6% ... Training loss: 0.096 ... Validation loss: 0.273\r",
      "Progress: 31.6% ... Training loss: 0.096 ... Validation loss: 0.278\r",
      "Progress: 31.6% ... Training loss: 0.097 ... Validation loss: 0.247\r",
      "Progress: 31.6% ... Training loss: 0.103 ... Validation loss: 0.292\r",
      "Progress: 31.6% ... Training loss: 0.100 ... Validation loss: 0.246\r",
      "Progress: 31.6% ... Training loss: 0.095 ... Validation loss: 0.274\r",
      "Progress: 31.7% ... Training loss: 0.095 ... Validation loss: 0.261\r",
      "Progress: 31.7% ... Training loss: 0.098 ... Validation loss: 0.251\r",
      "Progress: 31.7% ... Training loss: 0.099 ... Validation loss: 0.307\r",
      "Progress: 31.7% ... Training loss: 0.098 ... Validation loss: 0.276\r",
      "Progress: 31.7% ... Training loss: 0.096 ... Validation loss: 0.272\r",
      "Progress: 31.7% ... Training loss: 0.096 ... Validation loss: 0.259\r",
      "Progress: 31.7% ... Training loss: 0.097 ... Validation loss: 0.260\r",
      "Progress: 31.8% ... Training loss: 0.105 ... Validation loss: 0.298\r",
      "Progress: 31.8% ... Training loss: 0.099 ... Validation loss: 0.266\r",
      "Progress: 31.8% ... Training loss: 0.096 ... Validation loss: 0.280\r",
      "Progress: 31.8% ... Training loss: 0.103 ... Validation loss: 0.250\r",
      "Progress: 31.8% ... Training loss: 0.103 ... Validation loss: 0.301\r",
      "Progress: 31.8% ... Training loss: 0.098 ... Validation loss: 0.241\r",
      "Progress: 31.8% ... Training loss: 0.098 ... Validation loss: 0.272\r",
      "Progress: 31.8% ... Training loss: 0.101 ... Validation loss: 0.315\r",
      "Progress: 31.9% ... Training loss: 0.099 ... Validation loss: 0.268\r",
      "Progress: 31.9% ... Training loss: 0.095 ... Validation loss: 0.276\r",
      "Progress: 31.9% ... Training loss: 0.097 ... Validation loss: 0.255\r",
      "Progress: 31.9% ... Training loss: 0.095 ... Validation loss: 0.276\r",
      "Progress: 31.9% ... Training loss: 0.094 ... Validation loss: 0.269\r",
      "Progress: 31.9% ... Training loss: 0.096 ... Validation loss: 0.256\r",
      "Progress: 31.9% ... Training loss: 0.095 ... Validation loss: 0.252\r",
      "Progress: 31.9% ... Training loss: 0.094 ... Validation loss: 0.260\r",
      "Progress: 31.9% ... Training loss: 0.107 ... Validation loss: 0.227\r",
      "Progress: 32.0% ... Training loss: 0.096 ... Validation loss: 0.267\r",
      "Progress: 32.0% ... Training loss: 0.097 ... Validation loss: 0.246\r",
      "Progress: 32.0% ... Training loss: 0.096 ... Validation loss: 0.272\r",
      "Progress: 32.0% ... Training loss: 0.105 ... Validation loss: 0.226\r",
      "Progress: 32.0% ... Training loss: 0.113 ... Validation loss: 0.309\r",
      "Progress: 32.0% ... Training loss: 0.170 ... Validation loss: 0.240\r",
      "Progress: 32.0% ... Training loss: 0.145 ... Validation loss: 0.376\r",
      "Progress: 32.0% ... Training loss: 0.107 ... Validation loss: 0.258\r",
      "Progress: 32.1% ... Training loss: 0.147 ... Validation loss: 0.374\r",
      "Progress: 32.1% ... Training loss: 0.143 ... Validation loss: 0.244\r",
      "Progress: 32.1% ... Training loss: 0.121 ... Validation loss: 0.391\r",
      "Progress: 32.1% ... Training loss: 0.118 ... Validation loss: 0.253\r",
      "Progress: 32.1% ... Training loss: 0.165 ... Validation loss: 0.412\r",
      "Progress: 32.1% ... Training loss: 0.158 ... Validation loss: 0.244\r",
      "Progress: 32.1% ... Training loss: 0.148 ... Validation loss: 0.427\r",
      "Progress: 32.1% ... Training loss: 0.109 ... Validation loss: 0.245\r",
      "Progress: 32.2% ... Training loss: 0.136 ... Validation loss: 0.352\r",
      "Progress: 32.2% ... Training loss: 0.099 ... Validation loss: 0.251\r",
      "Progress: 32.2% ... Training loss: 0.095 ... Validation loss: 0.282\r",
      "Progress: 32.2% ... Training loss: 0.094 ... Validation loss: 0.309\r",
      "Progress: 32.2% ... Training loss: 0.094 ... Validation loss: 0.298\r",
      "Progress: 32.2% ... Training loss: 0.093 ... Validation loss: 0.272\r",
      "Progress: 32.2% ... Training loss: 0.094 ... Validation loss: 0.254\r",
      "Progress: 32.2% ... Training loss: 0.097 ... Validation loss: 0.250\r",
      "Progress: 32.3% ... Training loss: 0.097 ... Validation loss: 0.292\r",
      "Progress: 32.3% ... Training loss: 0.093 ... Validation loss: 0.259\r",
      "Progress: 32.3% ... Training loss: 0.094 ... Validation loss: 0.268\r",
      "Progress: 32.3% ... Training loss: 0.099 ... Validation loss: 0.303\r",
      "Progress: 32.3% ... Training loss: 0.147 ... Validation loss: 0.239\r",
      "Progress: 32.3% ... Training loss: 0.165 ... Validation loss: 0.374\r",
      "Progress: 32.3% ... Training loss: 0.131 ... Validation loss: 0.218\r",
      "Progress: 32.4% ... Training loss: 0.136 ... Validation loss: 0.339\r",
      "Progress: 32.4% ... Training loss: 0.120 ... Validation loss: 0.239\r",
      "Progress: 32.4% ... Training loss: 0.115 ... Validation loss: 0.321\r",
      "Progress: 32.4% ... Training loss: 0.110 ... Validation loss: 0.231\r",
      "Progress: 32.4% ... Training loss: 0.096 ... Validation loss: 0.277\r",
      "Progress: 32.4% ... Training loss: 0.096 ... Validation loss: 0.265\r",
      "Progress: 32.4% ... Training loss: 0.094 ... Validation loss: 0.252\r",
      "Progress: 32.4% ... Training loss: 0.095 ... Validation loss: 0.276\r",
      "Progress: 32.5% ... Training loss: 0.100 ... Validation loss: 0.235\r",
      "Progress: 32.5% ... Training loss: 0.112 ... Validation loss: 0.271\r",
      "Progress: 32.5% ... Training loss: 0.123 ... Validation loss: 0.229\r",
      "Progress: 32.5% ... Training loss: 0.106 ... Validation loss: 0.298\r",
      "Progress: 32.5% ... Training loss: 0.108 ... Validation loss: 0.230\r",
      "Progress: 32.5% ... Training loss: 0.115 ... Validation loss: 0.301\r",
      "Progress: 32.5% ... Training loss: 0.115 ... Validation loss: 0.235\r",
      "Progress: 32.5% ... Training loss: 0.147 ... Validation loss: 0.319\r",
      "Progress: 32.5% ... Training loss: 0.143 ... Validation loss: 0.234\r",
      "Progress: 32.6% ... Training loss: 0.138 ... Validation loss: 0.326\r",
      "Progress: 32.6% ... Training loss: 0.135 ... Validation loss: 0.237\r",
      "Progress: 32.6% ... Training loss: 0.116 ... Validation loss: 0.293\r",
      "Progress: 32.6% ... Training loss: 0.093 ... Validation loss: 0.234\r",
      "Progress: 32.6% ... Training loss: 0.096 ... Validation loss: 0.238\r",
      "Progress: 32.6% ... Training loss: 0.097 ... Validation loss: 0.255\r",
      "Progress: 32.6% ... Training loss: 0.094 ... Validation loss: 0.241\r",
      "Progress: 32.6% ... Training loss: 0.097 ... Validation loss: 0.235\r",
      "Progress: 32.7% ... Training loss: 0.095 ... Validation loss: 0.268\r",
      "Progress: 32.7% ... Training loss: 0.092 ... Validation loss: 0.251\r",
      "Progress: 32.7% ... Training loss: 0.093 ... Validation loss: 0.237\r",
      "Progress: 32.7% ... Training loss: 0.093 ... Validation loss: 0.233\r",
      "Progress: 32.7% ... Training loss: 0.096 ... Validation loss: 0.255\r",
      "Progress: 32.7% ... Training loss: 0.100 ... Validation loss: 0.234\r",
      "Progress: 32.7% ... Training loss: 0.116 ... Validation loss: 0.289\r",
      "Progress: 32.8% ... Training loss: 0.110 ... Validation loss: 0.223\r",
      "Progress: 32.8% ... Training loss: 0.102 ... Validation loss: 0.269\r",
      "Progress: 32.8% ... Training loss: 0.110 ... Validation loss: 0.225\r",
      "Progress: 32.8% ... Training loss: 0.122 ... Validation loss: 0.307\r",
      "Progress: 32.8% ... Training loss: 0.099 ... Validation loss: 0.235\r",
      "Progress: 32.8% ... Training loss: 0.102 ... Validation loss: 0.291\r",
      "Progress: 32.8% ... Training loss: 0.103 ... Validation loss: 0.239\r",
      "Progress: 32.8% ... Training loss: 0.097 ... Validation loss: 0.289\r",
      "Progress: 32.9% ... Training loss: 0.113 ... Validation loss: 0.229\r",
      "Progress: 32.9% ... Training loss: 0.104 ... Validation loss: 0.266\r",
      "Progress: 32.9% ... Training loss: 0.094 ... Validation loss: 0.232\r",
      "Progress: 32.9% ... Training loss: 0.104 ... Validation loss: 0.283\r",
      "Progress: 32.9% ... Training loss: 0.100 ... Validation loss: 0.226\r",
      "Progress: 32.9% ... Training loss: 0.098 ... Validation loss: 0.262\r",
      "Progress: 32.9% ... Training loss: 0.092 ... Validation loss: 0.229\r",
      "Progress: 32.9% ... Training loss: 0.097 ... Validation loss: 0.268\r",
      "Progress: 33.0% ... Training loss: 0.095 ... Validation loss: 0.229\r",
      "Progress: 33.0% ... Training loss: 0.091 ... Validation loss: 0.242\r",
      "Progress: 33.0% ... Training loss: 0.092 ... Validation loss: 0.240\r",
      "Progress: 33.0% ... Training loss: 0.108 ... Validation loss: 0.281\r",
      "Progress: 33.0% ... Training loss: 0.092 ... Validation loss: 0.230\r",
      "Progress: 33.0% ... Training loss: 0.091 ... Validation loss: 0.241\r",
      "Progress: 33.0% ... Training loss: 0.119 ... Validation loss: 0.292\r",
      "Progress: 33.0% ... Training loss: 0.145 ... Validation loss: 0.214\r",
      "Progress: 33.0% ... Training loss: 0.130 ... Validation loss: 0.329\r",
      "Progress: 33.1% ... Training loss: 0.129 ... Validation loss: 0.218\r",
      "Progress: 33.1% ... Training loss: 0.104 ... Validation loss: 0.298\r",
      "Progress: 33.1% ... Training loss: 0.106 ... Validation loss: 0.218\r",
      "Progress: 33.1% ... Training loss: 0.102 ... Validation loss: 0.294\r",
      "Progress: 33.1% ... Training loss: 0.091 ... Validation loss: 0.219\r",
      "Progress: 33.1% ... Training loss: 0.094 ... Validation loss: 0.223\r",
      "Progress: 33.1% ... Training loss: 0.092 ... Validation loss: 0.246\r",
      "Progress: 33.1% ... Training loss: 0.092 ... Validation loss: 0.244\r",
      "Progress: 33.2% ... Training loss: 0.097 ... Validation loss: 0.227\r",
      "Progress: 33.2% ... Training loss: 0.099 ... Validation loss: 0.278\r",
      "Progress: 33.2% ... Training loss: 0.109 ... Validation loss: 0.219\r",
      "Progress: 33.2% ... Training loss: 0.124 ... Validation loss: 0.293\r",
      "Progress: 33.2% ... Training loss: 0.130 ... Validation loss: 0.212\r",
      "Progress: 33.2% ... Training loss: 0.129 ... Validation loss: 0.313\r",
      "Progress: 33.2% ... Training loss: 0.100 ... Validation loss: 0.219\r",
      "Progress: 33.2% ... Training loss: 0.101 ... Validation loss: 0.269\r",
      "Progress: 33.3% ... Training loss: 0.092 ... Validation loss: 0.220\r",
      "Progress: 33.3% ... Training loss: 0.098 ... Validation loss: 0.255\r",
      "Progress: 33.3% ... Training loss: 0.095 ... Validation loss: 0.224\r",
      "Progress: 33.3% ... Training loss: 0.091 ... Validation loss: 0.239\r",
      "Progress: 33.3% ... Training loss: 0.092 ... Validation loss: 0.222\r",
      "Progress: 33.3% ... Training loss: 0.090 ... Validation loss: 0.226\r",
      "Progress: 33.3% ... Training loss: 0.089 ... Validation loss: 0.238\r",
      "Progress: 33.4% ... Training loss: 0.089 ... Validation loss: 0.235\r",
      "Progress: 33.4% ... Training loss: 0.099 ... Validation loss: 0.261\r",
      "Progress: 33.4% ... Training loss: 0.095 ... Validation loss: 0.218\r",
      "Progress: 33.4% ... Training loss: 0.090 ... Validation loss: 0.247\r",
      "Progress: 33.4% ... Training loss: 0.094 ... Validation loss: 0.228\r",
      "Progress: 33.4% ... Training loss: 0.090 ... Validation loss: 0.249\r",
      "Progress: 33.4% ... Training loss: 0.090 ... Validation loss: 0.265\r",
      "Progress: 33.4% ... Training loss: 0.089 ... Validation loss: 0.267\r",
      "Progress: 33.5% ... Training loss: 0.090 ... Validation loss: 0.246\r",
      "Progress: 33.5% ... Training loss: 0.095 ... Validation loss: 0.234\r",
      "Progress: 33.5% ... Training loss: 0.093 ... Validation loss: 0.277\r",
      "Progress: 33.5% ... Training loss: 0.094 ... Validation loss: 0.222\r",
      "Progress: 33.5% ... Training loss: 0.090 ... Validation loss: 0.248\r",
      "Progress: 33.5% ... Training loss: 0.097 ... Validation loss: 0.225\r",
      "Progress: 33.5% ... Training loss: 0.095 ... Validation loss: 0.278\r",
      "Progress: 33.5% ... Training loss: 0.091 ... Validation loss: 0.246\r",
      "Progress: 33.5% ... Training loss: 0.094 ... Validation loss: 0.268\r",
      "Progress: 33.6% ... Training loss: 0.090 ... Validation loss: 0.234\r",
      "Progress: 33.6% ... Training loss: 0.095 ... Validation loss: 0.230\r",
      "Progress: 33.6% ... Training loss: 0.134 ... Validation loss: 0.340\r",
      "Progress: 33.6% ... Training loss: 0.120 ... Validation loss: 0.218\r",
      "Progress: 33.6% ... Training loss: 0.092 ... Validation loss: 0.278\r",
      "Progress: 33.6% ... Training loss: 0.088 ... Validation loss: 0.256\r",
      "Progress: 33.6% ... Training loss: 0.088 ... Validation loss: 0.243\r",
      "Progress: 33.6% ... Training loss: 0.094 ... Validation loss: 0.264\r",
      "Progress: 33.7% ... Training loss: 0.093 ... Validation loss: 0.232\r",
      "Progress: 33.7% ... Training loss: 0.090 ... Validation loss: 0.268\r",
      "Progress: 33.7% ... Training loss: 0.088 ... Validation loss: 0.239\r",
      "Progress: 33.7% ... Training loss: 0.088 ... Validation loss: 0.252\r",
      "Progress: 33.7% ... Training loss: 0.090 ... Validation loss: 0.242\r",
      "Progress: 33.7% ... Training loss: 0.096 ... Validation loss: 0.283\r",
      "Progress: 33.7% ... Training loss: 0.105 ... Validation loss: 0.232\r",
      "Progress: 33.8% ... Training loss: 0.096 ... Validation loss: 0.272\r",
      "Progress: 33.8% ... Training loss: 0.119 ... Validation loss: 0.225\r",
      "Progress: 33.8% ... Training loss: 0.139 ... Validation loss: 0.345\r",
      "Progress: 33.8% ... Training loss: 0.127 ... Validation loss: 0.224\r",
      "Progress: 33.8% ... Training loss: 0.125 ... Validation loss: 0.337\r",
      "Progress: 33.8% ... Training loss: 0.117 ... Validation loss: 0.229\r",
      "Progress: 33.8% ... Training loss: 0.096 ... Validation loss: 0.277\r",
      "Progress: 33.8% ... Training loss: 0.099 ... Validation loss: 0.220\r",
      "Progress: 33.9% ... Training loss: 0.106 ... Validation loss: 0.267\r",
      "Progress: 33.9% ... Training loss: 0.104 ... Validation loss: 0.213\r",
      "Progress: 33.9% ... Training loss: 0.093 ... Validation loss: 0.240\r",
      "Progress: 33.9% ... Training loss: 0.096 ... Validation loss: 0.228\r",
      "Progress: 33.9% ... Training loss: 0.089 ... Validation loss: 0.223\r",
      "Progress: 33.9% ... Training loss: 0.091 ... Validation loss: 0.230\r",
      "Progress: 33.9% ... Training loss: 0.089 ... Validation loss: 0.244\r",
      "Progress: 33.9% ... Training loss: 0.102 ... Validation loss: 0.269\r",
      "Progress: 34.0% ... Training loss: 0.088 ... Validation loss: 0.224\r",
      "Progress: 34.0% ... Training loss: 0.088 ... Validation loss: 0.243\r",
      "Progress: 34.0% ... Training loss: 0.089 ... Validation loss: 0.268\r",
      "Progress: 34.0% ... Training loss: 0.090 ... Validation loss: 0.261\r",
      "Progress: 34.0% ... Training loss: 0.090 ... Validation loss: 0.283\r",
      "Progress: 34.0% ... Training loss: 0.090 ... Validation loss: 0.230\r",
      "Progress: 34.0% ... Training loss: 0.099 ... Validation loss: 0.293\r",
      "Progress: 34.0% ... Training loss: 0.092 ... Validation loss: 0.237\r",
      "Progress: 34.0% ... Training loss: 0.089 ... Validation loss: 0.238\r",
      "Progress: 34.1% ... Training loss: 0.092 ... Validation loss: 0.277\r",
      "Progress: 34.1% ... Training loss: 0.091 ... Validation loss: 0.235\r",
      "Progress: 34.1% ... Training loss: 0.089 ... Validation loss: 0.250\r",
      "Progress: 34.1% ... Training loss: 0.088 ... Validation loss: 0.246\r",
      "Progress: 34.1% ... Training loss: 0.089 ... Validation loss: 0.236\r",
      "Progress: 34.1% ... Training loss: 0.097 ... Validation loss: 0.291\r",
      "Progress: 34.1% ... Training loss: 0.130 ... Validation loss: 0.230\r",
      "Progress: 34.1% ... Training loss: 0.111 ... Validation loss: 0.325\r",
      "Progress: 34.2% ... Training loss: 0.113 ... Validation loss: 0.218\r",
      "Progress: 34.2% ... Training loss: 0.127 ... Validation loss: 0.339\r",
      "Progress: 34.2% ... Training loss: 0.131 ... Validation loss: 0.217\r",
      "Progress: 34.2% ... Training loss: 0.147 ... Validation loss: 0.345\r",
      "Progress: 34.2% ... Training loss: 0.101 ... Validation loss: 0.213\r",
      "Progress: 34.2% ... Training loss: 0.095 ... Validation loss: 0.267\r",
      "Progress: 34.2% ... Training loss: 0.097 ... Validation loss: 0.227\r",
      "Progress: 34.2% ... Training loss: 0.090 ... Validation loss: 0.277\r",
      "Progress: 34.3% ... Training loss: 0.090 ... Validation loss: 0.241\r",
      "Progress: 34.3% ... Training loss: 0.096 ... Validation loss: 0.287\r",
      "Progress: 34.3% ... Training loss: 0.094 ... Validation loss: 0.228\r",
      "Progress: 34.3% ... Training loss: 0.089 ... Validation loss: 0.255\r",
      "Progress: 34.3% ... Training loss: 0.089 ... Validation loss: 0.221\r",
      "Progress: 34.3% ... Training loss: 0.089 ... Validation loss: 0.251\r",
      "Progress: 34.3% ... Training loss: 0.088 ... Validation loss: 0.226\r",
      "Progress: 34.4% ... Training loss: 0.090 ... Validation loss: 0.252\r",
      "Progress: 34.4% ... Training loss: 0.091 ... Validation loss: 0.230\r",
      "Progress: 34.4% ... Training loss: 0.089 ... Validation loss: 0.286\r",
      "Progress: 34.4% ... Training loss: 0.087 ... Validation loss: 0.232\r",
      "Progress: 34.4% ... Training loss: 0.087 ... Validation loss: 0.251\r",
      "Progress: 34.4% ... Training loss: 0.087 ... Validation loss: 0.237\r",
      "Progress: 34.4% ... Training loss: 0.086 ... Validation loss: 0.253\r",
      "Progress: 34.4% ... Training loss: 0.088 ... Validation loss: 0.242\r",
      "Progress: 34.5% ... Training loss: 0.087 ... Validation loss: 0.235\r",
      "Progress: 34.5% ... Training loss: 0.088 ... Validation loss: 0.237\r",
      "Progress: 34.5% ... Training loss: 0.088 ... Validation loss: 0.251\r",
      "Progress: 34.5% ... Training loss: 0.086 ... Validation loss: 0.253\r",
      "Progress: 34.5% ... Training loss: 0.097 ... Validation loss: 0.270\r",
      "Progress: 34.5% ... Training loss: 0.108 ... Validation loss: 0.213\r",
      "Progress: 34.5% ... Training loss: 0.102 ... Validation loss: 0.287\r",
      "Progress: 34.5% ... Training loss: 0.093 ... Validation loss: 0.219\r",
      "Progress: 34.5% ... Training loss: 0.087 ... Validation loss: 0.255\r",
      "Progress: 34.6% ... Training loss: 0.090 ... Validation loss: 0.266\r",
      "Progress: 34.6% ... Training loss: 0.086 ... Validation loss: 0.252\r",
      "Progress: 34.6% ... Training loss: 0.086 ... Validation loss: 0.235\r",
      "Progress: 34.6% ... Training loss: 0.089 ... Validation loss: 0.225\r",
      "Progress: 34.6% ... Training loss: 0.086 ... Validation loss: 0.240\r",
      "Progress: 34.6% ... Training loss: 0.087 ... Validation loss: 0.235\r",
      "Progress: 34.6% ... Training loss: 0.104 ... Validation loss: 0.278\r",
      "Progress: 34.6% ... Training loss: 0.106 ... Validation loss: 0.211\r",
      "Progress: 34.7% ... Training loss: 0.100 ... Validation loss: 0.284\r",
      "Progress: 34.7% ... Training loss: 0.109 ... Validation loss: 0.209\r",
      "Progress: 34.7% ... Training loss: 0.104 ... Validation loss: 0.279\r",
      "Progress: 34.7% ... Training loss: 0.101 ... Validation loss: 0.218\r",
      "Progress: 34.7% ... Training loss: 0.086 ... Validation loss: 0.254\r",
      "Progress: 34.7% ... Training loss: 0.087 ... Validation loss: 0.235\r",
      "Progress: 34.7% ... Training loss: 0.093 ... Validation loss: 0.218\r",
      "Progress: 34.8% ... Training loss: 0.086 ... Validation loss: 0.239\r",
      "Progress: 34.8% ... Training loss: 0.089 ... Validation loss: 0.246\r",
      "Progress: 34.8% ... Training loss: 0.089 ... Validation loss: 0.219\r",
      "Progress: 34.8% ... Training loss: 0.089 ... Validation loss: 0.269\r",
      "Progress: 34.8% ... Training loss: 0.085 ... Validation loss: 0.254\r",
      "Progress: 34.8% ... Training loss: 0.087 ... Validation loss: 0.252\r",
      "Progress: 34.8% ... Training loss: 0.085 ... Validation loss: 0.243\r",
      "Progress: 34.8% ... Training loss: 0.085 ... Validation loss: 0.224\r",
      "Progress: 34.9% ... Training loss: 0.086 ... Validation loss: 0.228\r",
      "Progress: 34.9% ... Training loss: 0.089 ... Validation loss: 0.213\r",
      "Progress: 34.9% ... Training loss: 0.086 ... Validation loss: 0.244\r",
      "Progress: 34.9% ... Training loss: 0.087 ... Validation loss: 0.243\r",
      "Progress: 34.9% ... Training loss: 0.084 ... Validation loss: 0.221\r",
      "Progress: 34.9% ... Training loss: 0.087 ... Validation loss: 0.255\r",
      "Progress: 34.9% ... Training loss: 0.089 ... Validation loss: 0.262\r",
      "Progress: 34.9% ... Training loss: 0.090 ... Validation loss: 0.230\r",
      "Progress: 35.0% ... Training loss: 0.091 ... Validation loss: 0.274\r",
      "Progress: 35.0% ... Training loss: 0.100 ... Validation loss: 0.222\r",
      "Progress: 35.0% ... Training loss: 0.098 ... Validation loss: 0.285\r",
      "Progress: 35.0% ... Training loss: 0.095 ... Validation loss: 0.229\r",
      "Progress: 35.0% ... Training loss: 0.109 ... Validation loss: 0.316\r",
      "Progress: 35.0% ... Training loss: 0.103 ... Validation loss: 0.216\r",
      "Progress: 35.0% ... Training loss: 0.111 ... Validation loss: 0.323\r",
      "Progress: 35.0% ... Training loss: 0.115 ... Validation loss: 0.208\r",
      "Progress: 35.0% ... Training loss: 0.150 ... Validation loss: 0.345\r",
      "Progress: 35.1% ... Training loss: 0.112 ... Validation loss: 0.206\r",
      "Progress: 35.1% ... Training loss: 0.103 ... Validation loss: 0.281\r",
      "Progress: 35.1% ... Training loss: 0.109 ... Validation loss: 0.216\r",
      "Progress: 35.1% ... Training loss: 0.100 ... Validation loss: 0.289\r",
      "Progress: 35.1% ... Training loss: 0.130 ... Validation loss: 0.217\r",
      "Progress: 35.1% ... Training loss: 0.148 ... Validation loss: 0.361\r",
      "Progress: 35.1% ... Training loss: 0.145 ... Validation loss: 0.207\r",
      "Progress: 35.1% ... Training loss: 0.126 ... Validation loss: 0.308\r",
      "Progress: 35.2% ... Training loss: 0.133 ... Validation loss: 0.208\r",
      "Progress: 35.2% ... Training loss: 0.124 ... Validation loss: 0.299\r",
      "Progress: 35.2% ... Training loss: 0.124 ... Validation loss: 0.207\r",
      "Progress: 35.2% ... Training loss: 0.126 ... Validation loss: 0.284\r",
      "Progress: 35.2% ... Training loss: 0.111 ... Validation loss: 0.206\r",
      "Progress: 35.2% ... Training loss: 0.086 ... Validation loss: 0.254\r",
      "Progress: 35.2% ... Training loss: 0.086 ... Validation loss: 0.240\r",
      "Progress: 35.2% ... Training loss: 0.087 ... Validation loss: 0.209\r",
      "Progress: 35.3% ... Training loss: 0.117 ... Validation loss: 0.289\r",
      "Progress: 35.3% ... Training loss: 0.098 ... Validation loss: 0.207\r",
      "Progress: 35.3% ... Training loss: 0.114 ... Validation loss: 0.298\r",
      "Progress: 35.3% ... Training loss: 0.096 ... Validation loss: 0.215\r",
      "Progress: 35.3% ... Training loss: 0.094 ... Validation loss: 0.273\r",
      "Progress: 35.3% ... Training loss: 0.091 ... Validation loss: 0.219\r",
      "Progress: 35.3% ... Training loss: 0.090 ... Validation loss: 0.257\r",
      "Progress: 35.4% ... Training loss: 0.094 ... Validation loss: 0.214\r",
      "Progress: 35.4% ... Training loss: 0.099 ... Validation loss: 0.284\r",
      "Progress: 35.4% ... Training loss: 0.089 ... Validation loss: 0.215\r",
      "Progress: 35.4% ... Training loss: 0.116 ... Validation loss: 0.294\r",
      "Progress: 35.4% ... Training loss: 0.111 ... Validation loss: 0.214\r",
      "Progress: 35.4% ... Training loss: 0.150 ... Validation loss: 0.328\r",
      "Progress: 35.4% ... Training loss: 0.129 ... Validation loss: 0.202\r",
      "Progress: 35.4% ... Training loss: 0.126 ... Validation loss: 0.331\r",
      "Progress: 35.5% ... Training loss: 0.118 ... Validation loss: 0.209\r",
      "Progress: 35.5% ... Training loss: 0.161 ... Validation loss: 0.357\r",
      "Progress: 35.5% ... Training loss: 0.183 ... Validation loss: 0.212\r",
      "Progress: 35.5% ... Training loss: 0.158 ... Validation loss: 0.363\r",
      "Progress: 35.5% ... Training loss: 0.120 ... Validation loss: 0.206\r",
      "Progress: 35.5% ... Training loss: 0.130 ... Validation loss: 0.323\r",
      "Progress: 35.5% ... Training loss: 0.131 ... Validation loss: 0.206\r",
      "Progress: 35.5% ... Training loss: 0.146 ... Validation loss: 0.313\r",
      "Progress: 35.5% ... Training loss: 0.147 ... Validation loss: 0.210\r",
      "Progress: 35.6% ... Training loss: 0.172 ... Validation loss: 0.349\r",
      "Progress: 35.6% ... Training loss: 0.163 ... Validation loss: 0.217\r",
      "Progress: 35.6% ... Training loss: 0.164 ... Validation loss: 0.323\r",
      "Progress: 35.6% ... Training loss: 0.160 ... Validation loss: 0.216\r",
      "Progress: 35.6% ... Training loss: 0.129 ... Validation loss: 0.299\r",
      "Progress: 35.6% ... Training loss: 0.143 ... Validation loss: 0.213\r",
      "Progress: 35.6% ... Training loss: 0.114 ... Validation loss: 0.302\r",
      "Progress: 35.6% ... Training loss: 0.092 ... Validation loss: 0.217\r",
      "Progress: 35.7% ... Training loss: 0.085 ... Validation loss: 0.247\r",
      "Progress: 35.7% ... Training loss: 0.086 ... Validation loss: 0.253\r",
      "Progress: 35.7% ... Training loss: 0.085 ... Validation loss: 0.243\r",
      "Progress: 35.7% ... Training loss: 0.086 ... Validation loss: 0.248\r",
      "Progress: 35.7% ... Training loss: 0.085 ... Validation loss: 0.218\r",
      "Progress: 35.7% ... Training loss: 0.084 ... Validation loss: 0.230\r",
      "Progress: 35.7% ... Training loss: 0.086 ... Validation loss: 0.225\r",
      "Progress: 35.8% ... Training loss: 0.087 ... Validation loss: 0.233\r",
      "Progress: 35.8% ... Training loss: 0.083 ... Validation loss: 0.228\r",
      "Progress: 35.8% ... Training loss: 0.091 ... Validation loss: 0.254\r",
      "Progress: 35.8% ... Training loss: 0.087 ... Validation loss: 0.222\r",
      "Progress: 35.8% ... Training loss: 0.108 ... Validation loss: 0.289\r",
      "Progress: 35.8% ... Training loss: 0.125 ... Validation loss: 0.219\r",
      "Progress: 35.8% ... Training loss: 0.097 ... Validation loss: 0.260\r",
      "Progress: 35.8% ... Training loss: 0.096 ... Validation loss: 0.210\r",
      "Progress: 35.9% ... Training loss: 0.091 ... Validation loss: 0.258\r",
      "Progress: 35.9% ... Training loss: 0.084 ... Validation loss: 0.230\r",
      "Progress: 35.9% ... Training loss: 0.085 ... Validation loss: 0.250\r",
      "Progress: 35.9% ... Training loss: 0.085 ... Validation loss: 0.224\r",
      "Progress: 35.9% ... Training loss: 0.084 ... Validation loss: 0.248\r",
      "Progress: 35.9% ... Training loss: 0.088 ... Validation loss: 0.226\r",
      "Progress: 35.9% ... Training loss: 0.084 ... Validation loss: 0.242\r",
      "Progress: 35.9% ... Training loss: 0.086 ... Validation loss: 0.244\r",
      "Progress: 36.0% ... Training loss: 0.090 ... Validation loss: 0.227\r",
      "Progress: 36.0% ... Training loss: 0.089 ... Validation loss: 0.256\r",
      "Progress: 36.0% ... Training loss: 0.085 ... Validation loss: 0.228\r",
      "Progress: 36.0% ... Training loss: 0.083 ... Validation loss: 0.230\r",
      "Progress: 36.0% ... Training loss: 0.086 ... Validation loss: 0.249\r",
      "Progress: 36.0% ... Training loss: 0.088 ... Validation loss: 0.220\r",
      "Progress: 36.0% ... Training loss: 0.084 ... Validation loss: 0.238\r",
      "Progress: 36.0% ... Training loss: 0.083 ... Validation loss: 0.229\r",
      "Progress: 36.0% ... Training loss: 0.091 ... Validation loss: 0.215\r",
      "Progress: 36.1% ... Training loss: 0.103 ... Validation loss: 0.263\r",
      "Progress: 36.1% ... Training loss: 0.085 ... Validation loss: 0.215\r",
      "Progress: 36.1% ... Training loss: 0.085 ... Validation loss: 0.218\r",
      "Progress: 36.1% ... Training loss: 0.082 ... Validation loss: 0.222\r",
      "Progress: 36.1% ... Training loss: 0.087 ... Validation loss: 0.243\r",
      "Progress: 36.1% ... Training loss: 0.086 ... Validation loss: 0.205\r",
      "Progress: 36.1% ... Training loss: 0.083 ... Validation loss: 0.237\r",
      "Progress: 36.1% ... Training loss: 0.082 ... Validation loss: 0.221\r",
      "Progress: 36.2% ... Training loss: 0.084 ... Validation loss: 0.236\r",
      "Progress: 36.2% ... Training loss: 0.084 ... Validation loss: 0.224\r",
      "Progress: 36.2% ... Training loss: 0.082 ... Validation loss: 0.236\r",
      "Progress: 36.2% ... Training loss: 0.081 ... Validation loss: 0.232\r",
      "Progress: 36.2% ... Training loss: 0.084 ... Validation loss: 0.220\r",
      "Progress: 36.2% ... Training loss: 0.083 ... Validation loss: 0.221\r",
      "Progress: 36.2% ... Training loss: 0.083 ... Validation loss: 0.233\r",
      "Progress: 36.2% ... Training loss: 0.088 ... Validation loss: 0.228\r",
      "Progress: 36.3% ... Training loss: 0.083 ... Validation loss: 0.241\r",
      "Progress: 36.3% ... Training loss: 0.089 ... Validation loss: 0.226\r",
      "Progress: 36.3% ... Training loss: 0.082 ... Validation loss: 0.242\r",
      "Progress: 36.3% ... Training loss: 0.083 ... Validation loss: 0.242\r",
      "Progress: 36.3% ... Training loss: 0.083 ... Validation loss: 0.218\r",
      "Progress: 36.3% ... Training loss: 0.082 ... Validation loss: 0.222\r",
      "Progress: 36.3% ... Training loss: 0.083 ... Validation loss: 0.211\r",
      "Progress: 36.4% ... Training loss: 0.087 ... Validation loss: 0.251\r",
      "Progress: 36.4% ... Training loss: 0.084 ... Validation loss: 0.251\r",
      "Progress: 36.4% ... Training loss: 0.094 ... Validation loss: 0.214\r",
      "Progress: 36.4% ... Training loss: 0.083 ... Validation loss: 0.244\r",
      "Progress: 36.4% ... Training loss: 0.083 ... Validation loss: 0.228\r",
      "Progress: 36.4% ... Training loss: 0.082 ... Validation loss: 0.243\r",
      "Progress: 36.4% ... Training loss: 0.082 ... Validation loss: 0.237\r",
      "Progress: 36.4% ... Training loss: 0.087 ... Validation loss: 0.228\r",
      "Progress: 36.5% ... Training loss: 0.084 ... Validation loss: 0.239\r",
      "Progress: 36.5% ... Training loss: 0.082 ... Validation loss: 0.230\r",
      "Progress: 36.5% ... Training loss: 0.098 ... Validation loss: 0.209\r",
      "Progress: 36.5% ... Training loss: 0.091 ... Validation loss: 0.278\r",
      "Progress: 36.5% ... Training loss: 0.092 ... Validation loss: 0.213\r",
      "Progress: 36.5% ... Training loss: 0.084 ... Validation loss: 0.262\r",
      "Progress: 36.5% ... Training loss: 0.082 ... Validation loss: 0.226\r",
      "Progress: 36.5% ... Training loss: 0.083 ... Validation loss: 0.235\r",
      "Progress: 36.5% ... Training loss: 0.086 ... Validation loss: 0.221\r",
      "Progress: 36.6% ... Training loss: 0.082 ... Validation loss: 0.233\r",
      "Progress: 36.6% ... Training loss: 0.083 ... Validation loss: 0.255\r",
      "Progress: 36.6% ... Training loss: 0.089 ... Validation loss: 0.215\r",
      "Progress: 36.6% ... Training loss: 0.097 ... Validation loss: 0.270\r",
      "Progress: 36.6% ... Training loss: 0.083 ... Validation loss: 0.217\r",
      "Progress: 36.6% ... Training loss: 0.084 ... Validation loss: 0.245\r",
      "Progress: 36.6% ... Training loss: 0.081 ... Validation loss: 0.231\r",
      "Progress: 36.6% ... Training loss: 0.081 ... Validation loss: 0.241\r",
      "Progress: 36.7% ... Training loss: 0.084 ... Validation loss: 0.252\r",
      "Progress: 36.7% ... Training loss: 0.084 ... Validation loss: 0.220\r",
      "Progress: 36.7% ... Training loss: 0.090 ... Validation loss: 0.278\r",
      "Progress: 36.7% ... Training loss: 0.084 ... Validation loss: 0.222\r",
      "Progress: 36.7% ... Training loss: 0.087 ... Validation loss: 0.248\r",
      "Progress: 36.7% ... Training loss: 0.083 ... Validation loss: 0.223\r",
      "Progress: 36.7% ... Training loss: 0.081 ... Validation loss: 0.235\r",
      "Progress: 36.8% ... Training loss: 0.085 ... Validation loss: 0.259\r",
      "Progress: 36.8% ... Training loss: 0.090 ... Validation loss: 0.217\r",
      "Progress: 36.8% ... Training loss: 0.108 ... Validation loss: 0.304\r",
      "Progress: 36.8% ... Training loss: 0.095 ... Validation loss: 0.209\r",
      "Progress: 36.8% ... Training loss: 0.085 ... Validation loss: 0.247\r",
      "Progress: 36.8% ... Training loss: 0.084 ... Validation loss: 0.235\r",
      "Progress: 36.8% ... Training loss: 0.100 ... Validation loss: 0.309\r",
      "Progress: 36.8% ... Training loss: 0.102 ... Validation loss: 0.205\r",
      "Progress: 36.9% ... Training loss: 0.092 ... Validation loss: 0.271\r",
      "Progress: 36.9% ... Training loss: 0.085 ... Validation loss: 0.231\r",
      "Progress: 36.9% ... Training loss: 0.083 ... Validation loss: 0.260\r",
      "Progress: 36.9% ... Training loss: 0.088 ... Validation loss: 0.229\r",
      "Progress: 36.9% ... Training loss: 0.086 ... Validation loss: 0.269\r",
      "Progress: 36.9% ... Training loss: 0.082 ... Validation loss: 0.250\r",
      "Progress: 36.9% ... Training loss: 0.081 ... Validation loss: 0.245\r",
      "Progress: 36.9% ... Training loss: 0.089 ... Validation loss: 0.262\r",
      "Progress: 37.0% ... Training loss: 0.083 ... Validation loss: 0.229\r",
      "Progress: 37.0% ... Training loss: 0.088 ... Validation loss: 0.218\r",
      "Progress: 37.0% ... Training loss: 0.081 ... Validation loss: 0.249\r",
      "Progress: 37.0% ... Training loss: 0.081 ... Validation loss: 0.221\r",
      "Progress: 37.0% ... Training loss: 0.088 ... Validation loss: 0.261\r",
      "Progress: 37.0% ... Training loss: 0.080 ... Validation loss: 0.227\r",
      "Progress: 37.0% ... Training loss: 0.084 ... Validation loss: 0.235\r",
      "Progress: 37.0% ... Training loss: 0.083 ... Validation loss: 0.217\r",
      "Progress: 37.0% ... Training loss: 0.083 ... Validation loss: 0.254\r",
      "Progress: 37.1% ... Training loss: 0.087 ... Validation loss: 0.210\r",
      "Progress: 37.1% ... Training loss: 0.082 ... Validation loss: 0.233\r",
      "Progress: 37.1% ... Training loss: 0.081 ... Validation loss: 0.218\r",
      "Progress: 37.1% ... Training loss: 0.080 ... Validation loss: 0.217\r",
      "Progress: 37.1% ... Training loss: 0.083 ... Validation loss: 0.241\r",
      "Progress: 37.1% ... Training loss: 0.088 ... Validation loss: 0.232\r",
      "Progress: 37.1% ... Training loss: 0.081 ... Validation loss: 0.222\r",
      "Progress: 37.1% ... Training loss: 0.085 ... Validation loss: 0.215\r",
      "Progress: 37.2% ... Training loss: 0.091 ... Validation loss: 0.266\r",
      "Progress: 37.2% ... Training loss: 0.090 ... Validation loss: 0.206\r",
      "Progress: 37.2% ... Training loss: 0.082 ... Validation loss: 0.236\r",
      "Progress: 37.2% ... Training loss: 0.083 ... Validation loss: 0.238\r",
      "Progress: 37.2% ... Training loss: 0.084 ... Validation loss: 0.210\r",
      "Progress: 37.2% ... Training loss: 0.087 ... Validation loss: 0.237\r",
      "Progress: 37.2% ... Training loss: 0.091 ... Validation loss: 0.209\r",
      "Progress: 37.2% ... Training loss: 0.096 ... Validation loss: 0.273\r",
      "Progress: 37.3% ... Training loss: 0.098 ... Validation loss: 0.206\r",
      "Progress: 37.3% ... Training loss: 0.113 ... Validation loss: 0.278\r",
      "Progress: 37.3% ... Training loss: 0.118 ... Validation loss: 0.203\r",
      "Progress: 37.3% ... Training loss: 0.113 ... Validation loss: 0.298\r",
      "Progress: 37.3% ... Training loss: 0.142 ... Validation loss: 0.216\r",
      "Progress: 37.3% ... Training loss: 0.159 ... Validation loss: 0.374\r",
      "Progress: 37.3% ... Training loss: 0.161 ... Validation loss: 0.213\r",
      "Progress: 37.4% ... Training loss: 0.174 ... Validation loss: 0.368\r",
      "Progress: 37.4% ... Training loss: 0.151 ... Validation loss: 0.209\r",
      "Progress: 37.4% ... Training loss: 0.145 ... Validation loss: 0.344\r",
      "Progress: 37.4% ... Training loss: 0.157 ... Validation loss: 0.219\r",
      "Progress: 37.4% ... Training loss: 0.116 ... Validation loss: 0.334\r",
      "Progress: 37.4% ... Training loss: 0.105 ... Validation loss: 0.208\r",
      "Progress: 37.4% ... Training loss: 0.109 ... Validation loss: 0.323\r",
      "Progress: 37.4% ... Training loss: 0.121 ... Validation loss: 0.216\r",
      "Progress: 37.5% ... Training loss: 0.127 ... Validation loss: 0.335\r",
      "Progress: 37.5% ... Training loss: 0.096 ... Validation loss: 0.205\r",
      "Progress: 37.5% ... Training loss: 0.086 ... Validation loss: 0.298\r",
      "Progress: 37.5% ... Training loss: 0.090 ... Validation loss: 0.225\r",
      "Progress: 37.5% ... Training loss: 0.090 ... Validation loss: 0.256\r",
      "Progress: 37.5% ... Training loss: 0.091 ... Validation loss: 0.204\r",
      "Progress: 37.5% ... Training loss: 0.081 ... Validation loss: 0.251\r",
      "Progress: 37.5% ... Training loss: 0.079 ... Validation loss: 0.242\r",
      "Progress: 37.5% ... Training loss: 0.080 ... Validation loss: 0.226\r",
      "Progress: 37.6% ... Training loss: 0.083 ... Validation loss: 0.225\r",
      "Progress: 37.6% ... Training loss: 0.079 ... Validation loss: 0.238\r",
      "Progress: 37.6% ... Training loss: 0.080 ... Validation loss: 0.240\r",
      "Progress: 37.6% ... Training loss: 0.090 ... Validation loss: 0.210\r",
      "Progress: 37.6% ... Training loss: 0.088 ... Validation loss: 0.251\r",
      "Progress: 37.6% ... Training loss: 0.080 ... Validation loss: 0.216\r",
      "Progress: 37.6% ... Training loss: 0.081 ... Validation loss: 0.235\r",
      "Progress: 37.6% ... Training loss: 0.082 ... Validation loss: 0.272\r",
      "Progress: 37.7% ... Training loss: 0.081 ... Validation loss: 0.232\r",
      "Progress: 37.7% ... Training loss: 0.084 ... Validation loss: 0.276\r",
      "Progress: 37.7% ... Training loss: 0.098 ... Validation loss: 0.213\r",
      "Progress: 37.7% ... Training loss: 0.086 ... Validation loss: 0.249\r",
      "Progress: 37.7% ... Training loss: 0.082 ... Validation loss: 0.212\r",
      "Progress: 37.7% ... Training loss: 0.080 ... Validation loss: 0.240\r",
      "Progress: 37.7% ... Training loss: 0.080 ... Validation loss: 0.238\r",
      "Progress: 37.8% ... Training loss: 0.083 ... Validation loss: 0.222\r",
      "Progress: 37.8% ... Training loss: 0.080 ... Validation loss: 0.249\r",
      "Progress: 37.8% ... Training loss: 0.080 ... Validation loss: 0.230\r",
      "Progress: 37.8% ... Training loss: 0.081 ... Validation loss: 0.249\r",
      "Progress: 37.8% ... Training loss: 0.080 ... Validation loss: 0.221\r",
      "Progress: 37.8% ... Training loss: 0.080 ... Validation loss: 0.241\r",
      "Progress: 37.8% ... Training loss: 0.083 ... Validation loss: 0.214\r",
      "Progress: 37.8% ... Training loss: 0.089 ... Validation loss: 0.272\r",
      "Progress: 37.9% ... Training loss: 0.079 ... Validation loss: 0.226\r",
      "Progress: 37.9% ... Training loss: 0.082 ... Validation loss: 0.266\r",
      "Progress: 37.9% ... Training loss: 0.081 ... Validation loss: 0.231\r",
      "Progress: 37.9% ... Training loss: 0.082 ... Validation loss: 0.275\r",
      "Progress: 37.9% ... Training loss: 0.088 ... Validation loss: 0.220\r",
      "Progress: 37.9% ... Training loss: 0.080 ... Validation loss: 0.250\r",
      "Progress: 37.9% ... Training loss: 0.093 ... Validation loss: 0.215\r",
      "Progress: 37.9% ... Training loss: 0.084 ... Validation loss: 0.264\r",
      "Progress: 38.0% ... Training loss: 0.080 ... Validation loss: 0.210\r",
      "Progress: 38.0% ... Training loss: 0.095 ... Validation loss: 0.258\r",
      "Progress: 38.0% ... Training loss: 0.083 ... Validation loss: 0.216\r",
      "Progress: 38.0% ... Training loss: 0.094 ... Validation loss: 0.293\r",
      "Progress: 38.0% ... Training loss: 0.120 ... Validation loss: 0.222\r",
      "Progress: 38.0% ... Training loss: 0.151 ... Validation loss: 0.372\r",
      "Progress: 38.0% ... Training loss: 0.141 ... Validation loss: 0.211\r",
      "Progress: 38.0% ... Training loss: 0.126 ... Validation loss: 0.347\r",
      "Progress: 38.0% ... Training loss: 0.118 ... Validation loss: 0.221\r",
      "Progress: 38.1% ... Training loss: 0.145 ... Validation loss: 0.377\r",
      "Progress: 38.1% ... Training loss: 0.121 ... Validation loss: 0.209\r",
      "Progress: 38.1% ... Training loss: 0.146 ... Validation loss: 0.382\r",
      "Progress: 38.1% ... Training loss: 0.100 ... Validation loss: 0.217\r",
      "Progress: 38.1% ... Training loss: 0.122 ... Validation loss: 0.318\r",
      "Progress: 38.1% ... Training loss: 0.099 ... Validation loss: 0.210\r",
      "Progress: 38.1% ... Training loss: 0.089 ... Validation loss: 0.296\r",
      "Progress: 38.1% ... Training loss: 0.084 ... Validation loss: 0.240\r",
      "Progress: 38.2% ... Training loss: 0.078 ... Validation loss: 0.240\r",
      "Progress: 38.2% ... Training loss: 0.084 ... Validation loss: 0.227\r",
      "Progress: 38.2% ... Training loss: 0.080 ... Validation loss: 0.274\r",
      "Progress: 38.2% ... Training loss: 0.079 ... Validation loss: 0.237\r",
      "Progress: 38.2% ... Training loss: 0.084 ... Validation loss: 0.247\r",
      "Progress: 38.2% ... Training loss: 0.092 ... Validation loss: 0.226\r",
      "Progress: 38.2% ... Training loss: 0.084 ... Validation loss: 0.269\r",
      "Progress: 38.2% ... Training loss: 0.079 ... Validation loss: 0.249\r",
      "Progress: 38.3% ... Training loss: 0.081 ... Validation loss: 0.227\r",
      "Progress: 38.3% ... Training loss: 0.079 ... Validation loss: 0.242\r",
      "Progress: 38.3% ... Training loss: 0.080 ... Validation loss: 0.262\r",
      "Progress: 38.3% ... Training loss: 0.082 ... Validation loss: 0.272\r",
      "Progress: 38.3% ... Training loss: 0.080 ... Validation loss: 0.260\r",
      "Progress: 38.3% ... Training loss: 0.083 ... Validation loss: 0.241\r",
      "Progress: 38.3% ... Training loss: 0.078 ... Validation loss: 0.238\r",
      "Progress: 38.4% ... Training loss: 0.078 ... Validation loss: 0.229\r",
      "Progress: 38.4% ... Training loss: 0.078 ... Validation loss: 0.239\r",
      "Progress: 38.4% ... Training loss: 0.078 ... Validation loss: 0.237\r",
      "Progress: 38.4% ... Training loss: 0.078 ... Validation loss: 0.255\r",
      "Progress: 38.4% ... Training loss: 0.078 ... Validation loss: 0.256\r",
      "Progress: 38.4% ... Training loss: 0.079 ... Validation loss: 0.264\r",
      "Progress: 38.4% ... Training loss: 0.079 ... Validation loss: 0.244\r",
      "Progress: 38.4% ... Training loss: 0.092 ... Validation loss: 0.313\r",
      "Progress: 38.5% ... Training loss: 0.101 ... Validation loss: 0.221\r",
      "Progress: 38.5% ... Training loss: 0.098 ... Validation loss: 0.344\r",
      "Progress: 38.5% ... Training loss: 0.083 ... Validation loss: 0.235\r",
      "Progress: 38.5% ... Training loss: 0.087 ... Validation loss: 0.304\r",
      "Progress: 38.5% ... Training loss: 0.088 ... Validation loss: 0.246\r",
      "Progress: 38.5% ... Training loss: 0.079 ... Validation loss: 0.281\r",
      "Progress: 38.5% ... Training loss: 0.081 ... Validation loss: 0.286\r",
      "Progress: 38.5% ... Training loss: 0.078 ... Validation loss: 0.253\r",
      "Progress: 38.5% ... Training loss: 0.092 ... Validation loss: 0.305\r",
      "Progress: 38.6% ... Training loss: 0.092 ... Validation loss: 0.213\r",
      "Progress: 38.6% ... Training loss: 0.096 ... Validation loss: 0.305\r",
      "Progress: 38.6% ... Training loss: 0.100 ... Validation loss: 0.213\r",
      "Progress: 38.6% ... Training loss: 0.090 ... Validation loss: 0.301\r",
      "Progress: 38.6% ... Training loss: 0.098 ... Validation loss: 0.208\r",
      "Progress: 38.6% ... Training loss: 0.081 ... Validation loss: 0.272\r",
      "Progress: 38.6% ... Training loss: 0.078 ... Validation loss: 0.228\r",
      "Progress: 38.6% ... Training loss: 0.084 ... Validation loss: 0.211\r",
      "Progress: 38.7% ... Training loss: 0.080 ... Validation loss: 0.247\r",
      "Progress: 38.7% ... Training loss: 0.079 ... Validation loss: 0.240\r",
      "Progress: 38.7% ... Training loss: 0.077 ... Validation loss: 0.229\r",
      "Progress: 38.7% ... Training loss: 0.078 ... Validation loss: 0.242\r",
      "Progress: 38.7% ... Training loss: 0.077 ... Validation loss: 0.245\r",
      "Progress: 38.7% ... Training loss: 0.079 ... Validation loss: 0.252\r",
      "Progress: 38.7% ... Training loss: 0.077 ... Validation loss: 0.228\r",
      "Progress: 38.8% ... Training loss: 0.078 ... Validation loss: 0.230\r",
      "Progress: 38.8% ... Training loss: 0.077 ... Validation loss: 0.245\r",
      "Progress: 38.8% ... Training loss: 0.079 ... Validation loss: 0.220\r",
      "Progress: 38.8% ... Training loss: 0.077 ... Validation loss: 0.226\r",
      "Progress: 38.8% ... Training loss: 0.077 ... Validation loss: 0.233\r",
      "Progress: 38.8% ... Training loss: 0.081 ... Validation loss: 0.216\r",
      "Progress: 38.8% ... Training loss: 0.082 ... Validation loss: 0.263\r",
      "Progress: 38.8% ... Training loss: 0.079 ... Validation loss: 0.217\r",
      "Progress: 38.9% ... Training loss: 0.084 ... Validation loss: 0.266\r",
      "Progress: 38.9% ... Training loss: 0.085 ... Validation loss: 0.212\r",
      "Progress: 38.9% ... Training loss: 0.083 ... Validation loss: 0.259\r",
      "Progress: 38.9% ... Training loss: 0.089 ... Validation loss: 0.209\r",
      "Progress: 38.9% ... Training loss: 0.088 ... Validation loss: 0.267\r",
      "Progress: 38.9% ... Training loss: 0.088 ... Validation loss: 0.210\r",
      "Progress: 38.9% ... Training loss: 0.081 ... Validation loss: 0.246\r",
      "Progress: 38.9% ... Training loss: 0.078 ... Validation loss: 0.224\r",
      "Progress: 39.0% ... Training loss: 0.080 ... Validation loss: 0.253\r",
      "Progress: 39.0% ... Training loss: 0.081 ... Validation loss: 0.222\r",
      "Progress: 39.0% ... Training loss: 0.089 ... Validation loss: 0.273\r",
      "Progress: 39.0% ... Training loss: 0.078 ... Validation loss: 0.241\r",
      "Progress: 39.0% ... Training loss: 0.079 ... Validation loss: 0.253\r",
      "Progress: 39.0% ... Training loss: 0.078 ... Validation loss: 0.248\r",
      "Progress: 39.0% ... Training loss: 0.081 ... Validation loss: 0.228\r",
      "Progress: 39.0% ... Training loss: 0.093 ... Validation loss: 0.276\r",
      "Progress: 39.0% ... Training loss: 0.085 ... Validation loss: 0.199\r",
      "Progress: 39.1% ... Training loss: 0.078 ... Validation loss: 0.248\r",
      "Progress: 39.1% ... Training loss: 0.079 ... Validation loss: 0.214\r",
      "Progress: 39.1% ... Training loss: 0.090 ... Validation loss: 0.277\r",
      "Progress: 39.1% ... Training loss: 0.108 ... Validation loss: 0.201\r",
      "Progress: 39.1% ... Training loss: 0.103 ... Validation loss: 0.280\r",
      "Progress: 39.1% ... Training loss: 0.098 ... Validation loss: 0.190\r",
      "Progress: 39.1% ... Training loss: 0.103 ... Validation loss: 0.271\r",
      "Progress: 39.1% ... Training loss: 0.078 ... Validation loss: 0.195\r",
      "Progress: 39.2% ... Training loss: 0.078 ... Validation loss: 0.225\r",
      "Progress: 39.2% ... Training loss: 0.079 ... Validation loss: 0.238\r",
      "Progress: 39.2% ... Training loss: 0.082 ... Validation loss: 0.215\r",
      "Progress: 39.2% ... Training loss: 0.092 ... Validation loss: 0.270\r",
      "Progress: 39.2% ... Training loss: 0.082 ... Validation loss: 0.205\r",
      "Progress: 39.2% ... Training loss: 0.089 ... Validation loss: 0.267\r",
      "Progress: 39.2% ... Training loss: 0.093 ... Validation loss: 0.198\r",
      "Progress: 39.2% ... Training loss: 0.095 ... Validation loss: 0.282\r",
      "Progress: 39.3% ... Training loss: 0.104 ... Validation loss: 0.200\r",
      "Progress: 39.3% ... Training loss: 0.161 ... Validation loss: 0.373\r",
      "Progress: 39.3% ... Training loss: 0.134 ... Validation loss: 0.194\r",
      "Progress: 39.3% ... Training loss: 0.103 ... Validation loss: 0.275\r",
      "Progress: 39.3% ... Training loss: 0.139 ... Validation loss: 0.200\r",
      "Progress: 39.3% ... Training loss: 0.123 ... Validation loss: 0.307\r",
      "Progress: 39.3% ... Training loss: 0.113 ... Validation loss: 0.187\r",
      "Progress: 39.4% ... Training loss: 0.138 ... Validation loss: 0.337\r",
      "Progress: 39.4% ... Training loss: 0.160 ... Validation loss: 0.193\r",
      "Progress: 39.4% ... Training loss: 0.137 ... Validation loss: 0.340\r",
      "Progress: 39.4% ... Training loss: 0.165 ... Validation loss: 0.200\r",
      "Progress: 39.4% ... Training loss: 0.132 ... Validation loss: 0.342\r",
      "Progress: 39.4% ... Training loss: 0.131 ... Validation loss: 0.203\r",
      "Progress: 39.4% ... Training loss: 0.129 ... Validation loss: 0.344\r",
      "Progress: 39.4% ... Training loss: 0.154 ... Validation loss: 0.197\r",
      "Progress: 39.5% ... Training loss: 0.097 ... Validation loss: 0.279\r",
      "Progress: 39.5% ... Training loss: 0.152 ... Validation loss: 0.192\r",
      "Progress: 39.5% ... Training loss: 0.099 ... Validation loss: 0.258\r",
      "Progress: 39.5% ... Training loss: 0.084 ... Validation loss: 0.199\r",
      "Progress: 39.5% ... Training loss: 0.078 ... Validation loss: 0.225\r",
      "Progress: 39.5% ... Training loss: 0.080 ... Validation loss: 0.197\r",
      "Progress: 39.5% ... Training loss: 0.076 ... Validation loss: 0.221\r",
      "Progress: 39.5% ... Training loss: 0.083 ... Validation loss: 0.197\r",
      "Progress: 39.5% ... Training loss: 0.082 ... Validation loss: 0.241\r",
      "Progress: 39.6% ... Training loss: 0.091 ... Validation loss: 0.185\r",
      "Progress: 39.6% ... Training loss: 0.083 ... Validation loss: 0.223\r",
      "Progress: 39.6% ... Training loss: 0.089 ... Validation loss: 0.181\r",
      "Progress: 39.6% ... Training loss: 0.091 ... Validation loss: 0.251\r",
      "Progress: 39.6% ... Training loss: 0.080 ... Validation loss: 0.200\r",
      "Progress: 39.6% ... Training loss: 0.092 ... Validation loss: 0.238\r",
      "Progress: 39.6% ... Training loss: 0.082 ... Validation loss: 0.192\r",
      "Progress: 39.6% ... Training loss: 0.083 ... Validation loss: 0.237\r",
      "Progress: 39.7% ... Training loss: 0.079 ... Validation loss: 0.200\r",
      "Progress: 39.7% ... Training loss: 0.078 ... Validation loss: 0.219\r",
      "Progress: 39.7% ... Training loss: 0.078 ... Validation loss: 0.214\r",
      "Progress: 39.7% ... Training loss: 0.076 ... Validation loss: 0.227\r",
      "Progress: 39.7% ... Training loss: 0.085 ... Validation loss: 0.197\r",
      "Progress: 39.7% ... Training loss: 0.080 ... Validation loss: 0.245\r",
      "Progress: 39.7% ... Training loss: 0.086 ... Validation loss: 0.201\r",
      "Progress: 39.8% ... Training loss: 0.083 ... Validation loss: 0.254\r",
      "Progress: 39.8% ... Training loss: 0.076 ... Validation loss: 0.202\r",
      "Progress: 39.8% ... Training loss: 0.076 ... Validation loss: 0.215\r",
      "Progress: 39.8% ... Training loss: 0.076 ... Validation loss: 0.229\r",
      "Progress: 39.8% ... Training loss: 0.078 ... Validation loss: 0.211\r",
      "Progress: 39.8% ... Training loss: 0.080 ... Validation loss: 0.243\r",
      "Progress: 39.8% ... Training loss: 0.097 ... Validation loss: 0.195\r",
      "Progress: 39.8% ... Training loss: 0.114 ... Validation loss: 0.283\r",
      "Progress: 39.9% ... Training loss: 0.110 ... Validation loss: 0.188\r",
      "Progress: 39.9% ... Training loss: 0.095 ... Validation loss: 0.283\r",
      "Progress: 39.9% ... Training loss: 0.086 ... Validation loss: 0.197\r",
      "Progress: 39.9% ... Training loss: 0.094 ... Validation loss: 0.280\r",
      "Progress: 39.9% ... Training loss: 0.102 ... Validation loss: 0.192\r",
      "Progress: 39.9% ... Training loss: 0.119 ... Validation loss: 0.318\r",
      "Progress: 39.9% ... Training loss: 0.127 ... Validation loss: 0.191\r",
      "Progress: 39.9% ... Training loss: 0.098 ... Validation loss: 0.271\r",
      "Progress: 40.0% ... Training loss: 0.094 ... Validation loss: 0.205\r",
      "Progress: 40.0% ... Training loss: 0.083 ... Validation loss: 0.258\r",
      "Progress: 40.0% ... Training loss: 0.105 ... Validation loss: 0.191\r",
      "Progress: 40.0% ... Training loss: 0.090 ... Validation loss: 0.266\r",
      "Progress: 40.0% ... Training loss: 0.086 ... Validation loss: 0.201\r",
      "Progress: 40.0% ... Training loss: 0.077 ... Validation loss: 0.232\r",
      "Progress: 40.0% ... Training loss: 0.079 ... Validation loss: 0.199\r",
      "Progress: 40.0% ... Training loss: 0.087 ... Validation loss: 0.254\r",
      "Progress: 40.0% ... Training loss: 0.091 ... Validation loss: 0.193\r",
      "Progress: 40.1% ... Training loss: 0.079 ... Validation loss: 0.212\r",
      "Progress: 40.1% ... Training loss: 0.094 ... Validation loss: 0.259\r",
      "Progress: 40.1% ... Training loss: 0.098 ... Validation loss: 0.186\r",
      "Progress: 40.1% ... Training loss: 0.096 ... Validation loss: 0.247\r",
      "Progress: 40.1% ... Training loss: 0.085 ... Validation loss: 0.191\r",
      "Progress: 40.1% ... Training loss: 0.090 ... Validation loss: 0.248\r",
      "Progress: 40.1% ... Training loss: 0.100 ... Validation loss: 0.191\r",
      "Progress: 40.1% ... Training loss: 0.121 ... Validation loss: 0.294\r",
      "Progress: 40.2% ... Training loss: 0.106 ... Validation loss: 0.187\r",
      "Progress: 40.2% ... Training loss: 0.078 ... Validation loss: 0.242\r",
      "Progress: 40.2% ... Training loss: 0.075 ... Validation loss: 0.224\r",
      "Progress: 40.2% ... Training loss: 0.077 ... Validation loss: 0.219\r",
      "Progress: 40.2% ... Training loss: 0.081 ... Validation loss: 0.233\r",
      "Progress: 40.2% ... Training loss: 0.076 ... Validation loss: 0.201\r",
      "Progress: 40.2% ... Training loss: 0.077 ... Validation loss: 0.223\r",
      "Progress: 40.2% ... Training loss: 0.080 ... Validation loss: 0.205\r",
      "Progress: 40.3% ... Training loss: 0.077 ... Validation loss: 0.226\r",
      "Progress: 40.3% ... Training loss: 0.076 ... Validation loss: 0.213\r",
      "Progress: 40.3% ... Training loss: 0.082 ... Validation loss: 0.191\r",
      "Progress: 40.3% ... Training loss: 0.075 ... Validation loss: 0.215\r",
      "Progress: 40.3% ... Training loss: 0.086 ... Validation loss: 0.197\r",
      "Progress: 40.3% ... Training loss: 0.088 ... Validation loss: 0.247\r",
      "Progress: 40.3% ... Training loss: 0.085 ... Validation loss: 0.185\r",
      "Progress: 40.4% ... Training loss: 0.078 ... Validation loss: 0.211\r",
      "Progress: 40.4% ... Training loss: 0.075 ... Validation loss: 0.197\r",
      "Progress: 40.4% ... Training loss: 0.074 ... Validation loss: 0.212\r",
      "Progress: 40.4% ... Training loss: 0.077 ... Validation loss: 0.245\r",
      "Progress: 40.4% ... Training loss: 0.076 ... Validation loss: 0.227\r",
      "Progress: 40.4% ... Training loss: 0.076 ... Validation loss: 0.236\r",
      "Progress: 40.4% ... Training loss: 0.076 ... Validation loss: 0.213\r",
      "Progress: 40.4% ... Training loss: 0.079 ... Validation loss: 0.238\r",
      "Progress: 40.5% ... Training loss: 0.074 ... Validation loss: 0.221\r",
      "Progress: 40.5% ... Training loss: 0.074 ... Validation loss: 0.231\r",
      "Progress: 40.5% ... Training loss: 0.074 ... Validation loss: 0.233\r",
      "Progress: 40.5% ... Training loss: 0.075 ... Validation loss: 0.241\r",
      "Progress: 40.5% ... Training loss: 0.082 ... Validation loss: 0.216\r",
      "Progress: 40.5% ... Training loss: 0.078 ... Validation loss: 0.261\r",
      "Progress: 40.5% ... Training loss: 0.081 ... Validation loss: 0.196\r",
      "Progress: 40.5% ... Training loss: 0.077 ... Validation loss: 0.231\r",
      "Progress: 40.5% ... Training loss: 0.088 ... Validation loss: 0.200\r",
      "Progress: 40.6% ... Training loss: 0.078 ... Validation loss: 0.244\r",
      "Progress: 40.6% ... Training loss: 0.074 ... Validation loss: 0.221\r",
      "Progress: 40.6% ... Training loss: 0.078 ... Validation loss: 0.204\r",
      "Progress: 40.6% ... Training loss: 0.084 ... Validation loss: 0.243\r",
      "Progress: 40.6% ... Training loss: 0.111 ... Validation loss: 0.201\r",
      "Progress: 40.6% ... Training loss: 0.145 ... Validation loss: 0.337\r",
      "Progress: 40.6% ... Training loss: 0.084 ... Validation loss: 0.207\r",
      "Progress: 40.6% ... Training loss: 0.076 ... Validation loss: 0.229\r",
      "Progress: 40.7% ... Training loss: 0.082 ... Validation loss: 0.208\r",
      "Progress: 40.7% ... Training loss: 0.090 ... Validation loss: 0.270\r",
      "Progress: 40.7% ... Training loss: 0.138 ... Validation loss: 0.204\r",
      "Progress: 40.7% ... Training loss: 0.127 ... Validation loss: 0.339\r",
      "Progress: 40.7% ... Training loss: 0.179 ... Validation loss: 0.196\r",
      "Progress: 40.7% ... Training loss: 0.143 ... Validation loss: 0.346\r",
      "Progress: 40.7% ... Training loss: 0.117 ... Validation loss: 0.186\r",
      "Progress: 40.8% ... Training loss: 0.117 ... Validation loss: 0.298\r",
      "Progress: 40.8% ... Training loss: 0.141 ... Validation loss: 0.187\r",
      "Progress: 40.8% ... Training loss: 0.142 ... Validation loss: 0.339\r",
      "Progress: 40.8% ... Training loss: 0.129 ... Validation loss: 0.184\r",
      "Progress: 40.8% ... Training loss: 0.145 ... Validation loss: 0.323\r",
      "Progress: 40.8% ... Training loss: 0.101 ... Validation loss: 0.186\r",
      "Progress: 40.8% ... Training loss: 0.154 ... Validation loss: 0.375\r",
      "Progress: 40.8% ... Training loss: 0.104 ... Validation loss: 0.189\r",
      "Progress: 40.9% ... Training loss: 0.097 ... Validation loss: 0.290\r",
      "Progress: 40.9% ... Training loss: 0.098 ... Validation loss: 0.208\r",
      "Progress: 40.9% ... Training loss: 0.081 ... Validation loss: 0.283\r",
      "Progress: 40.9% ... Training loss: 0.090 ... Validation loss: 0.201\r",
      "Progress: 40.9% ... Training loss: 0.077 ... Validation loss: 0.237\r",
      "Progress: 40.9% ... Training loss: 0.080 ... Validation loss: 0.199\r",
      "Progress: 40.9% ... Training loss: 0.076 ... Validation loss: 0.227\r",
      "Progress: 40.9% ... Training loss: 0.075 ... Validation loss: 0.212\r",
      "Progress: 41.0% ... Training loss: 0.074 ... Validation loss: 0.229\r",
      "Progress: 41.0% ... Training loss: 0.075 ... Validation loss: 0.218\r",
      "Progress: 41.0% ... Training loss: 0.080 ... Validation loss: 0.249\r",
      "Progress: 41.0% ... Training loss: 0.079 ... Validation loss: 0.211\r",
      "Progress: 41.0% ... Training loss: 0.076 ... Validation loss: 0.200\r",
      "Progress: 41.0% ... Training loss: 0.082 ... Validation loss: 0.236\r",
      "Progress: 41.0% ... Training loss: 0.079 ... Validation loss: 0.210\r",
      "Progress: 41.0% ... Training loss: 0.075 ... Validation loss: 0.234\r",
      "Progress: 41.0% ... Training loss: 0.074 ... Validation loss: 0.226\r",
      "Progress: 41.1% ... Training loss: 0.074 ... Validation loss: 0.225\r",
      "Progress: 41.1% ... Training loss: 0.078 ... Validation loss: 0.208\r",
      "Progress: 41.1% ... Training loss: 0.078 ... Validation loss: 0.248\r",
      "Progress: 41.1% ... Training loss: 0.075 ... Validation loss: 0.207\r",
      "Progress: 41.1% ... Training loss: 0.074 ... Validation loss: 0.214\r",
      "Progress: 41.1% ... Training loss: 0.076 ... Validation loss: 0.237\r",
      "Progress: 41.1% ... Training loss: 0.080 ... Validation loss: 0.189\r",
      "Progress: 41.1% ... Training loss: 0.093 ... Validation loss: 0.246\r",
      "Progress: 41.2% ... Training loss: 0.076 ... Validation loss: 0.206\r",
      "Progress: 41.2% ... Training loss: 0.074 ... Validation loss: 0.200\r",
      "Progress: 41.2% ... Training loss: 0.077 ... Validation loss: 0.217\r",
      "Progress: 41.2% ... Training loss: 0.075 ... Validation loss: 0.196\r",
      "Progress: 41.2% ... Training loss: 0.074 ... Validation loss: 0.201\r",
      "Progress: 41.2% ... Training loss: 0.081 ... Validation loss: 0.192\r",
      "Progress: 41.2% ... Training loss: 0.075 ... Validation loss: 0.216\r",
      "Progress: 41.2% ... Training loss: 0.074 ... Validation loss: 0.217\r",
      "Progress: 41.3% ... Training loss: 0.074 ... Validation loss: 0.213\r",
      "Progress: 41.3% ... Training loss: 0.074 ... Validation loss: 0.221\r",
      "Progress: 41.3% ... Training loss: 0.074 ... Validation loss: 0.210\r",
      "Progress: 41.3% ... Training loss: 0.083 ... Validation loss: 0.230\r",
      "Progress: 41.3% ... Training loss: 0.094 ... Validation loss: 0.186\r",
      "Progress: 41.3% ... Training loss: 0.081 ... Validation loss: 0.249\r",
      "Progress: 41.3% ... Training loss: 0.074 ... Validation loss: 0.207\r",
      "Progress: 41.4% ... Training loss: 0.074 ... Validation loss: 0.229\r",
      "Progress: 41.4% ... Training loss: 0.074 ... Validation loss: 0.202\r",
      "Progress: 41.4% ... Training loss: 0.111 ... Validation loss: 0.305\r",
      "Progress: 41.4% ... Training loss: 0.119 ... Validation loss: 0.199\r",
      "Progress: 41.4% ... Training loss: 0.135 ... Validation loss: 0.341\r",
      "Progress: 41.4% ... Training loss: 0.097 ... Validation loss: 0.190\r",
      "Progress: 41.4% ... Training loss: 0.086 ... Validation loss: 0.291\r",
      "Progress: 41.4% ... Training loss: 0.091 ... Validation loss: 0.201\r",
      "Progress: 41.5% ... Training loss: 0.078 ... Validation loss: 0.254\r",
      "Progress: 41.5% ... Training loss: 0.077 ... Validation loss: 0.209\r",
      "Progress: 41.5% ... Training loss: 0.094 ... Validation loss: 0.304\r",
      "Progress: 41.5% ... Training loss: 0.085 ... Validation loss: 0.212\r",
      "Progress: 41.5% ... Training loss: 0.085 ... Validation loss: 0.277\r",
      "Progress: 41.5% ... Training loss: 0.086 ... Validation loss: 0.204\r",
      "Progress: 41.5% ... Training loss: 0.075 ... Validation loss: 0.259\r",
      "Progress: 41.5% ... Training loss: 0.074 ... Validation loss: 0.242\r",
      "Progress: 41.5% ... Training loss: 0.079 ... Validation loss: 0.258\r",
      "Progress: 41.6% ... Training loss: 0.078 ... Validation loss: 0.204\r",
      "Progress: 41.6% ... Training loss: 0.089 ... Validation loss: 0.270\r",
      "Progress: 41.6% ... Training loss: 0.099 ... Validation loss: 0.196\r",
      "Progress: 41.6% ... Training loss: 0.122 ... Validation loss: 0.308\r",
      "Progress: 41.6% ... Training loss: 0.099 ... Validation loss: 0.184\r",
      "Progress: 41.6% ... Training loss: 0.085 ... Validation loss: 0.261\r",
      "Progress: 41.6% ... Training loss: 0.082 ... Validation loss: 0.198\r",
      "Progress: 41.6% ... Training loss: 0.084 ... Validation loss: 0.264\r",
      "Progress: 41.7% ... Training loss: 0.073 ... Validation loss: 0.211\r",
      "Progress: 41.7% ... Training loss: 0.089 ... Validation loss: 0.272\r",
      "Progress: 41.7% ... Training loss: 0.093 ... Validation loss: 0.192\r",
      "Progress: 41.7% ... Training loss: 0.072 ... Validation loss: 0.225\r",
      "Progress: 41.7% ... Training loss: 0.080 ... Validation loss: 0.235\r",
      "Progress: 41.7% ... Training loss: 0.087 ... Validation loss: 0.189\r",
      "Progress: 41.7% ... Training loss: 0.097 ... Validation loss: 0.267\r",
      "Progress: 41.8% ... Training loss: 0.075 ... Validation loss: 0.215\r",
      "Progress: 41.8% ... Training loss: 0.075 ... Validation loss: 0.228\r",
      "Progress: 41.8% ... Training loss: 0.078 ... Validation loss: 0.216\r",
      "Progress: 41.8% ... Training loss: 0.082 ... Validation loss: 0.259\r",
      "Progress: 41.8% ... Training loss: 0.103 ... Validation loss: 0.200\r",
      "Progress: 41.8% ... Training loss: 0.095 ... Validation loss: 0.287\r",
      "Progress: 41.8% ... Training loss: 0.076 ... Validation loss: 0.213\r",
      "Progress: 41.8% ... Training loss: 0.090 ... Validation loss: 0.273\r",
      "Progress: 41.9% ... Training loss: 0.079 ... Validation loss: 0.210\r",
      "Progress: 41.9% ... Training loss: 0.073 ... Validation loss: 0.225\r",
      "Progress: 41.9% ... Training loss: 0.075 ... Validation loss: 0.238\r",
      "Progress: 41.9% ... Training loss: 0.075 ... Validation loss: 0.210\r",
      "Progress: 41.9% ... Training loss: 0.073 ... Validation loss: 0.234\r",
      "Progress: 41.9% ... Training loss: 0.079 ... Validation loss: 0.212\r",
      "Progress: 41.9% ... Training loss: 0.106 ... Validation loss: 0.294\r",
      "Progress: 41.9% ... Training loss: 0.088 ... Validation loss: 0.205\r",
      "Progress: 42.0% ... Training loss: 0.085 ... Validation loss: 0.271\r",
      "Progress: 42.0% ... Training loss: 0.098 ... Validation loss: 0.194\r",
      "Progress: 42.0% ... Training loss: 0.096 ... Validation loss: 0.271\r",
      "Progress: 42.0% ... Training loss: 0.092 ... Validation loss: 0.201\r",
      "Progress: 42.0% ... Training loss: 0.081 ... Validation loss: 0.268\r",
      "Progress: 42.0% ... Training loss: 0.082 ... Validation loss: 0.204\r",
      "Progress: 42.0% ... Training loss: 0.090 ... Validation loss: 0.269\r",
      "Progress: 42.0% ... Training loss: 0.113 ... Validation loss: 0.199\r",
      "Progress: 42.0% ... Training loss: 0.090 ... Validation loss: 0.273\r",
      "Progress: 42.1% ... Training loss: 0.091 ... Validation loss: 0.199\r",
      "Progress: 42.1% ... Training loss: 0.085 ... Validation loss: 0.262\r",
      "Progress: 42.1% ... Training loss: 0.077 ... Validation loss: 0.194\r",
      "Progress: 42.1% ... Training loss: 0.074 ... Validation loss: 0.208\r",
      "Progress: 42.1% ... Training loss: 0.075 ... Validation loss: 0.199\r",
      "Progress: 42.1% ... Training loss: 0.075 ... Validation loss: 0.217\r",
      "Progress: 42.1% ... Training loss: 0.073 ... Validation loss: 0.222\r",
      "Progress: 42.1% ... Training loss: 0.074 ... Validation loss: 0.212\r",
      "Progress: 42.2% ... Training loss: 0.078 ... Validation loss: 0.199\r",
      "Progress: 42.2% ... Training loss: 0.073 ... Validation loss: 0.214\r",
      "Progress: 42.2% ... Training loss: 0.079 ... Validation loss: 0.224\r",
      "Progress: 42.2% ... Training loss: 0.075 ... Validation loss: 0.198\r",
      "Progress: 42.2% ... Training loss: 0.072 ... Validation loss: 0.209\r",
      "Progress: 42.2% ... Training loss: 0.072 ... Validation loss: 0.216\r",
      "Progress: 42.2% ... Training loss: 0.075 ... Validation loss: 0.226\r",
      "Progress: 42.2% ... Training loss: 0.076 ... Validation loss: 0.207\r",
      "Progress: 42.3% ... Training loss: 0.082 ... Validation loss: 0.248\r",
      "Progress: 42.3% ... Training loss: 0.091 ... Validation loss: 0.185\r",
      "Progress: 42.3% ... Training loss: 0.095 ... Validation loss: 0.257\r",
      "Progress: 42.3% ... Training loss: 0.081 ... Validation loss: 0.194\r",
      "Progress: 42.3% ... Training loss: 0.094 ... Validation loss: 0.279\r",
      "Progress: 42.3% ... Training loss: 0.092 ... Validation loss: 0.194\r",
      "Progress: 42.3% ... Training loss: 0.113 ... Validation loss: 0.342\r",
      "Progress: 42.4% ... Training loss: 0.107 ... Validation loss: 0.189\r",
      "Progress: 42.4% ... Training loss: 0.091 ... Validation loss: 0.276\r",
      "Progress: 42.4% ... Training loss: 0.094 ... Validation loss: 0.201\r",
      "Progress: 42.4% ... Training loss: 0.097 ... Validation loss: 0.269\r",
      "Progress: 42.4% ... Training loss: 0.096 ... Validation loss: 0.177\r",
      "Progress: 42.4% ... Training loss: 0.087 ... Validation loss: 0.246\r",
      "Progress: 42.4% ... Training loss: 0.091 ... Validation loss: 0.191\r",
      "Progress: 42.4% ... Training loss: 0.096 ... Validation loss: 0.268\r",
      "Progress: 42.5% ... Training loss: 0.080 ... Validation loss: 0.189\r",
      "Progress: 42.5% ... Training loss: 0.084 ... Validation loss: 0.232\r",
      "Progress: 42.5% ... Training loss: 0.072 ... Validation loss: 0.207\r",
      "Progress: 42.5% ... Training loss: 0.077 ... Validation loss: 0.239\r",
      "Progress: 42.5% ... Training loss: 0.073 ... Validation loss: 0.206\r",
      "Progress: 42.5% ... Training loss: 0.078 ... Validation loss: 0.245\r",
      "Progress: 42.5% ... Training loss: 0.077 ... Validation loss: 0.201\r",
      "Progress: 42.5% ... Training loss: 0.075 ... Validation loss: 0.235\r",
      "Progress: 42.5% ... Training loss: 0.075 ... Validation loss: 0.208\r",
      "Progress: 42.6% ... Training loss: 0.083 ... Validation loss: 0.258\r",
      "Progress: 42.6% ... Training loss: 0.083 ... Validation loss: 0.198\r",
      "Progress: 42.6% ... Training loss: 0.077 ... Validation loss: 0.220\r",
      "Progress: 42.6% ... Training loss: 0.079 ... Validation loss: 0.198\r",
      "Progress: 42.6% ... Training loss: 0.075 ... Validation loss: 0.213\r",
      "Progress: 42.6% ... Training loss: 0.079 ... Validation loss: 0.182\r",
      "Progress: 42.6% ... Training loss: 0.073 ... Validation loss: 0.217\r",
      "Progress: 42.6% ... Training loss: 0.072 ... Validation loss: 0.192\r",
      "Progress: 42.7% ... Training loss: 0.072 ... Validation loss: 0.205\r",
      "Progress: 42.7% ... Training loss: 0.073 ... Validation loss: 0.199\r",
      "Progress: 42.7% ... Training loss: 0.078 ... Validation loss: 0.228\r",
      "Progress: 42.7% ... Training loss: 0.082 ... Validation loss: 0.198\r",
      "Progress: 42.7% ... Training loss: 0.087 ... Validation loss: 0.257\r",
      "Progress: 42.7% ... Training loss: 0.087 ... Validation loss: 0.188\r",
      "Progress: 42.7% ... Training loss: 0.091 ... Validation loss: 0.239\r",
      "Progress: 42.8% ... Training loss: 0.074 ... Validation loss: 0.191\r",
      "Progress: 42.8% ... Training loss: 0.071 ... Validation loss: 0.204\r",
      "Progress: 42.8% ... Training loss: 0.073 ... Validation loss: 0.196\r",
      "Progress: 42.8% ... Training loss: 0.072 ... Validation loss: 0.223\r",
      "Progress: 42.8% ... Training loss: 0.075 ... Validation loss: 0.208\r",
      "Progress: 42.8% ... Training loss: 0.076 ... Validation loss: 0.260\r",
      "Progress: 42.8% ... Training loss: 0.073 ... Validation loss: 0.215\r",
      "Progress: 42.8% ... Training loss: 0.071 ... Validation loss: 0.224\r",
      "Progress: 42.9% ... Training loss: 0.074 ... Validation loss: 0.233\r",
      "Progress: 42.9% ... Training loss: 0.074 ... Validation loss: 0.220\r",
      "Progress: 42.9% ... Training loss: 0.072 ... Validation loss: 0.216\r",
      "Progress: 42.9% ... Training loss: 0.090 ... Validation loss: 0.197\r",
      "Progress: 42.9% ... Training loss: 0.077 ... Validation loss: 0.247\r",
      "Progress: 42.9% ... Training loss: 0.073 ... Validation loss: 0.231\r",
      "Progress: 42.9% ... Training loss: 0.073 ... Validation loss: 0.227\r",
      "Progress: 42.9% ... Training loss: 0.071 ... Validation loss: 0.221\r",
      "Progress: 43.0% ... Training loss: 0.071 ... Validation loss: 0.212\r",
      "Progress: 43.0% ... Training loss: 0.073 ... Validation loss: 0.210\r",
      "Progress: 43.0% ... Training loss: 0.080 ... Validation loss: 0.262\r",
      "Progress: 43.0% ... Training loss: 0.076 ... Validation loss: 0.197\r",
      "Progress: 43.0% ... Training loss: 0.079 ... Validation loss: 0.258\r",
      "Progress: 43.0% ... Training loss: 0.094 ... Validation loss: 0.193\r",
      "Progress: 43.0% ... Training loss: 0.097 ... Validation loss: 0.331\r",
      "Progress: 43.0% ... Training loss: 0.122 ... Validation loss: 0.192\r",
      "Progress: 43.0% ... Training loss: 0.085 ... Validation loss: 0.301\r",
      "Progress: 43.1% ... Training loss: 0.084 ... Validation loss: 0.190\r",
      "Progress: 43.1% ... Training loss: 0.087 ... Validation loss: 0.257\r",
      "Progress: 43.1% ... Training loss: 0.072 ... Validation loss: 0.202\r",
      "Progress: 43.1% ... Training loss: 0.081 ... Validation loss: 0.247\r",
      "Progress: 43.1% ... Training loss: 0.078 ... Validation loss: 0.200\r",
      "Progress: 43.1% ... Training loss: 0.103 ... Validation loss: 0.287\r",
      "Progress: 43.1% ... Training loss: 0.115 ... Validation loss: 0.183\r",
      "Progress: 43.1% ... Training loss: 0.120 ... Validation loss: 0.299\r",
      "Progress: 43.2% ... Training loss: 0.124 ... Validation loss: 0.181\r",
      "Progress: 43.2% ... Training loss: 0.093 ... Validation loss: 0.251\r",
      "Progress: 43.2% ... Training loss: 0.100 ... Validation loss: 0.188\r",
      "Progress: 43.2% ... Training loss: 0.079 ... Validation loss: 0.233\r",
      "Progress: 43.2% ... Training loss: 0.082 ... Validation loss: 0.201\r",
      "Progress: 43.2% ... Training loss: 0.073 ... Validation loss: 0.232\r",
      "Progress: 43.2% ... Training loss: 0.074 ... Validation loss: 0.204\r",
      "Progress: 43.2% ... Training loss: 0.072 ... Validation loss: 0.221\r",
      "Progress: 43.3% ... Training loss: 0.072 ... Validation loss: 0.191\r",
      "Progress: 43.3% ... Training loss: 0.088 ... Validation loss: 0.253\r",
      "Progress: 43.3% ... Training loss: 0.092 ... Validation loss: 0.192\r",
      "Progress: 43.3% ... Training loss: 0.072 ... Validation loss: 0.231\r",
      "Progress: 43.3% ... Training loss: 0.075 ... Validation loss: 0.204\r",
      "Progress: 43.3% ... Training loss: 0.082 ... Validation loss: 0.264\r",
      "Progress: 43.3% ... Training loss: 0.079 ... Validation loss: 0.192\r",
      "Progress: 43.4% ... Training loss: 0.072 ... Validation loss: 0.223\r",
      "Progress: 43.4% ... Training loss: 0.073 ... Validation loss: 0.192\r",
      "Progress: 43.4% ... Training loss: 0.081 ... Validation loss: 0.248\r",
      "Progress: 43.4% ... Training loss: 0.079 ... Validation loss: 0.189\r",
      "Progress: 43.4% ... Training loss: 0.072 ... Validation loss: 0.227\r",
      "Progress: 43.4% ... Training loss: 0.071 ... Validation loss: 0.198\r",
      "Progress: 43.4% ... Training loss: 0.073 ... Validation loss: 0.203\r",
      "Progress: 43.4% ... Training loss: 0.071 ... Validation loss: 0.216\r",
      "Progress: 43.5% ... Training loss: 0.071 ... Validation loss: 0.221\r",
      "Progress: 43.5% ... Training loss: 0.087 ... Validation loss: 0.182\r",
      "Progress: 43.5% ... Training loss: 0.091 ... Validation loss: 0.244\r",
      "Progress: 43.5% ... Training loss: 0.089 ... Validation loss: 0.189\r",
      "Progress: 43.5% ... Training loss: 0.077 ... Validation loss: 0.228\r",
      "Progress: 43.5% ... Training loss: 0.071 ... Validation loss: 0.217\r",
      "Progress: 43.5% ... Training loss: 0.074 ... Validation loss: 0.236\r",
      "Progress: 43.5% ... Training loss: 0.075 ... Validation loss: 0.211\r",
      "Progress: 43.5% ... Training loss: 0.082 ... Validation loss: 0.256\r",
      "Progress: 43.6% ... Training loss: 0.088 ... Validation loss: 0.184\r",
      "Progress: 43.6% ... Training loss: 0.086 ... Validation loss: 0.288\r",
      "Progress: 43.6% ... Training loss: 0.106 ... Validation loss: 0.197\r",
      "Progress: 43.6% ... Training loss: 0.081 ... Validation loss: 0.278\r",
      "Progress: 43.6% ... Training loss: 0.074 ... Validation loss: 0.219\r",
      "Progress: 43.6% ... Training loss: 0.073 ... Validation loss: 0.250\r",
      "Progress: 43.6% ... Training loss: 0.072 ... Validation loss: 0.241\r",
      "Progress: 43.6% ... Training loss: 0.075 ... Validation loss: 0.226\r",
      "Progress: 43.7% ... Training loss: 0.073 ... Validation loss: 0.230\r",
      "Progress: 43.7% ... Training loss: 0.072 ... Validation loss: 0.224\r",
      "Progress: 43.7% ... Training loss: 0.071 ... Validation loss: 0.233\r",
      "Progress: 43.7% ... Training loss: 0.072 ... Validation loss: 0.216\r",
      "Progress: 43.7% ... Training loss: 0.072 ... Validation loss: 0.221\r",
      "Progress: 43.7% ... Training loss: 0.071 ... Validation loss: 0.226\r",
      "Progress: 43.7% ... Training loss: 0.075 ... Validation loss: 0.206\r",
      "Progress: 43.8% ... Training loss: 0.077 ... Validation loss: 0.195\r",
      "Progress: 43.8% ... Training loss: 0.073 ... Validation loss: 0.205\r",
      "Progress: 43.8% ... Training loss: 0.079 ... Validation loss: 0.196\r",
      "Progress: 43.8% ... Training loss: 0.078 ... Validation loss: 0.232\r",
      "Progress: 43.8% ... Training loss: 0.078 ... Validation loss: 0.195\r",
      "Progress: 43.8% ... Training loss: 0.083 ... Validation loss: 0.254\r",
      "Progress: 43.8% ... Training loss: 0.084 ... Validation loss: 0.194\r",
      "Progress: 43.8% ... Training loss: 0.075 ... Validation loss: 0.245\r",
      "Progress: 43.9% ... Training loss: 0.079 ... Validation loss: 0.186\r",
      "Progress: 43.9% ... Training loss: 0.079 ... Validation loss: 0.231\r",
      "Progress: 43.9% ... Training loss: 0.102 ... Validation loss: 0.185\r",
      "Progress: 43.9% ... Training loss: 0.075 ... Validation loss: 0.230\r",
      "Progress: 43.9% ... Training loss: 0.072 ... Validation loss: 0.206\r",
      "Progress: 43.9% ... Training loss: 0.072 ... Validation loss: 0.198\r",
      "Progress: 43.9% ... Training loss: 0.071 ... Validation loss: 0.212\r",
      "Progress: 43.9% ... Training loss: 0.083 ... Validation loss: 0.184\r",
      "Progress: 44.0% ... Training loss: 0.076 ... Validation loss: 0.224\r",
      "Progress: 44.0% ... Training loss: 0.073 ... Validation loss: 0.194\r",
      "Progress: 44.0% ... Training loss: 0.071 ... Validation loss: 0.219\r",
      "Progress: 44.0% ... Training loss: 0.075 ... Validation loss: 0.197\r",
      "Progress: 44.0% ... Training loss: 0.076 ... Validation loss: 0.241\r",
      "Progress: 44.0% ... Training loss: 0.071 ... Validation loss: 0.213\r",
      "Progress: 44.0% ... Training loss: 0.071 ... Validation loss: 0.211\r",
      "Progress: 44.0% ... Training loss: 0.075 ... Validation loss: 0.223\r",
      "Progress: 44.0% ... Training loss: 0.071 ... Validation loss: 0.246\r",
      "Progress: 44.1% ... Training loss: 0.071 ... Validation loss: 0.219\r",
      "Progress: 44.1% ... Training loss: 0.072 ... Validation loss: 0.236\r",
      "Progress: 44.1% ... Training loss: 0.075 ... Validation loss: 0.226\r",
      "Progress: 44.1% ... Training loss: 0.073 ... Validation loss: 0.215\r",
      "Progress: 44.1% ... Training loss: 0.090 ... Validation loss: 0.185\r",
      "Progress: 44.1% ... Training loss: 0.083 ... Validation loss: 0.252\r",
      "Progress: 44.1% ... Training loss: 0.074 ... Validation loss: 0.204\r",
      "Progress: 44.1% ... Training loss: 0.071 ... Validation loss: 0.218\r",
      "Progress: 44.2% ... Training loss: 0.077 ... Validation loss: 0.260\r",
      "Progress: 44.2% ... Training loss: 0.095 ... Validation loss: 0.193\r",
      "Progress: 44.2% ... Training loss: 0.079 ... Validation loss: 0.228\r",
      "Progress: 44.2% ... Training loss: 0.070 ... Validation loss: 0.199\r",
      "Progress: 44.2% ... Training loss: 0.071 ... Validation loss: 0.203\r",
      "Progress: 44.2% ... Training loss: 0.071 ... Validation loss: 0.220\r",
      "Progress: 44.2% ... Training loss: 0.070 ... Validation loss: 0.206\r",
      "Progress: 44.2% ... Training loss: 0.073 ... Validation loss: 0.215\r",
      "Progress: 44.3% ... Training loss: 0.071 ... Validation loss: 0.202\r",
      "Progress: 44.3% ... Training loss: 0.073 ... Validation loss: 0.197\r",
      "Progress: 44.3% ... Training loss: 0.073 ... Validation loss: 0.194\r",
      "Progress: 44.3% ... Training loss: 0.088 ... Validation loss: 0.253\r",
      "Progress: 44.3% ... Training loss: 0.084 ... Validation loss: 0.185\r",
      "Progress: 44.3% ... Training loss: 0.105 ... Validation loss: 0.281\r",
      "Progress: 44.3% ... Training loss: 0.082 ... Validation loss: 0.190\r",
      "Progress: 44.4% ... Training loss: 0.088 ... Validation loss: 0.257\r",
      "Progress: 44.4% ... Training loss: 0.074 ... Validation loss: 0.194\r",
      "Progress: 44.4% ... Training loss: 0.077 ... Validation loss: 0.250\r",
      "Progress: 44.4% ... Training loss: 0.074 ... Validation loss: 0.207\r",
      "Progress: 44.4% ... Training loss: 0.069 ... Validation loss: 0.224\r",
      "Progress: 44.4% ... Training loss: 0.070 ... Validation loss: 0.238\r",
      "Progress: 44.4% ... Training loss: 0.070 ... Validation loss: 0.207\r",
      "Progress: 44.4% ... Training loss: 0.069 ... Validation loss: 0.215\r",
      "Progress: 44.5% ... Training loss: 0.074 ... Validation loss: 0.203\r",
      "Progress: 44.5% ... Training loss: 0.080 ... Validation loss: 0.250\r",
      "Progress: 44.5% ... Training loss: 0.077 ... Validation loss: 0.202\r",
      "Progress: 44.5% ... Training loss: 0.082 ... Validation loss: 0.267\r",
      "Progress: 44.5% ... Training loss: 0.086 ... Validation loss: 0.192\r",
      "Progress: 44.5% ... Training loss: 0.080 ... Validation loss: 0.276\r",
      "Progress: 44.5% ... Training loss: 0.069 ... Validation loss: 0.225\r",
      "Progress: 44.5% ... Training loss: 0.069 ... Validation loss: 0.233\r",
      "Progress: 44.5% ... Training loss: 0.071 ... Validation loss: 0.252\r",
      "Progress: 44.6% ... Training loss: 0.071 ... Validation loss: 0.213\r",
      "Progress: 44.6% ... Training loss: 0.078 ... Validation loss: 0.253\r",
      "Progress: 44.6% ... Training loss: 0.077 ... Validation loss: 0.208\r",
      "Progress: 44.6% ... Training loss: 0.073 ... Validation loss: 0.241\r",
      "Progress: 44.6% ... Training loss: 0.070 ... Validation loss: 0.218\r",
      "Progress: 44.6% ... Training loss: 0.073 ... Validation loss: 0.206\r",
      "Progress: 44.6% ... Training loss: 0.075 ... Validation loss: 0.278\r",
      "Progress: 44.6% ... Training loss: 0.072 ... Validation loss: 0.213\r",
      "Progress: 44.7% ... Training loss: 0.070 ... Validation loss: 0.244\r",
      "Progress: 44.7% ... Training loss: 0.071 ... Validation loss: 0.218\r",
      "Progress: 44.7% ... Training loss: 0.069 ... Validation loss: 0.222\r",
      "Progress: 44.7% ... Training loss: 0.075 ... Validation loss: 0.262\r",
      "Progress: 44.7% ... Training loss: 0.078 ... Validation loss: 0.206\r",
      "Progress: 44.7% ... Training loss: 0.072 ... Validation loss: 0.246\r",
      "Progress: 44.7% ... Training loss: 0.074 ... Validation loss: 0.205\r",
      "Progress: 44.8% ... Training loss: 0.082 ... Validation loss: 0.259\r",
      "Progress: 44.8% ... Training loss: 0.077 ... Validation loss: 0.202\r",
      "Progress: 44.8% ... Training loss: 0.088 ... Validation loss: 0.276\r",
      "Progress: 44.8% ... Training loss: 0.074 ... Validation loss: 0.208\r",
      "Progress: 44.8% ... Training loss: 0.071 ... Validation loss: 0.194\r",
      "Progress: 44.8% ... Training loss: 0.073 ... Validation loss: 0.205\r",
      "Progress: 44.8% ... Training loss: 0.070 ... Validation loss: 0.203\r",
      "Progress: 44.8% ... Training loss: 0.075 ... Validation loss: 0.193\r",
      "Progress: 44.9% ... Training loss: 0.071 ... Validation loss: 0.231\r",
      "Progress: 44.9% ... Training loss: 0.075 ... Validation loss: 0.199\r",
      "Progress: 44.9% ... Training loss: 0.077 ... Validation loss: 0.241\r",
      "Progress: 44.9% ... Training loss: 0.071 ... Validation loss: 0.199\r",
      "Progress: 44.9% ... Training loss: 0.075 ... Validation loss: 0.231\r",
      "Progress: 44.9% ... Training loss: 0.071 ... Validation loss: 0.197\r",
      "Progress: 44.9% ... Training loss: 0.072 ... Validation loss: 0.232\r",
      "Progress: 44.9% ... Training loss: 0.070 ... Validation loss: 0.221\r",
      "Progress: 45.0% ... Training loss: 0.072 ... Validation loss: 0.238\r",
      "Progress: 45.0% ... Training loss: 0.070 ... Validation loss: 0.201\r",
      "Progress: 45.0% ... Training loss: 0.069 ... Validation loss: 0.206\r",
      "Progress: 45.0% ... Training loss: 0.069 ... Validation loss: 0.213\r",
      "Progress: 45.0% ... Training loss: 0.071 ... Validation loss: 0.191\r",
      "Progress: 45.0% ... Training loss: 0.071 ... Validation loss: 0.234\r",
      "Progress: 45.0% ... Training loss: 0.073 ... Validation loss: 0.206\r",
      "Progress: 45.0% ... Training loss: 0.089 ... Validation loss: 0.279\r",
      "Progress: 45.0% ... Training loss: 0.106 ... Validation loss: 0.186\r",
      "Progress: 45.1% ... Training loss: 0.112 ... Validation loss: 0.277\r",
      "Progress: 45.1% ... Training loss: 0.158 ... Validation loss: 0.193\r",
      "Progress: 45.1% ... Training loss: 0.120 ... Validation loss: 0.339\r",
      "Progress: 45.1% ... Training loss: 0.092 ... Validation loss: 0.189\r",
      "Progress: 45.1% ... Training loss: 0.078 ... Validation loss: 0.279\r",
      "Progress: 45.1% ... Training loss: 0.077 ... Validation loss: 0.201\r",
      "Progress: 45.1% ... Training loss: 0.070 ... Validation loss: 0.230\r",
      "Progress: 45.1% ... Training loss: 0.071 ... Validation loss: 0.234\r",
      "Progress: 45.2% ... Training loss: 0.072 ... Validation loss: 0.235\r",
      "Progress: 45.2% ... Training loss: 0.069 ... Validation loss: 0.213\r",
      "Progress: 45.2% ... Training loss: 0.084 ... Validation loss: 0.236\r",
      "Progress: 45.2% ... Training loss: 0.078 ... Validation loss: 0.200\r",
      "Progress: 45.2% ... Training loss: 0.104 ... Validation loss: 0.300\r",
      "Progress: 45.2% ... Training loss: 0.131 ... Validation loss: 0.190\r",
      "Progress: 45.2% ... Training loss: 0.096 ... Validation loss: 0.258\r",
      "Progress: 45.2% ... Training loss: 0.084 ... Validation loss: 0.180\r",
      "Progress: 45.3% ... Training loss: 0.085 ... Validation loss: 0.221\r",
      "Progress: 45.3% ... Training loss: 0.081 ... Validation loss: 0.180\r",
      "Progress: 45.3% ... Training loss: 0.075 ... Validation loss: 0.233\r",
      "Progress: 45.3% ... Training loss: 0.071 ... Validation loss: 0.204\r",
      "Progress: 45.3% ... Training loss: 0.070 ... Validation loss: 0.211\r",
      "Progress: 45.3% ... Training loss: 0.071 ... Validation loss: 0.213\r",
      "Progress: 45.3% ... Training loss: 0.072 ... Validation loss: 0.209\r",
      "Progress: 45.4% ... Training loss: 0.073 ... Validation loss: 0.229\r",
      "Progress: 45.4% ... Training loss: 0.072 ... Validation loss: 0.249\r",
      "Progress: 45.4% ... Training loss: 0.077 ... Validation loss: 0.220\r",
      "Progress: 45.4% ... Training loss: 0.083 ... Validation loss: 0.264\r",
      "Progress: 45.4% ... Training loss: 0.104 ... Validation loss: 0.191\r",
      "Progress: 45.4% ... Training loss: 0.112 ... Validation loss: 0.314\r",
      "Progress: 45.4% ... Training loss: 0.138 ... Validation loss: 0.192\r",
      "Progress: 45.4% ... Training loss: 0.124 ... Validation loss: 0.347\r",
      "Progress: 45.5% ... Training loss: 0.102 ... Validation loss: 0.195\r",
      "Progress: 45.5% ... Training loss: 0.095 ... Validation loss: 0.317\r",
      "Progress: 45.5% ... Training loss: 0.109 ... Validation loss: 0.192\r",
      "Progress: 45.5% ... Training loss: 0.094 ... Validation loss: 0.321\r",
      "Progress: 45.5% ... Training loss: 0.086 ... Validation loss: 0.192\r",
      "Progress: 45.5% ... Training loss: 0.080 ... Validation loss: 0.257\r",
      "Progress: 45.5% ... Training loss: 0.069 ... Validation loss: 0.203\r",
      "Progress: 45.5% ... Training loss: 0.070 ... Validation loss: 0.208\r",
      "Progress: 45.5% ... Training loss: 0.069 ... Validation loss: 0.220\r",
      "Progress: 45.6% ... Training loss: 0.071 ... Validation loss: 0.214\r",
      "Progress: 45.6% ... Training loss: 0.078 ... Validation loss: 0.197\r",
      "Progress: 45.6% ... Training loss: 0.073 ... Validation loss: 0.223\r",
      "Progress: 45.6% ... Training loss: 0.081 ... Validation loss: 0.191\r",
      "Progress: 45.6% ... Training loss: 0.074 ... Validation loss: 0.225\r",
      "Progress: 45.6% ... Training loss: 0.075 ... Validation loss: 0.197\r",
      "Progress: 45.6% ... Training loss: 0.072 ... Validation loss: 0.220\r",
      "Progress: 45.6% ... Training loss: 0.069 ... Validation loss: 0.200\r",
      "Progress: 45.7% ... Training loss: 0.069 ... Validation loss: 0.219\r",
      "Progress: 45.7% ... Training loss: 0.068 ... Validation loss: 0.218\r",
      "Progress: 45.7% ... Training loss: 0.076 ... Validation loss: 0.238\r",
      "Progress: 45.7% ... Training loss: 0.082 ... Validation loss: 0.181\r",
      "Progress: 45.7% ... Training loss: 0.075 ... Validation loss: 0.249\r",
      "Progress: 45.7% ... Training loss: 0.072 ... Validation loss: 0.207\r",
      "Progress: 45.7% ... Training loss: 0.069 ... Validation loss: 0.226\r",
      "Progress: 45.8% ... Training loss: 0.071 ... Validation loss: 0.220\r",
      "Progress: 45.8% ... Training loss: 0.070 ... Validation loss: 0.225\r",
      "Progress: 45.8% ... Training loss: 0.068 ... Validation loss: 0.217\r",
      "Progress: 45.8% ... Training loss: 0.069 ... Validation loss: 0.215\r",
      "Progress: 45.8% ... Training loss: 0.068 ... Validation loss: 0.219\r",
      "Progress: 45.8% ... Training loss: 0.068 ... Validation loss: 0.223\r",
      "Progress: 45.8% ... Training loss: 0.068 ... Validation loss: 0.215\r",
      "Progress: 45.8% ... Training loss: 0.073 ... Validation loss: 0.247\r",
      "Progress: 45.9% ... Training loss: 0.072 ... Validation loss: 0.206\r",
      "Progress: 45.9% ... Training loss: 0.081 ... Validation loss: 0.243\r",
      "Progress: 45.9% ... Training loss: 0.091 ... Validation loss: 0.187\r",
      "Progress: 45.9% ... Training loss: 0.118 ... Validation loss: 0.266\r",
      "Progress: 45.9% ... Training loss: 0.116 ... Validation loss: 0.186\r",
      "Progress: 45.9% ... Training loss: 0.130 ... Validation loss: 0.327\r",
      "Progress: 45.9% ... Training loss: 0.099 ... Validation loss: 0.183\r",
      "Progress: 45.9% ... Training loss: 0.077 ... Validation loss: 0.238\r",
      "Progress: 46.0% ... Training loss: 0.074 ... Validation loss: 0.199\r",
      "Progress: 46.0% ... Training loss: 0.079 ... Validation loss: 0.263\r",
      "Progress: 46.0% ... Training loss: 0.078 ... Validation loss: 0.210\r",
      "Progress: 46.0% ... Training loss: 0.078 ... Validation loss: 0.257\r",
      "Progress: 46.0% ... Training loss: 0.073 ... Validation loss: 0.196\r",
      "Progress: 46.0% ... Training loss: 0.069 ... Validation loss: 0.226\r",
      "Progress: 46.0% ... Training loss: 0.092 ... Validation loss: 0.193\r",
      "Progress: 46.0% ... Training loss: 0.088 ... Validation loss: 0.290\r",
      "Progress: 46.0% ... Training loss: 0.080 ... Validation loss: 0.192\r",
      "Progress: 46.1% ... Training loss: 0.069 ... Validation loss: 0.223\r",
      "Progress: 46.1% ... Training loss: 0.077 ... Validation loss: 0.256\r",
      "Progress: 46.1% ... Training loss: 0.069 ... Validation loss: 0.214\r",
      "Progress: 46.1% ... Training loss: 0.069 ... Validation loss: 0.223\r",
      "Progress: 46.1% ... Training loss: 0.069 ... Validation loss: 0.190\r",
      "Progress: 46.1% ... Training loss: 0.068 ... Validation loss: 0.211\r",
      "Progress: 46.1% ... Training loss: 0.074 ... Validation loss: 0.190\r",
      "Progress: 46.1% ... Training loss: 0.072 ... Validation loss: 0.214\r",
      "Progress: 46.2% ... Training loss: 0.082 ... Validation loss: 0.178\r",
      "Progress: 46.2% ... Training loss: 0.077 ... Validation loss: 0.229\r",
      "Progress: 46.2% ... Training loss: 0.102 ... Validation loss: 0.185\r",
      "Progress: 46.2% ... Training loss: 0.099 ... Validation loss: 0.300\r",
      "Progress: 46.2% ... Training loss: 0.083 ... Validation loss: 0.188\r",
      "Progress: 46.2% ... Training loss: 0.070 ... Validation loss: 0.236\r",
      "Progress: 46.2% ... Training loss: 0.068 ... Validation loss: 0.211\r",
      "Progress: 46.2% ... Training loss: 0.072 ... Validation loss: 0.192\r",
      "Progress: 46.3% ... Training loss: 0.073 ... Validation loss: 0.192\r",
      "Progress: 46.3% ... Training loss: 0.070 ... Validation loss: 0.208\r",
      "Progress: 46.3% ... Training loss: 0.068 ... Validation loss: 0.190\r",
      "Progress: 46.3% ... Training loss: 0.070 ... Validation loss: 0.202\r",
      "Progress: 46.3% ... Training loss: 0.080 ... Validation loss: 0.180\r",
      "Progress: 46.3% ... Training loss: 0.069 ... Validation loss: 0.209\r",
      "Progress: 46.3% ... Training loss: 0.073 ... Validation loss: 0.181\r",
      "Progress: 46.4% ... Training loss: 0.068 ... Validation loss: 0.209\r",
      "Progress: 46.4% ... Training loss: 0.069 ... Validation loss: 0.195\r",
      "Progress: 46.4% ... Training loss: 0.068 ... Validation loss: 0.197\r",
      "Progress: 46.4% ... Training loss: 0.069 ... Validation loss: 0.199\r",
      "Progress: 46.4% ... Training loss: 0.076 ... Validation loss: 0.233\r",
      "Progress: 46.4% ... Training loss: 0.070 ... Validation loss: 0.207\r",
      "Progress: 46.4% ... Training loss: 0.069 ... Validation loss: 0.224\r",
      "Progress: 46.4% ... Training loss: 0.073 ... Validation loss: 0.242\r",
      "Progress: 46.5% ... Training loss: 0.084 ... Validation loss: 0.185\r",
      "Progress: 46.5% ... Training loss: 0.088 ... Validation loss: 0.257\r",
      "Progress: 46.5% ... Training loss: 0.093 ... Validation loss: 0.185\r",
      "Progress: 46.5% ... Training loss: 0.073 ... Validation loss: 0.248\r",
      "Progress: 46.5% ... Training loss: 0.068 ... Validation loss: 0.199\r",
      "Progress: 46.5% ... Training loss: 0.076 ... Validation loss: 0.191\r",
      "Progress: 46.5% ... Training loss: 0.072 ... Validation loss: 0.222\r",
      "Progress: 46.5% ... Training loss: 0.074 ... Validation loss: 0.197\r",
      "Progress: 46.5% ... Training loss: 0.092 ... Validation loss: 0.283\r",
      "Progress: 46.6% ... Training loss: 0.090 ... Validation loss: 0.181\r",
      "Progress: 46.6% ... Training loss: 0.077 ... Validation loss: 0.234\r",
      "Progress: 46.6% ... Training loss: 0.091 ... Validation loss: 0.187\r",
      "Progress: 46.6% ... Training loss: 0.080 ... Validation loss: 0.254\r",
      "Progress: 46.6% ... Training loss: 0.081 ... Validation loss: 0.186\r",
      "Progress: 46.6% ... Training loss: 0.100 ... Validation loss: 0.319\r",
      "Progress: 46.6% ... Training loss: 0.096 ... Validation loss: 0.182\r",
      "Progress: 46.6% ... Training loss: 0.147 ... Validation loss: 0.339\r",
      "Progress: 46.7% ... Training loss: 0.166 ... Validation loss: 0.183\r",
      "Progress: 46.7% ... Training loss: 0.164 ... Validation loss: 0.395\r",
      "Progress: 46.7% ... Training loss: 0.194 ... Validation loss: 0.193\r",
      "Progress: 46.7% ... Training loss: 0.164 ... Validation loss: 0.395\r",
      "Progress: 46.7% ... Training loss: 0.143 ... Validation loss: 0.189\r",
      "Progress: 46.7% ... Training loss: 0.116 ... Validation loss: 0.318\r",
      "Progress: 46.7% ... Training loss: 0.099 ... Validation loss: 0.187\r",
      "Progress: 46.8% ... Training loss: 0.083 ... Validation loss: 0.265\r",
      "Progress: 46.8% ... Training loss: 0.076 ... Validation loss: 0.198\r",
      "Progress: 46.8% ... Training loss: 0.081 ... Validation loss: 0.265\r",
      "Progress: 46.8% ... Training loss: 0.083 ... Validation loss: 0.194\r",
      "Progress: 46.8% ... Training loss: 0.087 ... Validation loss: 0.312\r",
      "Progress: 46.8% ... Training loss: 0.106 ... Validation loss: 0.193\r",
      "Progress: 46.8% ... Training loss: 0.096 ... Validation loss: 0.332\r",
      "Progress: 46.8% ... Training loss: 0.129 ... Validation loss: 0.189\r",
      "Progress: 46.9% ... Training loss: 0.128 ... Validation loss: 0.370\r",
      "Progress: 46.9% ... Training loss: 0.120 ... Validation loss: 0.178\r",
      "Progress: 46.9% ... Training loss: 0.151 ... Validation loss: 0.332\r",
      "Progress: 46.9% ... Training loss: 0.089 ... Validation loss: 0.173\r",
      "Progress: 46.9% ... Training loss: 0.091 ... Validation loss: 0.271\r",
      "Progress: 46.9% ... Training loss: 0.072 ... Validation loss: 0.186\r",
      "Progress: 46.9% ... Training loss: 0.077 ... Validation loss: 0.244\r",
      "Progress: 46.9% ... Training loss: 0.083 ... Validation loss: 0.189\r",
      "Progress: 47.0% ... Training loss: 0.135 ... Validation loss: 0.290\r",
      "Progress: 47.0% ... Training loss: 0.079 ... Validation loss: 0.185\r",
      "Progress: 47.0% ... Training loss: 0.068 ... Validation loss: 0.211\r",
      "Progress: 47.0% ... Training loss: 0.069 ... Validation loss: 0.212\r",
      "Progress: 47.0% ... Training loss: 0.088 ... Validation loss: 0.179\r",
      "Progress: 47.0% ... Training loss: 0.101 ... Validation loss: 0.249\r",
      "Progress: 47.0% ... Training loss: 0.073 ... Validation loss: 0.178\r",
      "Progress: 47.0% ... Training loss: 0.079 ... Validation loss: 0.208\r",
      "Progress: 47.0% ... Training loss: 0.070 ... Validation loss: 0.179\r",
      "Progress: 47.1% ... Training loss: 0.068 ... Validation loss: 0.198\r",
      "Progress: 47.1% ... Training loss: 0.069 ... Validation loss: 0.194\r",
      "Progress: 47.1% ... Training loss: 0.070 ... Validation loss: 0.220\r",
      "Progress: 47.1% ... Training loss: 0.086 ... Validation loss: 0.181\r",
      "Progress: 47.1% ... Training loss: 0.091 ... Validation loss: 0.252\r",
      "Progress: 47.1% ... Training loss: 0.089 ... Validation loss: 0.171\r",
      "Progress: 47.1% ... Training loss: 0.086 ... Validation loss: 0.263\r",
      "Progress: 47.1% ... Training loss: 0.095 ... Validation loss: 0.175\r",
      "Progress: 47.2% ... Training loss: 0.116 ... Validation loss: 0.270\r",
      "Progress: 47.2% ... Training loss: 0.074 ... Validation loss: 0.168\r",
      "Progress: 47.2% ... Training loss: 0.068 ... Validation loss: 0.187\r",
      "Progress: 47.2% ... Training loss: 0.068 ... Validation loss: 0.185\r",
      "Progress: 47.2% ... Training loss: 0.068 ... Validation loss: 0.198\r",
      "Progress: 47.2% ... Training loss: 0.068 ... Validation loss: 0.202\r",
      "Progress: 47.2% ... Training loss: 0.070 ... Validation loss: 0.215\r",
      "Progress: 47.2% ... Training loss: 0.070 ... Validation loss: 0.191\r",
      "Progress: 47.3% ... Training loss: 0.070 ... Validation loss: 0.195\r",
      "Progress: 47.3% ... Training loss: 0.068 ... Validation loss: 0.210\r",
      "Progress: 47.3% ... Training loss: 0.070 ... Validation loss: 0.182\r",
      "Progress: 47.3% ... Training loss: 0.069 ... Validation loss: 0.201\r",
      "Progress: 47.3% ... Training loss: 0.069 ... Validation loss: 0.202\r",
      "Progress: 47.3% ... Training loss: 0.070 ... Validation loss: 0.192\r",
      "Progress: 47.3% ... Training loss: 0.068 ... Validation loss: 0.227\r",
      "Progress: 47.4% ... Training loss: 0.069 ... Validation loss: 0.199\r",
      "Progress: 47.4% ... Training loss: 0.069 ... Validation loss: 0.205\r",
      "Progress: 47.4% ... Training loss: 0.071 ... Validation loss: 0.188\r",
      "Progress: 47.4% ... Training loss: 0.071 ... Validation loss: 0.210\r",
      "Progress: 47.4% ... Training loss: 0.073 ... Validation loss: 0.185\r",
      "Progress: 47.4% ... Training loss: 0.084 ... Validation loss: 0.254\r",
      "Progress: 47.4% ... Training loss: 0.086 ... Validation loss: 0.169\r",
      "Progress: 47.4% ... Training loss: 0.076 ... Validation loss: 0.232\r",
      "Progress: 47.5% ... Training loss: 0.068 ... Validation loss: 0.205\r",
      "Progress: 47.5% ... Training loss: 0.068 ... Validation loss: 0.205\r",
      "Progress: 47.5% ... Training loss: 0.067 ... Validation loss: 0.189\r",
      "Progress: 47.5% ... Training loss: 0.069 ... Validation loss: 0.201\r",
      "Progress: 47.5% ... Training loss: 0.067 ... Validation loss: 0.194\r",
      "Progress: 47.5% ... Training loss: 0.068 ... Validation loss: 0.212\r",
      "Progress: 47.5% ... Training loss: 0.071 ... Validation loss: 0.193\r",
      "Progress: 47.5% ... Training loss: 0.069 ... Validation loss: 0.230\r",
      "Progress: 47.5% ... Training loss: 0.067 ... Validation loss: 0.211\r",
      "Progress: 47.6% ... Training loss: 0.073 ... Validation loss: 0.224\r",
      "Progress: 47.6% ... Training loss: 0.072 ... Validation loss: 0.186\r",
      "Progress: 47.6% ... Training loss: 0.073 ... Validation loss: 0.222\r",
      "Progress: 47.6% ... Training loss: 0.075 ... Validation loss: 0.191\r",
      "Progress: 47.6% ... Training loss: 0.069 ... Validation loss: 0.220\r",
      "Progress: 47.6% ... Training loss: 0.070 ... Validation loss: 0.202\r",
      "Progress: 47.6% ... Training loss: 0.079 ... Validation loss: 0.263\r",
      "Progress: 47.6% ... Training loss: 0.091 ... Validation loss: 0.189\r",
      "Progress: 47.7% ... Training loss: 0.073 ... Validation loss: 0.266\r",
      "Progress: 47.7% ... Training loss: 0.068 ... Validation loss: 0.219\r",
      "Progress: 47.7% ... Training loss: 0.071 ... Validation loss: 0.220\r",
      "Progress: 47.7% ... Training loss: 0.070 ... Validation loss: 0.194\r",
      "Progress: 47.7% ... Training loss: 0.067 ... Validation loss: 0.207\r",
      "Progress: 47.7% ... Training loss: 0.069 ... Validation loss: 0.200\r",
      "Progress: 47.7% ... Training loss: 0.089 ... Validation loss: 0.245\r",
      "Progress: 47.8% ... Training loss: 0.089 ... Validation loss: 0.176\r",
      "Progress: 47.8% ... Training loss: 0.113 ... Validation loss: 0.265\r",
      "Progress: 47.8% ... Training loss: 0.110 ... Validation loss: 0.174\r",
      "Progress: 47.8% ... Training loss: 0.125 ... Validation loss: 0.292\r",
      "Progress: 47.8% ... Training loss: 0.087 ... Validation loss: 0.169\r",
      "Progress: 47.8% ... Training loss: 0.076 ... Validation loss: 0.254\r",
      "Progress: 47.8% ... Training loss: 0.068 ... Validation loss: 0.223\r",
      "Progress: 47.8% ... Training loss: 0.081 ... Validation loss: 0.257\r",
      "Progress: 47.9% ... Training loss: 0.085 ... Validation loss: 0.182\r",
      "Progress: 47.9% ... Training loss: 0.097 ... Validation loss: 0.278\r",
      "Progress: 47.9% ... Training loss: 0.078 ... Validation loss: 0.187\r",
      "Progress: 47.9% ... Training loss: 0.079 ... Validation loss: 0.231\r",
      "Progress: 47.9% ... Training loss: 0.071 ... Validation loss: 0.191\r",
      "Progress: 47.9% ... Training loss: 0.068 ... Validation loss: 0.204\r",
      "Progress: 47.9% ... Training loss: 0.069 ... Validation loss: 0.198\r",
      "Progress: 47.9% ... Training loss: 0.069 ... Validation loss: 0.227\r",
      "Progress: 48.0% ... Training loss: 0.072 ... Validation loss: 0.208\r",
      "Progress: 48.0% ... Training loss: 0.077 ... Validation loss: 0.255\r",
      "Progress: 48.0% ... Training loss: 0.076 ... Validation loss: 0.196\r",
      "Progress: 48.0% ... Training loss: 0.086 ... Validation loss: 0.279\r",
      "Progress: 48.0% ... Training loss: 0.083 ... Validation loss: 0.183\r",
      "Progress: 48.0% ... Training loss: 0.073 ... Validation loss: 0.239\r",
      "Progress: 48.0% ... Training loss: 0.071 ... Validation loss: 0.198\r",
      "Progress: 48.0% ... Training loss: 0.079 ... Validation loss: 0.252\r",
      "Progress: 48.0% ... Training loss: 0.076 ... Validation loss: 0.188\r",
      "Progress: 48.1% ... Training loss: 0.067 ... Validation loss: 0.210\r",
      "Progress: 48.1% ... Training loss: 0.070 ... Validation loss: 0.195\r",
      "Progress: 48.1% ... Training loss: 0.078 ... Validation loss: 0.238\r",
      "Progress: 48.1% ... Training loss: 0.084 ... Validation loss: 0.176\r",
      "Progress: 48.1% ... Training loss: 0.073 ... Validation loss: 0.235\r",
      "Progress: 48.1% ... Training loss: 0.068 ... Validation loss: 0.204\r",
      "Progress: 48.1% ... Training loss: 0.070 ... Validation loss: 0.223\r",
      "Progress: 48.1% ... Training loss: 0.072 ... Validation loss: 0.210\r",
      "Progress: 48.2% ... Training loss: 0.072 ... Validation loss: 0.208\r",
      "Progress: 48.2% ... Training loss: 0.071 ... Validation loss: 0.199\r",
      "Progress: 48.2% ... Training loss: 0.084 ... Validation loss: 0.181\r",
      "Progress: 48.2% ... Training loss: 0.098 ... Validation loss: 0.270\r",
      "Progress: 48.2% ... Training loss: 0.079 ... Validation loss: 0.182\r",
      "Progress: 48.2% ... Training loss: 0.074 ... Validation loss: 0.237\r",
      "Progress: 48.2% ... Training loss: 0.091 ... Validation loss: 0.184\r",
      "Progress: 48.2% ... Training loss: 0.095 ... Validation loss: 0.303\r",
      "Progress: 48.3% ... Training loss: 0.084 ... Validation loss: 0.205\r",
      "Progress: 48.3% ... Training loss: 0.073 ... Validation loss: 0.270\r",
      "Progress: 48.3% ... Training loss: 0.069 ... Validation loss: 0.216\r",
      "Progress: 48.3% ... Training loss: 0.083 ... Validation loss: 0.254\r",
      "Progress: 48.3% ... Training loss: 0.086 ... Validation loss: 0.174\r",
      "Progress: 48.3% ... Training loss: 0.082 ... Validation loss: 0.268\r",
      "Progress: 48.3% ... Training loss: 0.071 ... Validation loss: 0.208\r",
      "Progress: 48.4% ... Training loss: 0.087 ... Validation loss: 0.296\r",
      "Progress: 48.4% ... Training loss: 0.102 ... Validation loss: 0.192\r",
      "Progress: 48.4% ... Training loss: 0.093 ... Validation loss: 0.293\r",
      "Progress: 48.4% ... Training loss: 0.105 ... Validation loss: 0.185\r",
      "Progress: 48.4% ... Training loss: 0.090 ... Validation loss: 0.276\r",
      "Progress: 48.4% ... Training loss: 0.102 ... Validation loss: 0.176\r",
      "Progress: 48.4% ... Training loss: 0.097 ... Validation loss: 0.266\r",
      "Progress: 48.4% ... Training loss: 0.090 ... Validation loss: 0.178\r",
      "Progress: 48.5% ... Training loss: 0.085 ... Validation loss: 0.254\r",
      "Progress: 48.5% ... Training loss: 0.068 ... Validation loss: 0.210\r",
      "Progress: 48.5% ... Training loss: 0.068 ... Validation loss: 0.199\r",
      "Progress: 48.5% ... Training loss: 0.071 ... Validation loss: 0.223\r",
      "Progress: 48.5% ... Training loss: 0.076 ... Validation loss: 0.184\r",
      "Progress: 48.5% ... Training loss: 0.072 ... Validation loss: 0.229\r",
      "Progress: 48.5% ... Training loss: 0.067 ... Validation loss: 0.220\r",
      "Progress: 48.5% ... Training loss: 0.069 ... Validation loss: 0.222\r",
      "Progress: 48.5% ... Training loss: 0.078 ... Validation loss: 0.181\r",
      "Progress: 48.6% ... Training loss: 0.074 ... Validation loss: 0.230\r",
      "Progress: 48.6% ... Training loss: 0.082 ... Validation loss: 0.180\r",
      "Progress: 48.6% ... Training loss: 0.089 ... Validation loss: 0.278\r",
      "Progress: 48.6% ... Training loss: 0.109 ... Validation loss: 0.177\r",
      "Progress: 48.6% ... Training loss: 0.108 ... Validation loss: 0.310\r",
      "Progress: 48.6% ... Training loss: 0.087 ... Validation loss: 0.177\r",
      "Progress: 48.6% ... Training loss: 0.081 ... Validation loss: 0.243\r",
      "Progress: 48.6% ... Training loss: 0.089 ... Validation loss: 0.175\r",
      "Progress: 48.7% ... Training loss: 0.080 ... Validation loss: 0.258\r",
      "Progress: 48.7% ... Training loss: 0.074 ... Validation loss: 0.191\r",
      "Progress: 48.7% ... Training loss: 0.069 ... Validation loss: 0.232\r",
      "Progress: 48.7% ... Training loss: 0.071 ... Validation loss: 0.192\r",
      "Progress: 48.7% ... Training loss: 0.067 ... Validation loss: 0.222\r",
      "Progress: 48.7% ... Training loss: 0.069 ... Validation loss: 0.198\r",
      "Progress: 48.7% ... Training loss: 0.071 ... Validation loss: 0.254\r",
      "Progress: 48.8% ... Training loss: 0.091 ... Validation loss: 0.185\r",
      "Progress: 48.8% ... Training loss: 0.099 ... Validation loss: 0.247\r",
      "Progress: 48.8% ... Training loss: 0.089 ... Validation loss: 0.174\r",
      "Progress: 48.8% ... Training loss: 0.108 ... Validation loss: 0.280\r",
      "Progress: 48.8% ... Training loss: 0.094 ... Validation loss: 0.177\r",
      "Progress: 48.8% ... Training loss: 0.105 ... Validation loss: 0.259\r",
      "Progress: 48.8% ... Training loss: 0.094 ... Validation loss: 0.166\r",
      "Progress: 48.8% ... Training loss: 0.091 ... Validation loss: 0.233\r",
      "Progress: 48.9% ... Training loss: 0.080 ... Validation loss: 0.180\r",
      "Progress: 48.9% ... Training loss: 0.098 ... Validation loss: 0.242\r",
      "Progress: 48.9% ... Training loss: 0.107 ... Validation loss: 0.173\r",
      "Progress: 48.9% ... Training loss: 0.091 ... Validation loss: 0.253\r",
      "Progress: 48.9% ... Training loss: 0.096 ... Validation loss: 0.178\r",
      "Progress: 48.9% ... Training loss: 0.081 ... Validation loss: 0.242\r",
      "Progress: 48.9% ... Training loss: 0.098 ... Validation loss: 0.180\r",
      "Progress: 48.9% ... Training loss: 0.099 ... Validation loss: 0.292\r",
      "Progress: 49.0% ... Training loss: 0.107 ... Validation loss: 0.171\r",
      "Progress: 49.0% ... Training loss: 0.079 ... Validation loss: 0.254\r",
      "Progress: 49.0% ... Training loss: 0.083 ... Validation loss: 0.179\r",
      "Progress: 49.0% ... Training loss: 0.073 ... Validation loss: 0.227\r",
      "Progress: 49.0% ... Training loss: 0.075 ... Validation loss: 0.180\r",
      "Progress: 49.0% ... Training loss: 0.085 ... Validation loss: 0.240\r",
      "Progress: 49.0% ... Training loss: 0.101 ... Validation loss: 0.179\r",
      "Progress: 49.0% ... Training loss: 0.079 ... Validation loss: 0.226\r",
      "Progress: 49.0% ... Training loss: 0.068 ... Validation loss: 0.187\r",
      "Progress: 49.1% ... Training loss: 0.078 ... Validation loss: 0.228\r",
      "Progress: 49.1% ... Training loss: 0.095 ... Validation loss: 0.176\r",
      "Progress: 49.1% ... Training loss: 0.077 ... Validation loss: 0.254\r",
      "Progress: 49.1% ... Training loss: 0.068 ... Validation loss: 0.195\r",
      "Progress: 49.1% ... Training loss: 0.069 ... Validation loss: 0.196\r",
      "Progress: 49.1% ... Training loss: 0.085 ... Validation loss: 0.237\r",
      "Progress: 49.1% ... Training loss: 0.105 ... Validation loss: 0.188\r",
      "Progress: 49.1% ... Training loss: 0.072 ... Validation loss: 0.257\r",
      "Progress: 49.2% ... Training loss: 0.069 ... Validation loss: 0.227\r",
      "Progress: 49.2% ... Training loss: 0.067 ... Validation loss: 0.231\r",
      "Progress: 49.2% ... Training loss: 0.067 ... Validation loss: 0.224\r",
      "Progress: 49.2% ... Training loss: 0.070 ... Validation loss: 0.240\r",
      "Progress: 49.2% ... Training loss: 0.068 ... Validation loss: 0.208\r",
      "Progress: 49.2% ... Training loss: 0.072 ... Validation loss: 0.249\r",
      "Progress: 49.2% ... Training loss: 0.090 ... Validation loss: 0.182\r",
      "Progress: 49.2% ... Training loss: 0.131 ... Validation loss: 0.316\r",
      "Progress: 49.3% ... Training loss: 0.128 ... Validation loss: 0.176\r",
      "Progress: 49.3% ... Training loss: 0.099 ... Validation loss: 0.240\r",
      "Progress: 49.3% ... Training loss: 0.105 ... Validation loss: 0.181\r",
      "Progress: 49.3% ... Training loss: 0.082 ... Validation loss: 0.240\r",
      "Progress: 49.3% ... Training loss: 0.084 ... Validation loss: 0.179\r",
      "Progress: 49.3% ... Training loss: 0.087 ... Validation loss: 0.249\r",
      "Progress: 49.3% ... Training loss: 0.095 ... Validation loss: 0.175\r",
      "Progress: 49.4% ... Training loss: 0.079 ... Validation loss: 0.243\r",
      "Progress: 49.4% ... Training loss: 0.101 ... Validation loss: 0.191\r",
      "Progress: 49.4% ... Training loss: 0.091 ... Validation loss: 0.300\r",
      "Progress: 49.4% ... Training loss: 0.081 ... Validation loss: 0.206\r",
      "Progress: 49.4% ... Training loss: 0.075 ... Validation loss: 0.262\r",
      "Progress: 49.4% ... Training loss: 0.067 ... Validation loss: 0.213\r",
      "Progress: 49.4% ... Training loss: 0.067 ... Validation loss: 0.218\r",
      "Progress: 49.4% ... Training loss: 0.071 ... Validation loss: 0.199\r",
      "Progress: 49.5% ... Training loss: 0.069 ... Validation loss: 0.224\r",
      "Progress: 49.5% ... Training loss: 0.067 ... Validation loss: 0.212\r",
      "Progress: 49.5% ... Training loss: 0.066 ... Validation loss: 0.212\r",
      "Progress: 49.5% ... Training loss: 0.072 ... Validation loss: 0.192\r",
      "Progress: 49.5% ... Training loss: 0.077 ... Validation loss: 0.246\r",
      "Progress: 49.5% ... Training loss: 0.070 ... Validation loss: 0.197\r",
      "Progress: 49.5% ... Training loss: 0.079 ... Validation loss: 0.255\r",
      "Progress: 49.5% ... Training loss: 0.080 ... Validation loss: 0.177\r",
      "Progress: 49.5% ... Training loss: 0.068 ... Validation loss: 0.221\r",
      "Progress: 49.6% ... Training loss: 0.068 ... Validation loss: 0.194\r",
      "Progress: 49.6% ... Training loss: 0.066 ... Validation loss: 0.209\r",
      "Progress: 49.6% ... Training loss: 0.067 ... Validation loss: 0.189\r",
      "Progress: 49.6% ... Training loss: 0.067 ... Validation loss: 0.193\r",
      "Progress: 49.6% ... Training loss: 0.068 ... Validation loss: 0.195\r",
      "Progress: 49.6% ... Training loss: 0.067 ... Validation loss: 0.206\r",
      "Progress: 49.6% ... Training loss: 0.068 ... Validation loss: 0.192\r",
      "Progress: 49.6% ... Training loss: 0.067 ... Validation loss: 0.204\r",
      "Progress: 49.7% ... Training loss: 0.068 ... Validation loss: 0.201\r",
      "Progress: 49.7% ... Training loss: 0.070 ... Validation loss: 0.234\r",
      "Progress: 49.7% ... Training loss: 0.066 ... Validation loss: 0.200\r",
      "Progress: 49.7% ... Training loss: 0.068 ... Validation loss: 0.185\r",
      "Progress: 49.7% ... Training loss: 0.068 ... Validation loss: 0.215\r",
      "Progress: 49.7% ... Training loss: 0.076 ... Validation loss: 0.185\r",
      "Progress: 49.7% ... Training loss: 0.071 ... Validation loss: 0.219\r",
      "Progress: 49.8% ... Training loss: 0.085 ... Validation loss: 0.179\r",
      "Progress: 49.8% ... Training loss: 0.071 ... Validation loss: 0.239\r",
      "Progress: 49.8% ... Training loss: 0.081 ... Validation loss: 0.186\r",
      "Progress: 49.8% ... Training loss: 0.067 ... Validation loss: 0.221\r",
      "Progress: 49.8% ... Training loss: 0.068 ... Validation loss: 0.218\r",
      "Progress: 49.8% ... Training loss: 0.067 ... Validation loss: 0.223\r",
      "Progress: 49.8% ... Training loss: 0.068 ... Validation loss: 0.213\r",
      "Progress: 49.8% ... Training loss: 0.066 ... Validation loss: 0.211\r",
      "Progress: 49.9% ... Training loss: 0.067 ... Validation loss: 0.202\r",
      "Progress: 49.9% ... Training loss: 0.067 ... Validation loss: 0.222\r",
      "Progress: 49.9% ... Training loss: 0.070 ... Validation loss: 0.197\r",
      "Progress: 49.9% ... Training loss: 0.073 ... Validation loss: 0.244\r",
      "Progress: 49.9% ... Training loss: 0.067 ... Validation loss: 0.196\r",
      "Progress: 49.9% ... Training loss: 0.068 ... Validation loss: 0.226\r",
      "Progress: 49.9% ... Training loss: 0.066 ... Validation loss: 0.201\r",
      "Progress: 49.9% ... Training loss: 0.070 ... Validation loss: 0.236\r",
      "Progress: 50.0% ... Training loss: 0.074 ... Validation loss: 0.189\r",
      "Progress: 50.0% ... Training loss: 0.073 ... Validation loss: 0.234\r",
      "Progress: 50.0% ... Training loss: 0.079 ... Validation loss: 0.177\r",
      "Progress: 50.0% ... Training loss: 0.068 ... Validation loss: 0.207\r",
      "Progress: 50.0% ... Training loss: 0.073 ... Validation loss: 0.185\r",
      "Progress: 50.0% ... Training loss: 0.088 ... Validation loss: 0.261\r",
      "Progress: 50.0% ... Training loss: 0.079 ... Validation loss: 0.179\r",
      "Progress: 50.0% ... Training loss: 0.076 ... Validation loss: 0.195\r",
      "Progress: 50.0% ... Training loss: 0.092 ... Validation loss: 0.169\r",
      "Progress: 50.1% ... Training loss: 0.092 ... Validation loss: 0.238\r",
      "Progress: 50.1% ... Training loss: 0.069 ... Validation loss: 0.184\r",
      "Progress: 50.1% ... Training loss: 0.069 ... Validation loss: 0.193\r",
      "Progress: 50.1% ... Training loss: 0.070 ... Validation loss: 0.211\r",
      "Progress: 50.1% ... Training loss: 0.067 ... Validation loss: 0.185\r",
      "Progress: 50.1% ... Training loss: 0.069 ... Validation loss: 0.225\r",
      "Progress: 50.1% ... Training loss: 0.065 ... Validation loss: 0.194\r",
      "Progress: 50.1% ... Training loss: 0.068 ... Validation loss: 0.210\r",
      "Progress: 50.2% ... Training loss: 0.067 ... Validation loss: 0.201\r",
      "Progress: 50.2% ... Training loss: 0.066 ... Validation loss: 0.225\r",
      "Progress: 50.2% ... Training loss: 0.069 ... Validation loss: 0.230\r",
      "Progress: 50.2% ... Training loss: 0.066 ... Validation loss: 0.205\r",
      "Progress: 50.2% ... Training loss: 0.067 ... Validation loss: 0.198\r",
      "Progress: 50.2% ... Training loss: 0.065 ... Validation loss: 0.211\r",
      "Progress: 50.2% ... Training loss: 0.066 ... Validation loss: 0.209\r",
      "Progress: 50.2% ... Training loss: 0.070 ... Validation loss: 0.193\r",
      "Progress: 50.3% ... Training loss: 0.067 ... Validation loss: 0.203\r",
      "Progress: 50.3% ... Training loss: 0.068 ... Validation loss: 0.212\r",
      "Progress: 50.3% ... Training loss: 0.067 ... Validation loss: 0.207\r",
      "Progress: 50.3% ... Training loss: 0.066 ... Validation loss: 0.222\r",
      "Progress: 50.3% ... Training loss: 0.066 ... Validation loss: 0.211\r",
      "Progress: 50.3% ... Training loss: 0.068 ... Validation loss: 0.234\r",
      "Progress: 50.3% ... Training loss: 0.076 ... Validation loss: 0.195\r",
      "Progress: 50.4% ... Training loss: 0.071 ... Validation loss: 0.234\r",
      "Progress: 50.4% ... Training loss: 0.076 ... Validation loss: 0.195\r",
      "Progress: 50.4% ... Training loss: 0.066 ... Validation loss: 0.208\r",
      "Progress: 50.4% ... Training loss: 0.068 ... Validation loss: 0.207\r",
      "Progress: 50.4% ... Training loss: 0.066 ... Validation loss: 0.218\r",
      "Progress: 50.4% ... Training loss: 0.069 ... Validation loss: 0.232\r",
      "Progress: 50.4% ... Training loss: 0.068 ... Validation loss: 0.200\r",
      "Progress: 50.4% ... Training loss: 0.071 ... Validation loss: 0.238\r",
      "Progress: 50.5% ... Training loss: 0.070 ... Validation loss: 0.187\r",
      "Progress: 50.5% ... Training loss: 0.072 ... Validation loss: 0.230\r",
      "Progress: 50.5% ... Training loss: 0.078 ... Validation loss: 0.182\r",
      "Progress: 50.5% ... Training loss: 0.075 ... Validation loss: 0.224\r",
      "Progress: 50.5% ... Training loss: 0.072 ... Validation loss: 0.185\r",
      "Progress: 50.5% ... Training loss: 0.069 ... Validation loss: 0.220\r",
      "Progress: 50.5% ... Training loss: 0.072 ... Validation loss: 0.195\r",
      "Progress: 50.5% ... Training loss: 0.080 ... Validation loss: 0.265\r",
      "Progress: 50.5% ... Training loss: 0.085 ... Validation loss: 0.176\r",
      "Progress: 50.6% ... Training loss: 0.098 ... Validation loss: 0.277\r",
      "Progress: 50.6% ... Training loss: 0.111 ... Validation loss: 0.169\r",
      "Progress: 50.6% ... Training loss: 0.148 ... Validation loss: 0.287\r",
      "Progress: 50.6% ... Training loss: 0.110 ... Validation loss: 0.167\r",
      "Progress: 50.6% ... Training loss: 0.131 ... Validation loss: 0.339\r",
      "Progress: 50.6% ... Training loss: 0.105 ... Validation loss: 0.178\r",
      "Progress: 50.6% ... Training loss: 0.079 ... Validation loss: 0.238\r",
      "Progress: 50.6% ... Training loss: 0.075 ... Validation loss: 0.176\r",
      "Progress: 50.7% ... Training loss: 0.067 ... Validation loss: 0.199\r",
      "Progress: 50.7% ... Training loss: 0.067 ... Validation loss: 0.206\r",
      "Progress: 50.7% ... Training loss: 0.066 ... Validation loss: 0.201\r",
      "Progress: 50.7% ... Training loss: 0.065 ... Validation loss: 0.204\r",
      "Progress: 50.7% ... Training loss: 0.065 ... Validation loss: 0.193\r",
      "Progress: 50.7% ... Training loss: 0.066 ... Validation loss: 0.201\r",
      "Progress: 50.7% ... Training loss: 0.067 ... Validation loss: 0.185\r",
      "Progress: 50.8% ... Training loss: 0.066 ... Validation loss: 0.190\r",
      "Progress: 50.8% ... Training loss: 0.068 ... Validation loss: 0.204\r",
      "Progress: 50.8% ... Training loss: 0.067 ... Validation loss: 0.182\r",
      "Progress: 50.8% ... Training loss: 0.066 ... Validation loss: 0.218\r",
      "Progress: 50.8% ... Training loss: 0.067 ... Validation loss: 0.205\r",
      "Progress: 50.8% ... Training loss: 0.066 ... Validation loss: 0.214\r",
      "Progress: 50.8% ... Training loss: 0.066 ... Validation loss: 0.210\r",
      "Progress: 50.8% ... Training loss: 0.069 ... Validation loss: 0.239\r",
      "Progress: 50.9% ... Training loss: 0.077 ... Validation loss: 0.198\r",
      "Progress: 50.9% ... Training loss: 0.083 ... Validation loss: 0.273\r",
      "Progress: 50.9% ... Training loss: 0.087 ... Validation loss: 0.179\r",
      "Progress: 50.9% ... Training loss: 0.106 ... Validation loss: 0.302\r",
      "Progress: 50.9% ... Training loss: 0.087 ... Validation loss: 0.186\r",
      "Progress: 50.9% ... Training loss: 0.069 ... Validation loss: 0.227\r",
      "Progress: 50.9% ... Training loss: 0.065 ... Validation loss: 0.197\r",
      "Progress: 50.9% ... Training loss: 0.072 ... Validation loss: 0.250\r",
      "Progress: 51.0% ... Training loss: 0.081 ... Validation loss: 0.183\r",
      "Progress: 51.0% ... Training loss: 0.066 ... Validation loss: 0.225\r",
      "Progress: 51.0% ... Training loss: 0.066 ... Validation loss: 0.225\r",
      "Progress: 51.0% ... Training loss: 0.067 ... Validation loss: 0.234\r",
      "Progress: 51.0% ... Training loss: 0.071 ... Validation loss: 0.198\r",
      "Progress: 51.0% ... Training loss: 0.065 ... Validation loss: 0.236\r",
      "Progress: 51.0% ... Training loss: 0.067 ... Validation loss: 0.237\r",
      "Progress: 51.0% ... Training loss: 0.067 ... Validation loss: 0.204\r",
      "Progress: 51.0% ... Training loss: 0.067 ... Validation loss: 0.213\r",
      "Progress: 51.1% ... Training loss: 0.065 ... Validation loss: 0.219\r",
      "Progress: 51.1% ... Training loss: 0.069 ... Validation loss: 0.207\r",
      "Progress: 51.1% ... Training loss: 0.072 ... Validation loss: 0.231\r",
      "Progress: 51.1% ... Training loss: 0.070 ... Validation loss: 0.194\r",
      "Progress: 51.1% ... Training loss: 0.065 ... Validation loss: 0.208\r",
      "Progress: 51.1% ... Training loss: 0.067 ... Validation loss: 0.209\r",
      "Progress: 51.1% ... Training loss: 0.070 ... Validation loss: 0.186\r",
      "Progress: 51.1% ... Training loss: 0.078 ... Validation loss: 0.250\r",
      "Progress: 51.2% ... Training loss: 0.072 ... Validation loss: 0.186\r",
      "Progress: 51.2% ... Training loss: 0.068 ... Validation loss: 0.218\r",
      "Progress: 51.2% ... Training loss: 0.068 ... Validation loss: 0.180\r",
      "Progress: 51.2% ... Training loss: 0.069 ... Validation loss: 0.211\r",
      "Progress: 51.2% ... Training loss: 0.078 ... Validation loss: 0.178\r",
      "Progress: 51.2% ... Training loss: 0.067 ... Validation loss: 0.214\r",
      "Progress: 51.2% ... Training loss: 0.067 ... Validation loss: 0.208\r",
      "Progress: 51.2% ... Training loss: 0.081 ... Validation loss: 0.241\r",
      "Progress: 51.3% ... Training loss: 0.093 ... Validation loss: 0.180\r",
      "Progress: 51.3% ... Training loss: 0.077 ... Validation loss: 0.249\r",
      "Progress: 51.3% ... Training loss: 0.066 ... Validation loss: 0.213\r",
      "Progress: 51.3% ... Training loss: 0.068 ... Validation loss: 0.207\r",
      "Progress: 51.3% ... Training loss: 0.065 ... Validation loss: 0.206\r",
      "Progress: 51.3% ... Training loss: 0.069 ... Validation loss: 0.199\r",
      "Progress: 51.3% ... Training loss: 0.068 ... Validation loss: 0.214\r",
      "Progress: 51.4% ... Training loss: 0.067 ... Validation loss: 0.205\r",
      "Progress: 51.4% ... Training loss: 0.066 ... Validation loss: 0.189\r",
      "Progress: 51.4% ... Training loss: 0.066 ... Validation loss: 0.186\r",
      "Progress: 51.4% ... Training loss: 0.067 ... Validation loss: 0.226\r",
      "Progress: 51.4% ... Training loss: 0.066 ... Validation loss: 0.218\r",
      "Progress: 51.4% ... Training loss: 0.071 ... Validation loss: 0.183\r",
      "Progress: 51.4% ... Training loss: 0.065 ... Validation loss: 0.211\r",
      "Progress: 51.4% ... Training loss: 0.065 ... Validation loss: 0.204\r",
      "Progress: 51.5% ... Training loss: 0.066 ... Validation loss: 0.220\r",
      "Progress: 51.5% ... Training loss: 0.066 ... Validation loss: 0.198\r",
      "Progress: 51.5% ... Training loss: 0.066 ... Validation loss: 0.218\r",
      "Progress: 51.5% ... Training loss: 0.065 ... Validation loss: 0.194\r",
      "Progress: 51.5% ... Training loss: 0.065 ... Validation loss: 0.200\r",
      "Progress: 51.5% ... Training loss: 0.075 ... Validation loss: 0.215\r",
      "Progress: 51.5% ... Training loss: 0.093 ... Validation loss: 0.171\r",
      "Progress: 51.5% ... Training loss: 0.122 ... Validation loss: 0.265\r",
      "Progress: 51.5% ... Training loss: 0.118 ... Validation loss: 0.175\r",
      "Progress: 51.6% ... Training loss: 0.093 ... Validation loss: 0.247\r",
      "Progress: 51.6% ... Training loss: 0.091 ... Validation loss: 0.178\r",
      "Progress: 51.6% ... Training loss: 0.085 ... Validation loss: 0.247\r",
      "Progress: 51.6% ... Training loss: 0.093 ... Validation loss: 0.173\r",
      "Progress: 51.6% ... Training loss: 0.107 ... Validation loss: 0.282\r",
      "Progress: 51.6% ... Training loss: 0.086 ... Validation loss: 0.175\r",
      "Progress: 51.6% ... Training loss: 0.073 ... Validation loss: 0.234\r",
      "Progress: 51.6% ... Training loss: 0.068 ... Validation loss: 0.189\r",
      "Progress: 51.7% ... Training loss: 0.066 ... Validation loss: 0.207\r",
      "Progress: 51.7% ... Training loss: 0.065 ... Validation loss: 0.212\r",
      "Progress: 51.7% ... Training loss: 0.066 ... Validation loss: 0.217\r",
      "Progress: 51.7% ... Training loss: 0.073 ... Validation loss: 0.271\r",
      "Progress: 51.7% ... Training loss: 0.066 ... Validation loss: 0.223\r",
      "Progress: 51.7% ... Training loss: 0.065 ... Validation loss: 0.207\r",
      "Progress: 51.7% ... Training loss: 0.069 ... Validation loss: 0.229\r",
      "Progress: 51.8% ... Training loss: 0.069 ... Validation loss: 0.210\r",
      "Progress: 51.8% ... Training loss: 0.065 ... Validation loss: 0.221\r",
      "Progress: 51.8% ... Training loss: 0.071 ... Validation loss: 0.182\r",
      "Progress: 51.8% ... Training loss: 0.067 ... Validation loss: 0.191\r",
      "Progress: 51.8% ... Training loss: 0.078 ... Validation loss: 0.251\r",
      "Progress: 51.8% ... Training loss: 0.089 ... Validation loss: 0.167\r",
      "Progress: 51.8% ... Training loss: 0.089 ... Validation loss: 0.277\r",
      "Progress: 51.8% ... Training loss: 0.085 ... Validation loss: 0.171\r",
      "Progress: 51.9% ... Training loss: 0.076 ... Validation loss: 0.243\r",
      "Progress: 51.9% ... Training loss: 0.082 ... Validation loss: 0.183\r",
      "Progress: 51.9% ... Training loss: 0.084 ... Validation loss: 0.250\r",
      "Progress: 51.9% ... Training loss: 0.070 ... Validation loss: 0.181\r",
      "Progress: 51.9% ... Training loss: 0.085 ... Validation loss: 0.263\r",
      "Progress: 51.9% ... Training loss: 0.097 ... Validation loss: 0.182\r",
      "Progress: 51.9% ... Training loss: 0.086 ... Validation loss: 0.280\r",
      "Progress: 51.9% ... Training loss: 0.089 ... Validation loss: 0.179\r",
      "Progress: 52.0% ... Training loss: 0.081 ... Validation loss: 0.259\r",
      "Progress: 52.0% ... Training loss: 0.086 ... Validation loss: 0.185\r",
      "Progress: 52.0% ... Training loss: 0.085 ... Validation loss: 0.253\r",
      "Progress: 52.0% ... Training loss: 0.065 ... Validation loss: 0.205\r",
      "Progress: 52.0% ... Training loss: 0.065 ... Validation loss: 0.221\r",
      "Progress: 52.0% ... Training loss: 0.067 ... Validation loss: 0.238\r",
      "Progress: 52.0% ... Training loss: 0.066 ... Validation loss: 0.226\r",
      "Progress: 52.0% ... Training loss: 0.068 ... Validation loss: 0.227\r",
      "Progress: 52.0% ... Training loss: 0.084 ... Validation loss: 0.273\r",
      "Progress: 52.1% ... Training loss: 0.069 ... Validation loss: 0.187\r",
      "Progress: 52.1% ... Training loss: 0.066 ... Validation loss: 0.227\r",
      "Progress: 52.1% ... Training loss: 0.064 ... Validation loss: 0.206\r",
      "Progress: 52.1% ... Training loss: 0.064 ... Validation loss: 0.226\r",
      "Progress: 52.1% ... Training loss: 0.064 ... Validation loss: 0.221\r",
      "Progress: 52.1% ... Training loss: 0.065 ... Validation loss: 0.205\r",
      "Progress: 52.1% ... Training loss: 0.069 ... Validation loss: 0.201\r",
      "Progress: 52.1% ... Training loss: 0.068 ... Validation loss: 0.214\r",
      "Progress: 52.2% ... Training loss: 0.072 ... Validation loss: 0.193\r",
      "Progress: 52.2% ... Training loss: 0.066 ... Validation loss: 0.233\r",
      "Progress: 52.2% ... Training loss: 0.069 ... Validation loss: 0.213\r",
      "Progress: 52.2% ... Training loss: 0.065 ... Validation loss: 0.233\r",
      "Progress: 52.2% ... Training loss: 0.066 ... Validation loss: 0.227\r",
      "Progress: 52.2% ... Training loss: 0.065 ... Validation loss: 0.235\r",
      "Progress: 52.2% ... Training loss: 0.065 ... Validation loss: 0.228\r",
      "Progress: 52.2% ... Training loss: 0.066 ... Validation loss: 0.220\r",
      "Progress: 52.3% ... Training loss: 0.069 ... Validation loss: 0.209\r",
      "Progress: 52.3% ... Training loss: 0.078 ... Validation loss: 0.262\r",
      "Progress: 52.3% ... Training loss: 0.081 ... Validation loss: 0.188\r",
      "Progress: 52.3% ... Training loss: 0.077 ... Validation loss: 0.277\r",
      "Progress: 52.3% ... Training loss: 0.083 ... Validation loss: 0.198\r",
      "Progress: 52.3% ... Training loss: 0.095 ... Validation loss: 0.297\r",
      "Progress: 52.3% ... Training loss: 0.092 ... Validation loss: 0.177\r",
      "Progress: 52.4% ... Training loss: 0.075 ... Validation loss: 0.236\r",
      "Progress: 52.4% ... Training loss: 0.081 ... Validation loss: 0.193\r",
      "Progress: 52.4% ... Training loss: 0.086 ... Validation loss: 0.281\r",
      "Progress: 52.4% ... Training loss: 0.069 ... Validation loss: 0.192\r",
      "Progress: 52.4% ... Training loss: 0.064 ... Validation loss: 0.210\r",
      "Progress: 52.4% ... Training loss: 0.067 ... Validation loss: 0.199\r",
      "Progress: 52.4% ... Training loss: 0.064 ... Validation loss: 0.193\r",
      "Progress: 52.4% ... Training loss: 0.065 ... Validation loss: 0.197\r",
      "Progress: 52.5% ... Training loss: 0.065 ... Validation loss: 0.211\r",
      "Progress: 52.5% ... Training loss: 0.064 ... Validation loss: 0.202\r",
      "Progress: 52.5% ... Training loss: 0.066 ... Validation loss: 0.192\r",
      "Progress: 52.5% ... Training loss: 0.068 ... Validation loss: 0.225\r",
      "Progress: 52.5% ... Training loss: 0.065 ... Validation loss: 0.189\r",
      "Progress: 52.5% ... Training loss: 0.065 ... Validation loss: 0.207\r",
      "Progress: 52.5% ... Training loss: 0.064 ... Validation loss: 0.209\r",
      "Progress: 52.5% ... Training loss: 0.094 ... Validation loss: 0.181\r",
      "Progress: 52.5% ... Training loss: 0.084 ... Validation loss: 0.243\r",
      "Progress: 52.6% ... Training loss: 0.072 ... Validation loss: 0.182\r",
      "Progress: 52.6% ... Training loss: 0.073 ... Validation loss: 0.230\r",
      "Progress: 52.6% ... Training loss: 0.089 ... Validation loss: 0.179\r",
      "Progress: 52.6% ... Training loss: 0.096 ... Validation loss: 0.271\r",
      "Progress: 52.6% ... Training loss: 0.093 ... Validation loss: 0.176\r",
      "Progress: 52.6% ... Training loss: 0.123 ... Validation loss: 0.286\r",
      "Progress: 52.6% ... Training loss: 0.131 ... Validation loss: 0.182\r",
      "Progress: 52.6% ... Training loss: 0.086 ... Validation loss: 0.250\r",
      "Progress: 52.7% ... Training loss: 0.077 ... Validation loss: 0.191\r",
      "Progress: 52.7% ... Training loss: 0.072 ... Validation loss: 0.252\r",
      "Progress: 52.7% ... Training loss: 0.073 ... Validation loss: 0.183\r",
      "Progress: 52.7% ... Training loss: 0.085 ... Validation loss: 0.247\r",
      "Progress: 52.7% ... Training loss: 0.083 ... Validation loss: 0.188\r",
      "Progress: 52.7% ... Training loss: 0.088 ... Validation loss: 0.292\r",
      "Progress: 52.7% ... Training loss: 0.073 ... Validation loss: 0.188\r",
      "Progress: 52.8% ... Training loss: 0.065 ... Validation loss: 0.214\r",
      "Progress: 52.8% ... Training loss: 0.072 ... Validation loss: 0.191\r",
      "Progress: 52.8% ... Training loss: 0.084 ... Validation loss: 0.256\r",
      "Progress: 52.8% ... Training loss: 0.076 ... Validation loss: 0.189\r",
      "Progress: 52.8% ... Training loss: 0.081 ... Validation loss: 0.240\r",
      "Progress: 52.8% ... Training loss: 0.073 ... Validation loss: 0.186\r",
      "Progress: 52.8% ... Training loss: 0.079 ... Validation loss: 0.223\r",
      "Progress: 52.8% ... Training loss: 0.071 ... Validation loss: 0.187\r",
      "Progress: 52.9% ... Training loss: 0.066 ... Validation loss: 0.205\r",
      "Progress: 52.9% ... Training loss: 0.065 ... Validation loss: 0.197\r",
      "Progress: 52.9% ... Training loss: 0.065 ... Validation loss: 0.227\r",
      "Progress: 52.9% ... Training loss: 0.070 ... Validation loss: 0.205\r",
      "Progress: 52.9% ... Training loss: 0.065 ... Validation loss: 0.215\r",
      "Progress: 52.9% ... Training loss: 0.066 ... Validation loss: 0.193\r",
      "Progress: 52.9% ... Training loss: 0.071 ... Validation loss: 0.235\r",
      "Progress: 52.9% ... Training loss: 0.071 ... Validation loss: 0.190\r",
      "Progress: 53.0% ... Training loss: 0.077 ... Validation loss: 0.248\r",
      "Progress: 53.0% ... Training loss: 0.077 ... Validation loss: 0.177\r",
      "Progress: 53.0% ... Training loss: 0.070 ... Validation loss: 0.253\r",
      "Progress: 53.0% ... Training loss: 0.079 ... Validation loss: 0.195\r",
      "Progress: 53.0% ... Training loss: 0.081 ... Validation loss: 0.247\r",
      "Progress: 53.0% ... Training loss: 0.069 ... Validation loss: 0.180\r",
      "Progress: 53.0% ... Training loss: 0.067 ... Validation loss: 0.201\r",
      "Progress: 53.0% ... Training loss: 0.064 ... Validation loss: 0.199\r",
      "Progress: 53.0% ... Training loss: 0.065 ... Validation loss: 0.185\r",
      "Progress: 53.1% ... Training loss: 0.063 ... Validation loss: 0.198\r",
      "Progress: 53.1% ... Training loss: 0.071 ... Validation loss: 0.180\r",
      "Progress: 53.1% ... Training loss: 0.088 ... Validation loss: 0.252\r",
      "Progress: 53.1% ... Training loss: 0.076 ... Validation loss: 0.189\r",
      "Progress: 53.1% ... Training loss: 0.067 ... Validation loss: 0.227\r",
      "Progress: 53.1% ... Training loss: 0.075 ... Validation loss: 0.182\r",
      "Progress: 53.1% ... Training loss: 0.070 ... Validation loss: 0.238\r",
      "Progress: 53.1% ... Training loss: 0.071 ... Validation loss: 0.185\r",
      "Progress: 53.2% ... Training loss: 0.064 ... Validation loss: 0.210\r",
      "Progress: 53.2% ... Training loss: 0.064 ... Validation loss: 0.201\r",
      "Progress: 53.2% ... Training loss: 0.063 ... Validation loss: 0.214\r",
      "Progress: 53.2% ... Training loss: 0.065 ... Validation loss: 0.187\r",
      "Progress: 53.2% ... Training loss: 0.063 ... Validation loss: 0.200\r",
      "Progress: 53.2% ... Training loss: 0.063 ... Validation loss: 0.202\r",
      "Progress: 53.2% ... Training loss: 0.064 ... Validation loss: 0.201\r",
      "Progress: 53.2% ... Training loss: 0.064 ... Validation loss: 0.209\r",
      "Progress: 53.3% ... Training loss: 0.065 ... Validation loss: 0.204\r",
      "Progress: 53.3% ... Training loss: 0.072 ... Validation loss: 0.243\r",
      "Progress: 53.3% ... Training loss: 0.070 ... Validation loss: 0.187\r",
      "Progress: 53.3% ... Training loss: 0.065 ... Validation loss: 0.212\r",
      "Progress: 53.3% ... Training loss: 0.072 ... Validation loss: 0.178\r",
      "Progress: 53.3% ... Training loss: 0.072 ... Validation loss: 0.224\r",
      "Progress: 53.3% ... Training loss: 0.073 ... Validation loss: 0.184\r",
      "Progress: 53.4% ... Training loss: 0.072 ... Validation loss: 0.225\r",
      "Progress: 53.4% ... Training loss: 0.074 ... Validation loss: 0.179\r",
      "Progress: 53.4% ... Training loss: 0.078 ... Validation loss: 0.243\r",
      "Progress: 53.4% ... Training loss: 0.072 ... Validation loss: 0.169\r",
      "Progress: 53.4% ... Training loss: 0.064 ... Validation loss: 0.189\r",
      "Progress: 53.4% ... Training loss: 0.064 ... Validation loss: 0.181\r",
      "Progress: 53.4% ... Training loss: 0.065 ... Validation loss: 0.191\r",
      "Progress: 53.4% ... Training loss: 0.064 ... Validation loss: 0.185\r",
      "Progress: 53.5% ... Training loss: 0.066 ... Validation loss: 0.173\r",
      "Progress: 53.5% ... Training loss: 0.064 ... Validation loss: 0.180\r",
      "Progress: 53.5% ... Training loss: 0.067 ... Validation loss: 0.194\r",
      "Progress: 53.5% ... Training loss: 0.067 ... Validation loss: 0.172\r",
      "Progress: 53.5% ... Training loss: 0.067 ... Validation loss: 0.218\r",
      "Progress: 53.5% ... Training loss: 0.069 ... Validation loss: 0.185\r",
      "Progress: 53.5% ... Training loss: 0.065 ... Validation loss: 0.195\r",
      "Progress: 53.5% ... Training loss: 0.079 ... Validation loss: 0.171\r",
      "Progress: 53.5% ... Training loss: 0.077 ... Validation loss: 0.225\r",
      "Progress: 53.6% ... Training loss: 0.065 ... Validation loss: 0.182\r",
      "Progress: 53.6% ... Training loss: 0.067 ... Validation loss: 0.207\r",
      "Progress: 53.6% ... Training loss: 0.064 ... Validation loss: 0.187\r",
      "Progress: 53.6% ... Training loss: 0.063 ... Validation loss: 0.189\r",
      "Progress: 53.6% ... Training loss: 0.064 ... Validation loss: 0.195\r",
      "Progress: 53.6% ... Training loss: 0.064 ... Validation loss: 0.192\r",
      "Progress: 53.6% ... Training loss: 0.065 ... Validation loss: 0.181\r",
      "Progress: 53.6% ... Training loss: 0.064 ... Validation loss: 0.196\r",
      "Progress: 53.7% ... Training loss: 0.065 ... Validation loss: 0.194\r",
      "Progress: 53.7% ... Training loss: 0.063 ... Validation loss: 0.190\r",
      "Progress: 53.7% ... Training loss: 0.069 ... Validation loss: 0.176\r",
      "Progress: 53.7% ... Training loss: 0.073 ... Validation loss: 0.225\r",
      "Progress: 53.7% ... Training loss: 0.072 ... Validation loss: 0.184\r",
      "Progress: 53.7% ... Training loss: 0.067 ... Validation loss: 0.239\r",
      "Progress: 53.7% ... Training loss: 0.064 ... Validation loss: 0.210\r",
      "Progress: 53.8% ... Training loss: 0.066 ... Validation loss: 0.184\r",
      "Progress: 53.8% ... Training loss: 0.079 ... Validation loss: 0.245\r",
      "Progress: 53.8% ... Training loss: 0.070 ... Validation loss: 0.174\r",
      "Progress: 53.8% ... Training loss: 0.073 ... Validation loss: 0.220\r",
      "Progress: 53.8% ... Training loss: 0.078 ... Validation loss: 0.173\r",
      "Progress: 53.8% ... Training loss: 0.075 ... Validation loss: 0.239\r",
      "Progress: 53.8% ... Training loss: 0.064 ... Validation loss: 0.189\r",
      "Progress: 53.8% ... Training loss: 0.066 ... Validation loss: 0.200\r",
      "Progress: 53.9% ... Training loss: 0.065 ... Validation loss: 0.198\r",
      "Progress: 53.9% ... Training loss: 0.064 ... Validation loss: 0.220\r",
      "Progress: 53.9% ... Training loss: 0.065 ... Validation loss: 0.193\r",
      "Progress: 53.9% ... Training loss: 0.069 ... Validation loss: 0.227\r",
      "Progress: 53.9% ... Training loss: 0.064 ... Validation loss: 0.211\r",
      "Progress: 53.9% ... Training loss: 0.069 ... Validation loss: 0.248\r",
      "Progress: 53.9% ... Training loss: 0.064 ... Validation loss: 0.200\r",
      "Progress: 53.9% ... Training loss: 0.065 ... Validation loss: 0.194\r",
      "Progress: 54.0% ... Training loss: 0.068 ... Validation loss: 0.216\r",
      "Progress: 54.0% ... Training loss: 0.083 ... Validation loss: 0.168\r",
      "Progress: 54.0% ... Training loss: 0.093 ... Validation loss: 0.241\r",
      "Progress: 54.0% ... Training loss: 0.077 ... Validation loss: 0.162\r",
      "Progress: 54.0% ... Training loss: 0.067 ... Validation loss: 0.197\r",
      "Progress: 54.0% ... Training loss: 0.076 ... Validation loss: 0.172\r",
      "Progress: 54.0% ... Training loss: 0.076 ... Validation loss: 0.226\r",
      "Progress: 54.0% ... Training loss: 0.081 ... Validation loss: 0.166\r",
      "Progress: 54.0% ... Training loss: 0.069 ... Validation loss: 0.201\r",
      "Progress: 54.1% ... Training loss: 0.069 ... Validation loss: 0.165\r",
      "Progress: 54.1% ... Training loss: 0.072 ... Validation loss: 0.204\r",
      "Progress: 54.1% ... Training loss: 0.063 ... Validation loss: 0.182\r",
      "Progress: 54.1% ... Training loss: 0.064 ... Validation loss: 0.188\r",
      "Progress: 54.1% ... Training loss: 0.064 ... Validation loss: 0.198\r",
      "Progress: 54.1% ... Training loss: 0.065 ... Validation loss: 0.204\r",
      "Progress: 54.1% ... Training loss: 0.069 ... Validation loss: 0.175\r",
      "Progress: 54.1% ... Training loss: 0.068 ... Validation loss: 0.209\r",
      "Progress: 54.2% ... Training loss: 0.069 ... Validation loss: 0.177\r",
      "Progress: 54.2% ... Training loss: 0.064 ... Validation loss: 0.209\r",
      "Progress: 54.2% ... Training loss: 0.068 ... Validation loss: 0.181\r",
      "Progress: 54.2% ... Training loss: 0.066 ... Validation loss: 0.208\r",
      "Progress: 54.2% ... Training loss: 0.065 ... Validation loss: 0.191\r",
      "Progress: 54.2% ... Training loss: 0.064 ... Validation loss: 0.195\r",
      "Progress: 54.2% ... Training loss: 0.064 ... Validation loss: 0.200\r",
      "Progress: 54.2% ... Training loss: 0.064 ... Validation loss: 0.188\r",
      "Progress: 54.3% ... Training loss: 0.066 ... Validation loss: 0.178\r",
      "Progress: 54.3% ... Training loss: 0.064 ... Validation loss: 0.197\r",
      "Progress: 54.3% ... Training loss: 0.063 ... Validation loss: 0.200\r",
      "Progress: 54.3% ... Training loss: 0.066 ... Validation loss: 0.216\r",
      "Progress: 54.3% ... Training loss: 0.065 ... Validation loss: 0.192\r",
      "Progress: 54.3% ... Training loss: 0.066 ... Validation loss: 0.225\r",
      "Progress: 54.3% ... Training loss: 0.063 ... Validation loss: 0.199\r",
      "Progress: 54.4% ... Training loss: 0.064 ... Validation loss: 0.209\r",
      "Progress: 54.4% ... Training loss: 0.067 ... Validation loss: 0.221\r",
      "Progress: 54.4% ... Training loss: 0.064 ... Validation loss: 0.188\r",
      "Progress: 54.4% ... Training loss: 0.063 ... Validation loss: 0.195\r",
      "Progress: 54.4% ... Training loss: 0.066 ... Validation loss: 0.193\r",
      "Progress: 54.4% ... Training loss: 0.063 ... Validation loss: 0.198\r",
      "Progress: 54.4% ... Training loss: 0.064 ... Validation loss: 0.187\r",
      "Progress: 54.4% ... Training loss: 0.065 ... Validation loss: 0.175\r",
      "Progress: 54.5% ... Training loss: 0.069 ... Validation loss: 0.226\r",
      "Progress: 54.5% ... Training loss: 0.064 ... Validation loss: 0.193\r",
      "Progress: 54.5% ... Training loss: 0.063 ... Validation loss: 0.203\r",
      "Progress: 54.5% ... Training loss: 0.064 ... Validation loss: 0.188\r",
      "Progress: 54.5% ... Training loss: 0.065 ... Validation loss: 0.201\r",
      "Progress: 54.5% ... Training loss: 0.064 ... Validation loss: 0.211\r",
      "Progress: 54.5% ... Training loss: 0.064 ... Validation loss: 0.205\r",
      "Progress: 54.5% ... Training loss: 0.064 ... Validation loss: 0.196\r",
      "Progress: 54.5% ... Training loss: 0.066 ... Validation loss: 0.207\r",
      "Progress: 54.6% ... Training loss: 0.069 ... Validation loss: 0.184\r",
      "Progress: 54.6% ... Training loss: 0.066 ... Validation loss: 0.206\r",
      "Progress: 54.6% ... Training loss: 0.063 ... Validation loss: 0.201\r",
      "Progress: 54.6% ... Training loss: 0.078 ... Validation loss: 0.181\r",
      "Progress: 54.6% ... Training loss: 0.072 ... Validation loss: 0.224\r",
      "Progress: 54.6% ... Training loss: 0.071 ... Validation loss: 0.170\r",
      "Progress: 54.6% ... Training loss: 0.076 ... Validation loss: 0.246\r",
      "Progress: 54.6% ... Training loss: 0.073 ... Validation loss: 0.192\r",
      "Progress: 54.7% ... Training loss: 0.115 ... Validation loss: 0.307\r",
      "Progress: 54.7% ... Training loss: 0.084 ... Validation loss: 0.175\r",
      "Progress: 54.7% ... Training loss: 0.084 ... Validation loss: 0.251\r",
      "Progress: 54.7% ... Training loss: 0.099 ... Validation loss: 0.175\r",
      "Progress: 54.7% ... Training loss: 0.130 ... Validation loss: 0.306\r",
      "Progress: 54.7% ... Training loss: 0.118 ... Validation loss: 0.168\r",
      "Progress: 54.7% ... Training loss: 0.144 ... Validation loss: 0.262\r",
      "Progress: 54.8% ... Training loss: 0.098 ... Validation loss: 0.164\r",
      "Progress: 54.8% ... Training loss: 0.106 ... Validation loss: 0.277\r",
      "Progress: 54.8% ... Training loss: 0.080 ... Validation loss: 0.171\r",
      "Progress: 54.8% ... Training loss: 0.092 ... Validation loss: 0.260\r",
      "Progress: 54.8% ... Training loss: 0.078 ... Validation loss: 0.177\r",
      "Progress: 54.8% ... Training loss: 0.065 ... Validation loss: 0.204\r",
      "Progress: 54.8% ... Training loss: 0.065 ... Validation loss: 0.210\r",
      "Progress: 54.8% ... Training loss: 0.066 ... Validation loss: 0.192\r",
      "Progress: 54.9% ... Training loss: 0.069 ... Validation loss: 0.226\r",
      "Progress: 54.9% ... Training loss: 0.070 ... Validation loss: 0.182\r",
      "Progress: 54.9% ... Training loss: 0.075 ... Validation loss: 0.228\r",
      "Progress: 54.9% ... Training loss: 0.069 ... Validation loss: 0.180\r",
      "Progress: 54.9% ... Training loss: 0.064 ... Validation loss: 0.192\r",
      "Progress: 54.9% ... Training loss: 0.065 ... Validation loss: 0.213\r",
      "Progress: 54.9% ... Training loss: 0.064 ... Validation loss: 0.181\r",
      "Progress: 54.9% ... Training loss: 0.065 ... Validation loss: 0.192\r",
      "Progress: 55.0% ... Training loss: 0.067 ... Validation loss: 0.219\r",
      "Progress: 55.0% ... Training loss: 0.064 ... Validation loss: 0.201\r",
      "Progress: 55.0% ... Training loss: 0.065 ... Validation loss: 0.203\r",
      "Progress: 55.0% ... Training loss: 0.073 ... Validation loss: 0.171\r",
      "Progress: 55.0% ... Training loss: 0.065 ... Validation loss: 0.195\r",
      "Progress: 55.0% ... Training loss: 0.063 ... Validation loss: 0.193\r",
      "Progress: 55.0% ... Training loss: 0.063 ... Validation loss: 0.203\r",
      "Progress: 55.0% ... Training loss: 0.067 ... Validation loss: 0.172\r",
      "Progress: 55.0% ... Training loss: 0.064 ... Validation loss: 0.209\r",
      "Progress: 55.1% ... Training loss: 0.065 ... Validation loss: 0.194\r",
      "Progress: 55.1% ... Training loss: 0.063 ... Validation loss: 0.202\r",
      "Progress: 55.1% ... Training loss: 0.064 ... Validation loss: 0.217\r",
      "Progress: 55.1% ... Training loss: 0.063 ... Validation loss: 0.195\r",
      "Progress: 55.1% ... Training loss: 0.064 ... Validation loss: 0.216\r",
      "Progress: 55.1% ... Training loss: 0.063 ... Validation loss: 0.204\r",
      "Progress: 55.1% ... Training loss: 0.065 ... Validation loss: 0.189\r",
      "Progress: 55.1% ... Training loss: 0.063 ... Validation loss: 0.191\r",
      "Progress: 55.2% ... Training loss: 0.063 ... Validation loss: 0.193\r",
      "Progress: 55.2% ... Training loss: 0.063 ... Validation loss: 0.199\r",
      "Progress: 55.2% ... Training loss: 0.063 ... Validation loss: 0.203\r",
      "Progress: 55.2% ... Training loss: 0.064 ... Validation loss: 0.184\r",
      "Progress: 55.2% ... Training loss: 0.063 ... Validation loss: 0.208\r",
      "Progress: 55.2% ... Training loss: 0.062 ... Validation loss: 0.189\r",
      "Progress: 55.2% ... Training loss: 0.063 ... Validation loss: 0.212\r",
      "Progress: 55.2% ... Training loss: 0.065 ... Validation loss: 0.182\r",
      "Progress: 55.3% ... Training loss: 0.068 ... Validation loss: 0.200\r",
      "Progress: 55.3% ... Training loss: 0.068 ... Validation loss: 0.174\r",
      "Progress: 55.3% ... Training loss: 0.078 ... Validation loss: 0.212\r",
      "Progress: 55.3% ... Training loss: 0.080 ... Validation loss: 0.166\r",
      "Progress: 55.3% ... Training loss: 0.070 ... Validation loss: 0.196\r",
      "Progress: 55.3% ... Training loss: 0.063 ... Validation loss: 0.173\r",
      "Progress: 55.3% ... Training loss: 0.066 ... Validation loss: 0.190\r",
      "Progress: 55.4% ... Training loss: 0.064 ... Validation loss: 0.182\r",
      "Progress: 55.4% ... Training loss: 0.063 ... Validation loss: 0.197\r",
      "Progress: 55.4% ... Training loss: 0.064 ... Validation loss: 0.198\r",
      "Progress: 55.4% ... Training loss: 0.068 ... Validation loss: 0.207\r",
      "Progress: 55.4% ... Training loss: 0.074 ... Validation loss: 0.168\r",
      "Progress: 55.4% ... Training loss: 0.075 ... Validation loss: 0.232\r",
      "Progress: 55.4% ... Training loss: 0.072 ... Validation loss: 0.172\r",
      "Progress: 55.4% ... Training loss: 0.072 ... Validation loss: 0.218\r",
      "Progress: 55.5% ... Training loss: 0.082 ... Validation loss: 0.168\r",
      "Progress: 55.5% ... Training loss: 0.094 ... Validation loss: 0.233\r",
      "Progress: 55.5% ... Training loss: 0.079 ... Validation loss: 0.159\r",
      "Progress: 55.5% ... Training loss: 0.064 ... Validation loss: 0.207\r",
      "Progress: 55.5% ... Training loss: 0.064 ... Validation loss: 0.179\r",
      "Progress: 55.5% ... Training loss: 0.064 ... Validation loss: 0.196\r",
      "Progress: 55.5% ... Training loss: 0.065 ... Validation loss: 0.176\r",
      "Progress: 55.5% ... Training loss: 0.063 ... Validation loss: 0.199\r",
      "Progress: 55.5% ... Training loss: 0.066 ... Validation loss: 0.204\r",
      "Progress: 55.6% ... Training loss: 0.062 ... Validation loss: 0.206\r",
      "Progress: 55.6% ... Training loss: 0.065 ... Validation loss: 0.205\r",
      "Progress: 55.6% ... Training loss: 0.063 ... Validation loss: 0.185\r",
      "Progress: 55.6% ... Training loss: 0.063 ... Validation loss: 0.188\r",
      "Progress: 55.6% ... Training loss: 0.063 ... Validation loss: 0.198\r",
      "Progress: 55.6% ... Training loss: 0.063 ... Validation loss: 0.181\r",
      "Progress: 55.6% ... Training loss: 0.064 ... Validation loss: 0.185\r",
      "Progress: 55.6% ... Training loss: 0.067 ... Validation loss: 0.219\r",
      "Progress: 55.7% ... Training loss: 0.069 ... Validation loss: 0.179\r",
      "Progress: 55.7% ... Training loss: 0.065 ... Validation loss: 0.214\r",
      "Progress: 55.7% ... Training loss: 0.065 ... Validation loss: 0.173\r",
      "Progress: 55.7% ... Training loss: 0.065 ... Validation loss: 0.199\r",
      "Progress: 55.7% ... Training loss: 0.064 ... Validation loss: 0.184\r",
      "Progress: 55.7% ... Training loss: 0.063 ... Validation loss: 0.193\r",
      "Progress: 55.7% ... Training loss: 0.064 ... Validation loss: 0.194\r",
      "Progress: 55.8% ... Training loss: 0.064 ... Validation loss: 0.180\r",
      "Progress: 55.8% ... Training loss: 0.063 ... Validation loss: 0.178\r",
      "Progress: 55.8% ... Training loss: 0.063 ... Validation loss: 0.193\r",
      "Progress: 55.8% ... Training loss: 0.063 ... Validation loss: 0.197\r",
      "Progress: 55.8% ... Training loss: 0.064 ... Validation loss: 0.201\r",
      "Progress: 55.8% ... Training loss: 0.063 ... Validation loss: 0.196\r",
      "Progress: 55.8% ... Training loss: 0.068 ... Validation loss: 0.203\r",
      "Progress: 55.8% ... Training loss: 0.066 ... Validation loss: 0.173\r",
      "Progress: 55.9% ... Training loss: 0.079 ... Validation loss: 0.220\r",
      "Progress: 55.9% ... Training loss: 0.089 ... Validation loss: 0.171\r",
      "Progress: 55.9% ... Training loss: 0.078 ... Validation loss: 0.241\r",
      "Progress: 55.9% ... Training loss: 0.065 ... Validation loss: 0.188\r",
      "Progress: 55.9% ... Training loss: 0.062 ... Validation loss: 0.193\r",
      "Progress: 55.9% ... Training loss: 0.065 ... Validation loss: 0.203\r",
      "Progress: 55.9% ... Training loss: 0.063 ... Validation loss: 0.179\r",
      "Progress: 55.9% ... Training loss: 0.063 ... Validation loss: 0.196\r",
      "Progress: 56.0% ... Training loss: 0.067 ... Validation loss: 0.189\r",
      "Progress: 56.0% ... Training loss: 0.068 ... Validation loss: 0.228\r",
      "Progress: 56.0% ... Training loss: 0.068 ... Validation loss: 0.179\r",
      "Progress: 56.0% ... Training loss: 0.064 ... Validation loss: 0.204\r",
      "Progress: 56.0% ... Training loss: 0.068 ... Validation loss: 0.175\r",
      "Progress: 56.0% ... Training loss: 0.064 ... Validation loss: 0.194\r",
      "Progress: 56.0% ... Training loss: 0.063 ... Validation loss: 0.207\r",
      "Progress: 56.0% ... Training loss: 0.062 ... Validation loss: 0.198\r",
      "Progress: 56.0% ... Training loss: 0.075 ... Validation loss: 0.180\r",
      "Progress: 56.1% ... Training loss: 0.078 ... Validation loss: 0.239\r",
      "Progress: 56.1% ... Training loss: 0.065 ... Validation loss: 0.186\r",
      "Progress: 56.1% ... Training loss: 0.064 ... Validation loss: 0.199\r",
      "Progress: 56.1% ... Training loss: 0.065 ... Validation loss: 0.175\r",
      "Progress: 56.1% ... Training loss: 0.062 ... Validation loss: 0.189\r",
      "Progress: 56.1% ... Training loss: 0.062 ... Validation loss: 0.178\r",
      "Progress: 56.1% ... Training loss: 0.066 ... Validation loss: 0.202\r",
      "Progress: 56.1% ... Training loss: 0.069 ... Validation loss: 0.180\r",
      "Progress: 56.2% ... Training loss: 0.070 ... Validation loss: 0.210\r",
      "Progress: 56.2% ... Training loss: 0.075 ... Validation loss: 0.165\r",
      "Progress: 56.2% ... Training loss: 0.067 ... Validation loss: 0.213\r",
      "Progress: 56.2% ... Training loss: 0.062 ... Validation loss: 0.193\r",
      "Progress: 56.2% ... Training loss: 0.064 ... Validation loss: 0.187\r",
      "Progress: 56.2% ... Training loss: 0.081 ... Validation loss: 0.253\r",
      "Progress: 56.2% ... Training loss: 0.085 ... Validation loss: 0.168\r",
      "Progress: 56.2% ... Training loss: 0.112 ... Validation loss: 0.283\r",
      "Progress: 56.3% ... Training loss: 0.095 ... Validation loss: 0.169\r",
      "Progress: 56.3% ... Training loss: 0.105 ... Validation loss: 0.272\r",
      "Progress: 56.3% ... Training loss: 0.070 ... Validation loss: 0.166\r",
      "Progress: 56.3% ... Training loss: 0.068 ... Validation loss: 0.203\r",
      "Progress: 56.3% ... Training loss: 0.075 ... Validation loss: 0.174\r",
      "Progress: 56.3% ... Training loss: 0.074 ... Validation loss: 0.230\r",
      "Progress: 56.3% ... Training loss: 0.065 ... Validation loss: 0.175\r",
      "Progress: 56.4% ... Training loss: 0.062 ... Validation loss: 0.189\r",
      "Progress: 56.4% ... Training loss: 0.063 ... Validation loss: 0.199\r",
      "Progress: 56.4% ... Training loss: 0.064 ... Validation loss: 0.186\r",
      "Progress: 56.4% ... Training loss: 0.063 ... Validation loss: 0.177\r",
      "Progress: 56.4% ... Training loss: 0.063 ... Validation loss: 0.179\r",
      "Progress: 56.4% ... Training loss: 0.065 ... Validation loss: 0.178\r",
      "Progress: 56.4% ... Training loss: 0.065 ... Validation loss: 0.200\r",
      "Progress: 56.4% ... Training loss: 0.069 ... Validation loss: 0.181\r",
      "Progress: 56.5% ... Training loss: 0.074 ... Validation loss: 0.228\r",
      "Progress: 56.5% ... Training loss: 0.069 ... Validation loss: 0.183\r",
      "Progress: 56.5% ... Training loss: 0.071 ... Validation loss: 0.230\r",
      "Progress: 56.5% ... Training loss: 0.065 ... Validation loss: 0.183\r",
      "Progress: 56.5% ... Training loss: 0.071 ... Validation loss: 0.236\r",
      "Progress: 56.5% ... Training loss: 0.065 ... Validation loss: 0.194\r",
      "Progress: 56.5% ... Training loss: 0.069 ... Validation loss: 0.230\r",
      "Progress: 56.5% ... Training loss: 0.069 ... Validation loss: 0.175\r",
      "Progress: 56.5% ... Training loss: 0.071 ... Validation loss: 0.256\r",
      "Progress: 56.6% ... Training loss: 0.071 ... Validation loss: 0.201\r",
      "Progress: 56.6% ... Training loss: 0.069 ... Validation loss: 0.241\r",
      "Progress: 56.6% ... Training loss: 0.063 ... Validation loss: 0.184\r",
      "Progress: 56.6% ... Training loss: 0.062 ... Validation loss: 0.196\r",
      "Progress: 56.6% ... Training loss: 0.062 ... Validation loss: 0.192\r",
      "Progress: 56.6% ... Training loss: 0.063 ... Validation loss: 0.192\r",
      "Progress: 56.6% ... Training loss: 0.064 ... Validation loss: 0.202\r",
      "Progress: 56.6% ... Training loss: 0.063 ... Validation loss: 0.186\r",
      "Progress: 56.7% ... Training loss: 0.063 ... Validation loss: 0.203\r",
      "Progress: 56.7% ... Training loss: 0.062 ... Validation loss: 0.185\r",
      "Progress: 56.7% ... Training loss: 0.065 ... Validation loss: 0.213\r",
      "Progress: 56.7% ... Training loss: 0.079 ... Validation loss: 0.177\r",
      "Progress: 56.7% ... Training loss: 0.070 ... Validation loss: 0.224\r",
      "Progress: 56.7% ... Training loss: 0.063 ... Validation loss: 0.186\r",
      "Progress: 56.7% ... Training loss: 0.064 ... Validation loss: 0.221\r",
      "Progress: 56.8% ... Training loss: 0.062 ... Validation loss: 0.204\r",
      "Progress: 56.8% ... Training loss: 0.074 ... Validation loss: 0.183\r",
      "Progress: 56.8% ... Training loss: 0.067 ... Validation loss: 0.223\r",
      "Progress: 56.8% ... Training loss: 0.071 ... Validation loss: 0.186\r",
      "Progress: 56.8% ... Training loss: 0.064 ... Validation loss: 0.223\r",
      "Progress: 56.8% ... Training loss: 0.064 ... Validation loss: 0.190\r",
      "Progress: 56.8% ... Training loss: 0.063 ... Validation loss: 0.204\r",
      "Progress: 56.8% ... Training loss: 0.066 ... Validation loss: 0.183\r",
      "Progress: 56.9% ... Training loss: 0.063 ... Validation loss: 0.209\r",
      "Progress: 56.9% ... Training loss: 0.065 ... Validation loss: 0.193\r",
      "Progress: 56.9% ... Training loss: 0.066 ... Validation loss: 0.214\r",
      "Progress: 56.9% ... Training loss: 0.069 ... Validation loss: 0.178\r",
      "Progress: 56.9% ... Training loss: 0.063 ... Validation loss: 0.201\r",
      "Progress: 56.9% ... Training loss: 0.063 ... Validation loss: 0.188\r",
      "Progress: 56.9% ... Training loss: 0.064 ... Validation loss: 0.206\r",
      "Progress: 56.9% ... Training loss: 0.065 ... Validation loss: 0.205\r",
      "Progress: 57.0% ... Training loss: 0.065 ... Validation loss: 0.183\r",
      "Progress: 57.0% ... Training loss: 0.064 ... Validation loss: 0.197\r",
      "Progress: 57.0% ... Training loss: 0.063 ... Validation loss: 0.190\r",
      "Progress: 57.0% ... Training loss: 0.066 ... Validation loss: 0.172\r",
      "Progress: 57.0% ... Training loss: 0.070 ... Validation loss: 0.210\r",
      "Progress: 57.0% ... Training loss: 0.068 ... Validation loss: 0.172\r",
      "Progress: 57.0% ... Training loss: 0.065 ... Validation loss: 0.192\r",
      "Progress: 57.0% ... Training loss: 0.062 ... Validation loss: 0.189\r",
      "Progress: 57.0% ... Training loss: 0.062 ... Validation loss: 0.192\r",
      "Progress: 57.1% ... Training loss: 0.069 ... Validation loss: 0.173\r",
      "Progress: 57.1% ... Training loss: 0.062 ... Validation loss: 0.192\r",
      "Progress: 57.1% ... Training loss: 0.066 ... Validation loss: 0.207\r",
      "Progress: 57.1% ... Training loss: 0.067 ... Validation loss: 0.170\r",
      "Progress: 57.1% ... Training loss: 0.064 ... Validation loss: 0.207\r",
      "Progress: 57.1% ... Training loss: 0.063 ... Validation loss: 0.189\r",
      "Progress: 57.1% ... Training loss: 0.062 ... Validation loss: 0.185\r",
      "Progress: 57.1% ... Training loss: 0.063 ... Validation loss: 0.183\r",
      "Progress: 57.2% ... Training loss: 0.062 ... Validation loss: 0.181\r",
      "Progress: 57.2% ... Training loss: 0.063 ... Validation loss: 0.195\r",
      "Progress: 57.2% ... Training loss: 0.062 ... Validation loss: 0.176\r",
      "Progress: 57.2% ... Training loss: 0.062 ... Validation loss: 0.179\r",
      "Progress: 57.2% ... Training loss: 0.062 ... Validation loss: 0.178\r",
      "Progress: 57.2% ... Training loss: 0.062 ... Validation loss: 0.189\r",
      "Progress: 57.2% ... Training loss: 0.063 ... Validation loss: 0.199\r",
      "Progress: 57.2% ... Training loss: 0.075 ... Validation loss: 0.175\r",
      "Progress: 57.3% ... Training loss: 0.066 ... Validation loss: 0.218\r",
      "Progress: 57.3% ... Training loss: 0.063 ... Validation loss: 0.190\r",
      "Progress: 57.3% ... Training loss: 0.061 ... Validation loss: 0.187\r",
      "Progress: 57.3% ... Training loss: 0.062 ... Validation loss: 0.194\r",
      "Progress: 57.3% ... Training loss: 0.062 ... Validation loss: 0.192\r",
      "Progress: 57.3% ... Training loss: 0.062 ... Validation loss: 0.199\r",
      "Progress: 57.3% ... Training loss: 0.062 ... Validation loss: 0.197\r",
      "Progress: 57.4% ... Training loss: 0.071 ... Validation loss: 0.183\r",
      "Progress: 57.4% ... Training loss: 0.067 ... Validation loss: 0.211\r",
      "Progress: 57.4% ... Training loss: 0.070 ... Validation loss: 0.181\r",
      "Progress: 57.4% ... Training loss: 0.068 ... Validation loss: 0.206\r",
      "Progress: 57.4% ... Training loss: 0.068 ... Validation loss: 0.171\r",
      "Progress: 57.4% ... Training loss: 0.072 ... Validation loss: 0.221\r",
      "Progress: 57.4% ... Training loss: 0.087 ... Validation loss: 0.176\r",
      "Progress: 57.4% ... Training loss: 0.065 ... Validation loss: 0.207\r",
      "Progress: 57.5% ... Training loss: 0.062 ... Validation loss: 0.190\r",
      "Progress: 57.5% ... Training loss: 0.062 ... Validation loss: 0.195\r",
      "Progress: 57.5% ... Training loss: 0.061 ... Validation loss: 0.188\r",
      "Progress: 57.5% ... Training loss: 0.069 ... Validation loss: 0.174\r",
      "Progress: 57.5% ... Training loss: 0.079 ... Validation loss: 0.255\r",
      "Progress: 57.5% ... Training loss: 0.069 ... Validation loss: 0.173\r",
      "Progress: 57.5% ... Training loss: 0.067 ... Validation loss: 0.215\r",
      "Progress: 57.5% ... Training loss: 0.070 ... Validation loss: 0.178\r",
      "Progress: 57.5% ... Training loss: 0.072 ... Validation loss: 0.218\r",
      "Progress: 57.6% ... Training loss: 0.066 ... Validation loss: 0.183\r",
      "Progress: 57.6% ... Training loss: 0.066 ... Validation loss: 0.208\r",
      "Progress: 57.6% ... Training loss: 0.063 ... Validation loss: 0.189\r",
      "Progress: 57.6% ... Training loss: 0.073 ... Validation loss: 0.224\r",
      "Progress: 57.6% ... Training loss: 0.079 ... Validation loss: 0.171\r",
      "Progress: 57.6% ... Training loss: 0.082 ... Validation loss: 0.236\r",
      "Progress: 57.6% ... Training loss: 0.064 ... Validation loss: 0.175\r",
      "Progress: 57.6% ... Training loss: 0.072 ... Validation loss: 0.233\r",
      "Progress: 57.7% ... Training loss: 0.068 ... Validation loss: 0.179\r",
      "Progress: 57.7% ... Training loss: 0.063 ... Validation loss: 0.194\r",
      "Progress: 57.7% ... Training loss: 0.064 ... Validation loss: 0.184\r",
      "Progress: 57.7% ... Training loss: 0.062 ... Validation loss: 0.194\r",
      "Progress: 57.7% ... Training loss: 0.063 ... Validation loss: 0.185\r",
      "Progress: 57.7% ... Training loss: 0.061 ... Validation loss: 0.206\r",
      "Progress: 57.7% ... Training loss: 0.062 ... Validation loss: 0.197\r",
      "Progress: 57.8% ... Training loss: 0.062 ... Validation loss: 0.194\r",
      "Progress: 57.8% ... Training loss: 0.063 ... Validation loss: 0.207\r",
      "Progress: 57.8% ... Training loss: 0.064 ... Validation loss: 0.213\r",
      "Progress: 57.8% ... Training loss: 0.065 ... Validation loss: 0.213\r",
      "Progress: 57.8% ... Training loss: 0.062 ... Validation loss: 0.186\r",
      "Progress: 57.8% ... Training loss: 0.062 ... Validation loss: 0.198\r",
      "Progress: 57.8% ... Training loss: 0.065 ... Validation loss: 0.207\r",
      "Progress: 57.8% ... Training loss: 0.065 ... Validation loss: 0.188\r",
      "Progress: 57.9% ... Training loss: 0.067 ... Validation loss: 0.213\r",
      "Progress: 57.9% ... Training loss: 0.079 ... Validation loss: 0.175\r",
      "Progress: 57.9% ... Training loss: 0.070 ... Validation loss: 0.254\r",
      "Progress: 57.9% ... Training loss: 0.069 ... Validation loss: 0.210\r",
      "Progress: 57.9% ... Training loss: 0.071 ... Validation loss: 0.270\r",
      "Progress: 57.9% ... Training loss: 0.063 ... Validation loss: 0.219\r",
      "Progress: 57.9% ... Training loss: 0.064 ... Validation loss: 0.228\r",
      "Progress: 57.9% ... Training loss: 0.063 ... Validation loss: 0.203\r",
      "Progress: 58.0% ... Training loss: 0.062 ... Validation loss: 0.192\r",
      "Progress: 58.0% ... Training loss: 0.062 ... Validation loss: 0.204\r",
      "Progress: 58.0% ... Training loss: 0.062 ... Validation loss: 0.194\r",
      "Progress: 58.0% ... Training loss: 0.063 ... Validation loss: 0.210\r",
      "Progress: 58.0% ... Training loss: 0.062 ... Validation loss: 0.209\r",
      "Progress: 58.0% ... Training loss: 0.063 ... Validation loss: 0.213\r",
      "Progress: 58.0% ... Training loss: 0.063 ... Validation loss: 0.204\r",
      "Progress: 58.0% ... Training loss: 0.068 ... Validation loss: 0.249\r",
      "Progress: 58.0% ... Training loss: 0.085 ... Validation loss: 0.178\r",
      "Progress: 58.1% ... Training loss: 0.069 ... Validation loss: 0.253\r",
      "Progress: 58.1% ... Training loss: 0.091 ... Validation loss: 0.174\r",
      "Progress: 58.1% ... Training loss: 0.069 ... Validation loss: 0.244\r",
      "Progress: 58.1% ... Training loss: 0.067 ... Validation loss: 0.172\r",
      "Progress: 58.1% ... Training loss: 0.069 ... Validation loss: 0.210\r",
      "Progress: 58.1% ... Training loss: 0.063 ... Validation loss: 0.198\r",
      "Progress: 58.1% ... Training loss: 0.063 ... Validation loss: 0.209\r",
      "Progress: 58.1% ... Training loss: 0.063 ... Validation loss: 0.188\r",
      "Progress: 58.2% ... Training loss: 0.065 ... Validation loss: 0.223\r",
      "Progress: 58.2% ... Training loss: 0.062 ... Validation loss: 0.193\r",
      "Progress: 58.2% ... Training loss: 0.063 ... Validation loss: 0.221\r",
      "Progress: 58.2% ... Training loss: 0.063 ... Validation loss: 0.190\r",
      "Progress: 58.2% ... Training loss: 0.063 ... Validation loss: 0.192\r",
      "Progress: 58.2% ... Training loss: 0.068 ... Validation loss: 0.210\r",
      "Progress: 58.2% ... Training loss: 0.066 ... Validation loss: 0.179\r",
      "Progress: 58.2% ... Training loss: 0.066 ... Validation loss: 0.216\r",
      "Progress: 58.3% ... Training loss: 0.065 ... Validation loss: 0.179\r",
      "Progress: 58.3% ... Training loss: 0.078 ... Validation loss: 0.237\r",
      "Progress: 58.3% ... Training loss: 0.079 ... Validation loss: 0.167\r",
      "Progress: 58.3% ... Training loss: 0.080 ... Validation loss: 0.247\r",
      "Progress: 58.3% ... Training loss: 0.064 ... Validation loss: 0.180\r",
      "Progress: 58.3% ... Training loss: 0.068 ... Validation loss: 0.203\r",
      "Progress: 58.3% ... Training loss: 0.061 ... Validation loss: 0.188\r",
      "Progress: 58.4% ... Training loss: 0.063 ... Validation loss: 0.180\r",
      "Progress: 58.4% ... Training loss: 0.062 ... Validation loss: 0.184\r",
      "Progress: 58.4% ... Training loss: 0.066 ... Validation loss: 0.211\r",
      "Progress: 58.4% ... Training loss: 0.062 ... Validation loss: 0.214\r",
      "Progress: 58.4% ... Training loss: 0.063 ... Validation loss: 0.219\r",
      "Progress: 58.4% ... Training loss: 0.070 ... Validation loss: 0.175\r",
      "Progress: 58.4% ... Training loss: 0.071 ... Validation loss: 0.225\r",
      "Progress: 58.4% ... Training loss: 0.071 ... Validation loss: 0.178\r",
      "Progress: 58.5% ... Training loss: 0.108 ... Validation loss: 0.280\r",
      "Progress: 58.5% ... Training loss: 0.103 ... Validation loss: 0.161\r",
      "Progress: 58.5% ... Training loss: 0.078 ... Validation loss: 0.240\r",
      "Progress: 58.5% ... Training loss: 0.072 ... Validation loss: 0.172\r",
      "Progress: 58.5% ... Training loss: 0.079 ... Validation loss: 0.210\r",
      "Progress: 58.5% ... Training loss: 0.067 ... Validation loss: 0.160\r",
      "Progress: 58.5% ... Training loss: 0.065 ... Validation loss: 0.192\r",
      "Progress: 58.5% ... Training loss: 0.062 ... Validation loss: 0.169\r",
      "Progress: 58.5% ... Training loss: 0.062 ... Validation loss: 0.174\r",
      "Progress: 58.6% ... Training loss: 0.069 ... Validation loss: 0.211\r",
      "Progress: 58.6% ... Training loss: 0.061 ... Validation loss: 0.185\r",
      "Progress: 58.6% ... Training loss: 0.062 ... Validation loss: 0.186\r",
      "Progress: 58.6% ... Training loss: 0.066 ... Validation loss: 0.212\r",
      "Progress: 58.6% ... Training loss: 0.064 ... Validation loss: 0.166\r",
      "Progress: 58.6% ... Training loss: 0.063 ... Validation loss: 0.189\r",
      "Progress: 58.6% ... Training loss: 0.063 ... Validation loss: 0.178\r",
      "Progress: 58.6% ... Training loss: 0.063 ... Validation loss: 0.188\r",
      "Progress: 58.7% ... Training loss: 0.066 ... Validation loss: 0.168\r",
      "Progress: 58.7% ... Training loss: 0.075 ... Validation loss: 0.228\r",
      "Progress: 58.7% ... Training loss: 0.081 ... Validation loss: 0.167\r",
      "Progress: 58.7% ... Training loss: 0.066 ... Validation loss: 0.211\r",
      "Progress: 58.7% ... Training loss: 0.066 ... Validation loss: 0.182\r",
      "Progress: 58.7% ... Training loss: 0.063 ... Validation loss: 0.207\r",
      "Progress: 58.7% ... Training loss: 0.063 ... Validation loss: 0.182\r",
      "Progress: 58.8% ... Training loss: 0.061 ... Validation loss: 0.186\r",
      "Progress: 58.8% ... Training loss: 0.062 ... Validation loss: 0.196\r",
      "Progress: 58.8% ... Training loss: 0.070 ... Validation loss: 0.221\r",
      "Progress: 58.8% ... Training loss: 0.077 ... Validation loss: 0.173\r",
      "Progress: 58.8% ... Training loss: 0.075 ... Validation loss: 0.237\r",
      "Progress: 58.8% ... Training loss: 0.068 ... Validation loss: 0.175\r",
      "Progress: 58.8% ... Training loss: 0.064 ... Validation loss: 0.222\r",
      "Progress: 58.8% ... Training loss: 0.069 ... Validation loss: 0.170\r",
      "Progress: 58.9% ... Training loss: 0.065 ... Validation loss: 0.191\r",
      "Progress: 58.9% ... Training loss: 0.064 ... Validation loss: 0.175\r",
      "Progress: 58.9% ... Training loss: 0.063 ... Validation loss: 0.186\r",
      "Progress: 58.9% ... Training loss: 0.062 ... Validation loss: 0.203\r",
      "Progress: 58.9% ... Training loss: 0.062 ... Validation loss: 0.191\r",
      "Progress: 58.9% ... Training loss: 0.062 ... Validation loss: 0.193\r",
      "Progress: 58.9% ... Training loss: 0.062 ... Validation loss: 0.184\r",
      "Progress: 58.9% ... Training loss: 0.062 ... Validation loss: 0.188\r",
      "Progress: 59.0% ... Training loss: 0.068 ... Validation loss: 0.172\r",
      "Progress: 59.0% ... Training loss: 0.062 ... Validation loss: 0.188\r",
      "Progress: 59.0% ... Training loss: 0.065 ... Validation loss: 0.166\r",
      "Progress: 59.0% ... Training loss: 0.065 ... Validation loss: 0.202\r",
      "Progress: 59.0% ... Training loss: 0.074 ... Validation loss: 0.164\r",
      "Progress: 59.0% ... Training loss: 0.079 ... Validation loss: 0.225\r",
      "Progress: 59.0% ... Training loss: 0.064 ... Validation loss: 0.172\r",
      "Progress: 59.0% ... Training loss: 0.062 ... Validation loss: 0.177\r",
      "Progress: 59.0% ... Training loss: 0.062 ... Validation loss: 0.179\r",
      "Progress: 59.1% ... Training loss: 0.062 ... Validation loss: 0.188\r",
      "Progress: 59.1% ... Training loss: 0.065 ... Validation loss: 0.195\r",
      "Progress: 59.1% ... Training loss: 0.064 ... Validation loss: 0.173\r",
      "Progress: 59.1% ... Training loss: 0.061 ... Validation loss: 0.188\r",
      "Progress: 59.1% ... Training loss: 0.065 ... Validation loss: 0.186\r",
      "Progress: 59.1% ... Training loss: 0.071 ... Validation loss: 0.217\r",
      "Progress: 59.1% ... Training loss: 0.064 ... Validation loss: 0.172\r",
      "Progress: 59.1% ... Training loss: 0.064 ... Validation loss: 0.183\r",
      "Progress: 59.2% ... Training loss: 0.063 ... Validation loss: 0.178\r",
      "Progress: 59.2% ... Training loss: 0.062 ... Validation loss: 0.220\r",
      "Progress: 59.2% ... Training loss: 0.062 ... Validation loss: 0.188\r",
      "Progress: 59.2% ... Training loss: 0.074 ... Validation loss: 0.219\r",
      "Progress: 59.2% ... Training loss: 0.080 ... Validation loss: 0.180\r",
      "Progress: 59.2% ... Training loss: 0.082 ... Validation loss: 0.264\r",
      "Progress: 59.2% ... Training loss: 0.087 ... Validation loss: 0.178\r",
      "Progress: 59.2% ... Training loss: 0.073 ... Validation loss: 0.252\r",
      "Progress: 59.3% ... Training loss: 0.071 ... Validation loss: 0.178\r",
      "Progress: 59.3% ... Training loss: 0.070 ... Validation loss: 0.209\r",
      "Progress: 59.3% ... Training loss: 0.068 ... Validation loss: 0.166\r",
      "Progress: 59.3% ... Training loss: 0.065 ... Validation loss: 0.207\r",
      "Progress: 59.3% ... Training loss: 0.062 ... Validation loss: 0.185\r",
      "Progress: 59.3% ... Training loss: 0.065 ... Validation loss: 0.206\r",
      "Progress: 59.3% ... Training loss: 0.071 ... Validation loss: 0.181\r",
      "Progress: 59.4% ... Training loss: 0.079 ... Validation loss: 0.256\r",
      "Progress: 59.4% ... Training loss: 0.071 ... Validation loss: 0.183\r",
      "Progress: 59.4% ... Training loss: 0.061 ... Validation loss: 0.196\r",
      "Progress: 59.4% ... Training loss: 0.063 ... Validation loss: 0.182\r",
      "Progress: 59.4% ... Training loss: 0.063 ... Validation loss: 0.182\r",
      "Progress: 59.4% ... Training loss: 0.067 ... Validation loss: 0.225\r",
      "Progress: 59.4% ... Training loss: 0.072 ... Validation loss: 0.175\r",
      "Progress: 59.4% ... Training loss: 0.081 ... Validation loss: 0.249\r",
      "Progress: 59.5% ... Training loss: 0.065 ... Validation loss: 0.176\r",
      "Progress: 59.5% ... Training loss: 0.062 ... Validation loss: 0.191\r",
      "Progress: 59.5% ... Training loss: 0.062 ... Validation loss: 0.188\r",
      "Progress: 59.5% ... Training loss: 0.063 ... Validation loss: 0.180\r",
      "Progress: 59.5% ... Training loss: 0.061 ... Validation loss: 0.191\r",
      "Progress: 59.5% ... Training loss: 0.062 ... Validation loss: 0.173\r",
      "Progress: 59.5% ... Training loss: 0.065 ... Validation loss: 0.208\r",
      "Progress: 59.5% ... Training loss: 0.065 ... Validation loss: 0.175\r",
      "Progress: 59.5% ... Training loss: 0.062 ... Validation loss: 0.184\r",
      "Progress: 59.6% ... Training loss: 0.068 ... Validation loss: 0.216\r",
      "Progress: 59.6% ... Training loss: 0.069 ... Validation loss: 0.170\r",
      "Progress: 59.6% ... Training loss: 0.070 ... Validation loss: 0.241\r",
      "Progress: 59.6% ... Training loss: 0.062 ... Validation loss: 0.210\r",
      "Progress: 59.6% ... Training loss: 0.066 ... Validation loss: 0.219\r",
      "Progress: 59.6% ... Training loss: 0.064 ... Validation loss: 0.199\r",
      "Progress: 59.6% ... Training loss: 0.062 ... Validation loss: 0.189\r",
      "Progress: 59.6% ... Training loss: 0.064 ... Validation loss: 0.216\r",
      "Progress: 59.7% ... Training loss: 0.061 ... Validation loss: 0.192\r",
      "Progress: 59.7% ... Training loss: 0.061 ... Validation loss: 0.188\r",
      "Progress: 59.7% ... Training loss: 0.064 ... Validation loss: 0.199\r",
      "Progress: 59.7% ... Training loss: 0.062 ... Validation loss: 0.193\r",
      "Progress: 59.7% ... Training loss: 0.063 ... Validation loss: 0.184\r",
      "Progress: 59.7% ... Training loss: 0.064 ... Validation loss: 0.191\r",
      "Progress: 59.7% ... Training loss: 0.064 ... Validation loss: 0.180\r",
      "Progress: 59.8% ... Training loss: 0.062 ... Validation loss: 0.193\r",
      "Progress: 59.8% ... Training loss: 0.065 ... Validation loss: 0.209\r",
      "Progress: 59.8% ... Training loss: 0.064 ... Validation loss: 0.196\r",
      "Progress: 59.8% ... Training loss: 0.064 ... Validation loss: 0.188\r",
      "Progress: 59.8% ... Training loss: 0.063 ... Validation loss: 0.173\r",
      "Progress: 59.8% ... Training loss: 0.061 ... Validation loss: 0.191\r",
      "Progress: 59.8% ... Training loss: 0.062 ... Validation loss: 0.176\r",
      "Progress: 59.8% ... Training loss: 0.064 ... Validation loss: 0.177\r",
      "Progress: 59.9% ... Training loss: 0.063 ... Validation loss: 0.210\r",
      "Progress: 59.9% ... Training loss: 0.063 ... Validation loss: 0.202\r",
      "Progress: 59.9% ... Training loss: 0.062 ... Validation loss: 0.183\r",
      "Progress: 59.9% ... Training loss: 0.064 ... Validation loss: 0.210\r",
      "Progress: 59.9% ... Training loss: 0.061 ... Validation loss: 0.190\r",
      "Progress: 59.9% ... Training loss: 0.062 ... Validation loss: 0.181\r",
      "Progress: 59.9% ... Training loss: 0.061 ... Validation loss: 0.185\r",
      "Progress: 59.9% ... Training loss: 0.062 ... Validation loss: 0.168\r",
      "Progress: 60.0% ... Training loss: 0.067 ... Validation loss: 0.183\r",
      "Progress: 60.0% ... Training loss: 0.064 ... Validation loss: 0.159\r",
      "Progress: 60.0% ... Training loss: 0.065 ... Validation loss: 0.190\r",
      "Progress: 60.0% ... Training loss: 0.066 ... Validation loss: 0.164\r",
      "Progress: 60.0% ... Training loss: 0.061 ... Validation loss: 0.186\r",
      "Progress: 60.0% ... Training loss: 0.077 ... Validation loss: 0.161\r",
      "Progress: 60.0% ... Training loss: 0.075 ... Validation loss: 0.208\r",
      "Progress: 60.0% ... Training loss: 0.070 ... Validation loss: 0.157\r",
      "Progress: 60.0% ... Training loss: 0.074 ... Validation loss: 0.203\r",
      "Progress: 60.1% ... Training loss: 0.072 ... Validation loss: 0.156\r",
      "Progress: 60.1% ... Training loss: 0.076 ... Validation loss: 0.229\r",
      "Progress: 60.1% ... Training loss: 0.070 ... Validation loss: 0.165\r",
      "Progress: 60.1% ... Training loss: 0.064 ... Validation loss: 0.212\r",
      "Progress: 60.1% ... Training loss: 0.065 ... Validation loss: 0.171\r",
      "Progress: 60.1% ... Training loss: 0.072 ... Validation loss: 0.236\r",
      "Progress: 60.1% ... Training loss: 0.075 ... Validation loss: 0.162\r",
      "Progress: 60.1% ... Training loss: 0.077 ... Validation loss: 0.223\r",
      "Progress: 60.2% ... Training loss: 0.064 ... Validation loss: 0.175\r",
      "Progress: 60.2% ... Training loss: 0.062 ... Validation loss: 0.191\r",
      "Progress: 60.2% ... Training loss: 0.061 ... Validation loss: 0.191\r",
      "Progress: 60.2% ... Training loss: 0.066 ... Validation loss: 0.215\r",
      "Progress: 60.2% ... Training loss: 0.062 ... Validation loss: 0.193\r",
      "Progress: 60.2% ... Training loss: 0.062 ... Validation loss: 0.217\r",
      "Progress: 60.2% ... Training loss: 0.061 ... Validation loss: 0.212\r",
      "Progress: 60.2% ... Training loss: 0.066 ... Validation loss: 0.232\r",
      "Progress: 60.3% ... Training loss: 0.072 ... Validation loss: 0.190\r",
      "Progress: 60.3% ... Training loss: 0.085 ... Validation loss: 0.287\r",
      "Progress: 60.3% ... Training loss: 0.076 ... Validation loss: 0.173\r",
      "Progress: 60.3% ... Training loss: 0.065 ... Validation loss: 0.216\r",
      "Progress: 60.3% ... Training loss: 0.072 ... Validation loss: 0.179\r",
      "Progress: 60.3% ... Training loss: 0.092 ... Validation loss: 0.260\r",
      "Progress: 60.3% ... Training loss: 0.121 ... Validation loss: 0.160\r",
      "Progress: 60.4% ... Training loss: 0.101 ... Validation loss: 0.260\r",
      "Progress: 60.4% ... Training loss: 0.089 ... Validation loss: 0.160\r",
      "Progress: 60.4% ... Training loss: 0.072 ... Validation loss: 0.218\r",
      "Progress: 60.4% ... Training loss: 0.071 ... Validation loss: 0.167\r",
      "Progress: 60.4% ... Training loss: 0.075 ... Validation loss: 0.219\r",
      "Progress: 60.4% ... Training loss: 0.073 ... Validation loss: 0.165\r",
      "Progress: 60.4% ... Training loss: 0.065 ... Validation loss: 0.214\r",
      "Progress: 60.4% ... Training loss: 0.063 ... Validation loss: 0.195\r",
      "Progress: 60.5% ... Training loss: 0.063 ... Validation loss: 0.167\r",
      "Progress: 60.5% ... Training loss: 0.060 ... Validation loss: 0.181\r",
      "Progress: 60.5% ... Training loss: 0.061 ... Validation loss: 0.171\r",
      "Progress: 60.5% ... Training loss: 0.061 ... Validation loss: 0.182\r",
      "Progress: 60.5% ... Training loss: 0.064 ... Validation loss: 0.196\r",
      "Progress: 60.5% ... Training loss: 0.065 ... Validation loss: 0.165\r",
      "Progress: 60.5% ... Training loss: 0.085 ... Validation loss: 0.205\r",
      "Progress: 60.5% ... Training loss: 0.079 ... Validation loss: 0.155\r",
      "Progress: 60.5% ... Training loss: 0.066 ... Validation loss: 0.214\r",
      "Progress: 60.6% ... Training loss: 0.067 ... Validation loss: 0.165\r",
      "Progress: 60.6% ... Training loss: 0.064 ... Validation loss: 0.204\r",
      "Progress: 60.6% ... Training loss: 0.060 ... Validation loss: 0.197\r",
      "Progress: 60.6% ... Training loss: 0.061 ... Validation loss: 0.208\r",
      "Progress: 60.6% ... Training loss: 0.061 ... Validation loss: 0.199\r",
      "Progress: 60.6% ... Training loss: 0.061 ... Validation loss: 0.200\r",
      "Progress: 60.6% ... Training loss: 0.061 ... Validation loss: 0.192\r",
      "Progress: 60.6% ... Training loss: 0.065 ... Validation loss: 0.172\r",
      "Progress: 60.7% ... Training loss: 0.067 ... Validation loss: 0.204\r",
      "Progress: 60.7% ... Training loss: 0.061 ... Validation loss: 0.175\r",
      "Progress: 60.7% ... Training loss: 0.063 ... Validation loss: 0.199\r",
      "Progress: 60.7% ... Training loss: 0.062 ... Validation loss: 0.184\r",
      "Progress: 60.7% ... Training loss: 0.064 ... Validation loss: 0.210\r",
      "Progress: 60.7% ... Training loss: 0.071 ... Validation loss: 0.184\r",
      "Progress: 60.7% ... Training loss: 0.069 ... Validation loss: 0.218\r",
      "Progress: 60.8% ... Training loss: 0.072 ... Validation loss: 0.166\r",
      "Progress: 60.8% ... Training loss: 0.101 ... Validation loss: 0.254\r",
      "Progress: 60.8% ... Training loss: 0.138 ... Validation loss: 0.169\r",
      "Progress: 60.8% ... Training loss: 0.126 ... Validation loss: 0.298\r",
      "Progress: 60.8% ... Training loss: 0.098 ... Validation loss: 0.162\r",
      "Progress: 60.8% ... Training loss: 0.085 ... Validation loss: 0.246\r",
      "Progress: 60.8% ... Training loss: 0.082 ... Validation loss: 0.160\r",
      "Progress: 60.8% ... Training loss: 0.066 ... Validation loss: 0.189\r",
      "Progress: 60.9% ... Training loss: 0.067 ... Validation loss: 0.173\r",
      "Progress: 60.9% ... Training loss: 0.064 ... Validation loss: 0.179\r",
      "Progress: 60.9% ... Training loss: 0.062 ... Validation loss: 0.168\r",
      "Progress: 60.9% ... Training loss: 0.079 ... Validation loss: 0.226\r",
      "Progress: 60.9% ... Training loss: 0.079 ... Validation loss: 0.157\r",
      "Progress: 60.9% ... Training loss: 0.063 ... Validation loss: 0.197\r",
      "Progress: 60.9% ... Training loss: 0.062 ... Validation loss: 0.189\r",
      "Progress: 60.9% ... Training loss: 0.063 ... Validation loss: 0.203\r",
      "Progress: 61.0% ... Training loss: 0.062 ... Validation loss: 0.182\r",
      "Progress: 61.0% ... Training loss: 0.061 ... Validation loss: 0.182\r",
      "Progress: 61.0% ... Training loss: 0.061 ... Validation loss: 0.183\r",
      "Progress: 61.0% ... Training loss: 0.064 ... Validation loss: 0.188\r",
      "Progress: 61.0% ... Training loss: 0.062 ... Validation loss: 0.189\r",
      "Progress: 61.0% ... Training loss: 0.062 ... Validation loss: 0.194\r",
      "Progress: 61.0% ... Training loss: 0.064 ... Validation loss: 0.183\r",
      "Progress: 61.0% ... Training loss: 0.063 ... Validation loss: 0.201\r",
      "Progress: 61.0% ... Training loss: 0.062 ... Validation loss: 0.190\r",
      "Progress: 61.1% ... Training loss: 0.062 ... Validation loss: 0.201\r",
      "Progress: 61.1% ... Training loss: 0.064 ... Validation loss: 0.193\r",
      "Progress: 61.1% ... Training loss: 0.070 ... Validation loss: 0.228\r",
      "Progress: 61.1% ... Training loss: 0.066 ... Validation loss: 0.180\r",
      "Progress: 61.1% ... Training loss: 0.069 ... Validation loss: 0.211\r",
      "Progress: 61.1% ... Training loss: 0.066 ... Validation loss: 0.159\r",
      "Progress: 61.1% ... Training loss: 0.065 ... Validation loss: 0.189\r",
      "Progress: 61.1% ... Training loss: 0.062 ... Validation loss: 0.166\r",
      "Progress: 61.2% ... Training loss: 0.064 ... Validation loss: 0.178\r",
      "Progress: 61.2% ... Training loss: 0.062 ... Validation loss: 0.169\r",
      "Progress: 61.2% ... Training loss: 0.062 ... Validation loss: 0.197\r",
      "Progress: 61.2% ... Training loss: 0.062 ... Validation loss: 0.188\r",
      "Progress: 61.2% ... Training loss: 0.062 ... Validation loss: 0.200\r",
      "Progress: 61.2% ... Training loss: 0.060 ... Validation loss: 0.180\r",
      "Progress: 61.2% ... Training loss: 0.061 ... Validation loss: 0.178\r",
      "Progress: 61.2% ... Training loss: 0.062 ... Validation loss: 0.184\r",
      "Progress: 61.3% ... Training loss: 0.064 ... Validation loss: 0.218\r",
      "Progress: 61.3% ... Training loss: 0.061 ... Validation loss: 0.196\r",
      "Progress: 61.3% ... Training loss: 0.063 ... Validation loss: 0.210\r",
      "Progress: 61.3% ... Training loss: 0.061 ... Validation loss: 0.181\r",
      "Progress: 61.3% ... Training loss: 0.068 ... Validation loss: 0.169\r",
      "Progress: 61.3% ... Training loss: 0.067 ... Validation loss: 0.214\r",
      "Progress: 61.3% ... Training loss: 0.061 ... Validation loss: 0.177\r",
      "Progress: 61.4% ... Training loss: 0.062 ... Validation loss: 0.192\r",
      "Progress: 61.4% ... Training loss: 0.064 ... Validation loss: 0.167\r",
      "Progress: 61.4% ... Training loss: 0.064 ... Validation loss: 0.198\r",
      "Progress: 61.4% ... Training loss: 0.084 ... Validation loss: 0.164\r",
      "Progress: 61.4% ... Training loss: 0.069 ... Validation loss: 0.223\r",
      "Progress: 61.4% ... Training loss: 0.061 ... Validation loss: 0.183\r",
      "Progress: 61.4% ... Training loss: 0.061 ... Validation loss: 0.183\r",
      "Progress: 61.4% ... Training loss: 0.061 ... Validation loss: 0.186\r",
      "Progress: 61.5% ... Training loss: 0.068 ... Validation loss: 0.200\r",
      "Progress: 61.5% ... Training loss: 0.070 ... Validation loss: 0.166\r",
      "Progress: 61.5% ... Training loss: 0.081 ... Validation loss: 0.239\r",
      "Progress: 61.5% ... Training loss: 0.102 ... Validation loss: 0.172\r",
      "Progress: 61.5% ... Training loss: 0.110 ... Validation loss: 0.283\r",
      "Progress: 61.5% ... Training loss: 0.068 ... Validation loss: 0.161\r",
      "Progress: 61.5% ... Training loss: 0.074 ... Validation loss: 0.215\r",
      "Progress: 61.5% ... Training loss: 0.068 ... Validation loss: 0.158\r",
      "Progress: 61.5% ... Training loss: 0.065 ... Validation loss: 0.194\r",
      "Progress: 61.6% ... Training loss: 0.071 ... Validation loss: 0.164\r",
      "Progress: 61.6% ... Training loss: 0.087 ... Validation loss: 0.224\r",
      "Progress: 61.6% ... Training loss: 0.072 ... Validation loss: 0.164\r",
      "Progress: 61.6% ... Training loss: 0.067 ... Validation loss: 0.203\r",
      "Progress: 61.6% ... Training loss: 0.073 ... Validation loss: 0.167\r",
      "Progress: 61.6% ... Training loss: 0.072 ... Validation loss: 0.203\r",
      "Progress: 61.6% ... Training loss: 0.073 ... Validation loss: 0.167\r",
      "Progress: 61.6% ... Training loss: 0.064 ... Validation loss: 0.193\r",
      "Progress: 61.7% ... Training loss: 0.065 ... Validation loss: 0.169\r",
      "Progress: 61.7% ... Training loss: 0.066 ... Validation loss: 0.191\r",
      "Progress: 61.7% ... Training loss: 0.062 ... Validation loss: 0.182\r",
      "Progress: 61.7% ... Training loss: 0.062 ... Validation loss: 0.183\r",
      "Progress: 61.7% ... Training loss: 0.062 ... Validation loss: 0.179\r",
      "Progress: 61.7% ... Training loss: 0.085 ... Validation loss: 0.170\r",
      "Progress: 61.7% ... Training loss: 0.068 ... Validation loss: 0.211\r",
      "Progress: 61.8% ... Training loss: 0.063 ... Validation loss: 0.171\r",
      "Progress: 61.8% ... Training loss: 0.063 ... Validation loss: 0.201\r",
      "Progress: 61.8% ... Training loss: 0.063 ... Validation loss: 0.180\r",
      "Progress: 61.8% ... Training loss: 0.063 ... Validation loss: 0.204\r",
      "Progress: 61.8% ... Training loss: 0.067 ... Validation loss: 0.172\r",
      "Progress: 61.8% ... Training loss: 0.068 ... Validation loss: 0.220\r",
      "Progress: 61.8% ... Training loss: 0.061 ... Validation loss: 0.181\r",
      "Progress: 61.8% ... Training loss: 0.061 ... Validation loss: 0.195\r",
      "Progress: 61.9% ... Training loss: 0.063 ... Validation loss: 0.214\r",
      "Progress: 61.9% ... Training loss: 0.063 ... Validation loss: 0.181\r",
      "Progress: 61.9% ... Training loss: 0.060 ... Validation loss: 0.185\r",
      "Progress: 61.9% ... Training loss: 0.064 ... Validation loss: 0.187\r",
      "Progress: 61.9% ... Training loss: 0.061 ... Validation loss: 0.192\r",
      "Progress: 61.9% ... Training loss: 0.067 ... Validation loss: 0.188\r",
      "Progress: 61.9% ... Training loss: 0.061 ... Validation loss: 0.197\r",
      "Progress: 61.9% ... Training loss: 0.061 ... Validation loss: 0.203\r",
      "Progress: 62.0% ... Training loss: 0.062 ... Validation loss: 0.183\r",
      "Progress: 62.0% ... Training loss: 0.062 ... Validation loss: 0.190\r",
      "Progress: 62.0% ... Training loss: 0.070 ... Validation loss: 0.176\r",
      "Progress: 62.0% ... Training loss: 0.076 ... Validation loss: 0.235\r",
      "Progress: 62.0% ... Training loss: 0.068 ... Validation loss: 0.175\r",
      "Progress: 62.0% ... Training loss: 0.080 ... Validation loss: 0.244\r",
      "Progress: 62.0% ... Training loss: 0.062 ... Validation loss: 0.188\r",
      "Progress: 62.0% ... Training loss: 0.062 ... Validation loss: 0.187\r",
      "Progress: 62.0% ... Training loss: 0.067 ... Validation loss: 0.212\r",
      "Progress: 62.1% ... Training loss: 0.085 ... Validation loss: 0.161\r",
      "Progress: 62.1% ... Training loss: 0.083 ... Validation loss: 0.240\r",
      "Progress: 62.1% ... Training loss: 0.092 ... Validation loss: 0.169\r",
      "Progress: 62.1% ... Training loss: 0.078 ... Validation loss: 0.243\r",
      "Progress: 62.1% ... Training loss: 0.076 ... Validation loss: 0.175\r",
      "Progress: 62.1% ... Training loss: 0.077 ... Validation loss: 0.232\r",
      "Progress: 62.1% ... Training loss: 0.078 ... Validation loss: 0.167\r",
      "Progress: 62.1% ... Training loss: 0.077 ... Validation loss: 0.236\r",
      "Progress: 62.2% ... Training loss: 0.068 ... Validation loss: 0.175\r",
      "Progress: 62.2% ... Training loss: 0.064 ... Validation loss: 0.226\r",
      "Progress: 62.2% ... Training loss: 0.067 ... Validation loss: 0.187\r",
      "Progress: 62.2% ... Training loss: 0.082 ... Validation loss: 0.256\r",
      "Progress: 62.2% ... Training loss: 0.072 ... Validation loss: 0.170\r",
      "Progress: 62.2% ... Training loss: 0.089 ... Validation loss: 0.253\r",
      "Progress: 62.2% ... Training loss: 0.092 ... Validation loss: 0.160\r",
      "Progress: 62.2% ... Training loss: 0.080 ... Validation loss: 0.219\r",
      "Progress: 62.3% ... Training loss: 0.073 ... Validation loss: 0.168\r",
      "Progress: 62.3% ... Training loss: 0.060 ... Validation loss: 0.187\r",
      "Progress: 62.3% ... Training loss: 0.067 ... Validation loss: 0.165\r",
      "Progress: 62.3% ... Training loss: 0.063 ... Validation loss: 0.180\r",
      "Progress: 62.3% ... Training loss: 0.066 ... Validation loss: 0.169\r",
      "Progress: 62.3% ... Training loss: 0.061 ... Validation loss: 0.187\r",
      "Progress: 62.3% ... Training loss: 0.061 ... Validation loss: 0.186\r",
      "Progress: 62.4% ... Training loss: 0.061 ... Validation loss: 0.171\r",
      "Progress: 62.4% ... Training loss: 0.061 ... Validation loss: 0.162\r",
      "Progress: 62.4% ... Training loss: 0.061 ... Validation loss: 0.174\r",
      "Progress: 62.4% ... Training loss: 0.065 ... Validation loss: 0.171\r",
      "Progress: 62.4% ... Training loss: 0.061 ... Validation loss: 0.185\r",
      "Progress: 62.4% ... Training loss: 0.062 ... Validation loss: 0.181\r",
      "Progress: 62.4% ... Training loss: 0.064 ... Validation loss: 0.167\r",
      "Progress: 62.4% ... Training loss: 0.077 ... Validation loss: 0.204\r",
      "Progress: 62.5% ... Training loss: 0.062 ... Validation loss: 0.171\r",
      "Progress: 62.5% ... Training loss: 0.064 ... Validation loss: 0.190\r",
      "Progress: 62.5% ... Training loss: 0.075 ... Validation loss: 0.158\r",
      "Progress: 62.5% ... Training loss: 0.069 ... Validation loss: 0.194\r",
      "Progress: 62.5% ... Training loss: 0.062 ... Validation loss: 0.161\r",
      "Progress: 62.5% ... Training loss: 0.075 ... Validation loss: 0.192\r",
      "Progress: 62.5% ... Training loss: 0.065 ... Validation loss: 0.161\r",
      "Progress: 62.5% ... Training loss: 0.062 ... Validation loss: 0.187\r",
      "Progress: 62.5% ... Training loss: 0.060 ... Validation loss: 0.180\r",
      "Progress: 62.6% ... Training loss: 0.061 ... Validation loss: 0.171\r",
      "Progress: 62.6% ... Training loss: 0.062 ... Validation loss: 0.190\r",
      "Progress: 62.6% ... Training loss: 0.066 ... Validation loss: 0.167\r",
      "Progress: 62.6% ... Training loss: 0.063 ... Validation loss: 0.179\r",
      "Progress: 62.6% ... Training loss: 0.063 ... Validation loss: 0.171\r",
      "Progress: 62.6% ... Training loss: 0.068 ... Validation loss: 0.200\r",
      "Progress: 62.6% ... Training loss: 0.078 ... Validation loss: 0.160\r",
      "Progress: 62.6% ... Training loss: 0.070 ... Validation loss: 0.208\r",
      "Progress: 62.7% ... Training loss: 0.083 ... Validation loss: 0.161\r",
      "Progress: 62.7% ... Training loss: 0.079 ... Validation loss: 0.216\r",
      "Progress: 62.7% ... Training loss: 0.066 ... Validation loss: 0.162\r",
      "Progress: 62.7% ... Training loss: 0.062 ... Validation loss: 0.174\r",
      "Progress: 62.7% ... Training loss: 0.063 ... Validation loss: 0.169\r",
      "Progress: 62.7% ... Training loss: 0.066 ... Validation loss: 0.199\r",
      "Progress: 62.7% ... Training loss: 0.062 ... Validation loss: 0.170\r",
      "Progress: 62.8% ... Training loss: 0.061 ... Validation loss: 0.180\r",
      "Progress: 62.8% ... Training loss: 0.061 ... Validation loss: 0.177\r",
      "Progress: 62.8% ... Training loss: 0.060 ... Validation loss: 0.174\r",
      "Progress: 62.8% ... Training loss: 0.065 ... Validation loss: 0.201\r",
      "Progress: 62.8% ... Training loss: 0.061 ... Validation loss: 0.165\r",
      "Progress: 62.8% ... Training loss: 0.060 ... Validation loss: 0.175\r",
      "Progress: 62.8% ... Training loss: 0.061 ... Validation loss: 0.164\r",
      "Progress: 62.8% ... Training loss: 0.066 ... Validation loss: 0.148\r",
      "Progress: 62.9% ... Training loss: 0.062 ... Validation loss: 0.169\r",
      "Progress: 62.9% ... Training loss: 0.068 ... Validation loss: 0.149\r",
      "Progress: 62.9% ... Training loss: 0.065 ... Validation loss: 0.169\r",
      "Progress: 62.9% ... Training loss: 0.064 ... Validation loss: 0.161\r",
      "Progress: 62.9% ... Training loss: 0.065 ... Validation loss: 0.195\r",
      "Progress: 62.9% ... Training loss: 0.065 ... Validation loss: 0.169\r",
      "Progress: 62.9% ... Training loss: 0.061 ... Validation loss: 0.181\r",
      "Progress: 62.9% ... Training loss: 0.062 ... Validation loss: 0.165\r",
      "Progress: 63.0% ... Training loss: 0.067 ... Validation loss: 0.189\r",
      "Progress: 63.0% ... Training loss: 0.083 ... Validation loss: 0.151\r",
      "Progress: 63.0% ... Training loss: 0.082 ... Validation loss: 0.214\r",
      "Progress: 63.0% ... Training loss: 0.115 ... Validation loss: 0.155\r",
      "Progress: 63.0% ... Training loss: 0.097 ... Validation loss: 0.228\r",
      "Progress: 63.0% ... Training loss: 0.106 ... Validation loss: 0.155\r",
      "Progress: 63.0% ... Training loss: 0.088 ... Validation loss: 0.210\r",
      "Progress: 63.0% ... Training loss: 0.098 ... Validation loss: 0.163\r",
      "Progress: 63.0% ... Training loss: 0.080 ... Validation loss: 0.212\r",
      "Progress: 63.1% ... Training loss: 0.077 ... Validation loss: 0.154\r",
      "Progress: 63.1% ... Training loss: 0.076 ... Validation loss: 0.207\r",
      "Progress: 63.1% ... Training loss: 0.070 ... Validation loss: 0.156\r",
      "Progress: 63.1% ... Training loss: 0.061 ... Validation loss: 0.170\r",
      "Progress: 63.1% ... Training loss: 0.065 ... Validation loss: 0.189\r",
      "Progress: 63.1% ... Training loss: 0.064 ... Validation loss: 0.162\r",
      "Progress: 63.1% ... Training loss: 0.060 ... Validation loss: 0.165\r",
      "Progress: 63.1% ... Training loss: 0.071 ... Validation loss: 0.154\r",
      "Progress: 63.2% ... Training loss: 0.069 ... Validation loss: 0.189\r",
      "Progress: 63.2% ... Training loss: 0.074 ... Validation loss: 0.150\r",
      "Progress: 63.2% ... Training loss: 0.072 ... Validation loss: 0.199\r",
      "Progress: 63.2% ... Training loss: 0.063 ... Validation loss: 0.157\r",
      "Progress: 63.2% ... Training loss: 0.063 ... Validation loss: 0.166\r",
      "Progress: 63.2% ... Training loss: 0.063 ... Validation loss: 0.188\r",
      "Progress: 63.2% ... Training loss: 0.061 ... Validation loss: 0.182\r",
      "Progress: 63.2% ... Training loss: 0.065 ... Validation loss: 0.161\r",
      "Progress: 63.3% ... Training loss: 0.063 ... Validation loss: 0.183\r",
      "Progress: 63.3% ... Training loss: 0.060 ... Validation loss: 0.177\r",
      "Progress: 63.3% ... Training loss: 0.062 ... Validation loss: 0.152\r",
      "Progress: 63.3% ... Training loss: 0.068 ... Validation loss: 0.173\r",
      "Progress: 63.3% ... Training loss: 0.070 ... Validation loss: 0.150\r",
      "Progress: 63.3% ... Training loss: 0.066 ... Validation loss: 0.175\r",
      "Progress: 63.3% ... Training loss: 0.064 ... Validation loss: 0.151\r",
      "Progress: 63.4% ... Training loss: 0.066 ... Validation loss: 0.182\r",
      "Progress: 63.4% ... Training loss: 0.061 ... Validation loss: 0.162\r",
      "Progress: 63.4% ... Training loss: 0.063 ... Validation loss: 0.170\r",
      "Progress: 63.4% ... Training loss: 0.060 ... Validation loss: 0.160\r",
      "Progress: 63.4% ... Training loss: 0.060 ... Validation loss: 0.165\r",
      "Progress: 63.4% ... Training loss: 0.061 ... Validation loss: 0.181\r",
      "Progress: 63.4% ... Training loss: 0.069 ... Validation loss: 0.166\r",
      "Progress: 63.4% ... Training loss: 0.063 ... Validation loss: 0.196\r",
      "Progress: 63.5% ... Training loss: 0.061 ... Validation loss: 0.170\r",
      "Progress: 63.5% ... Training loss: 0.061 ... Validation loss: 0.184\r",
      "Progress: 63.5% ... Training loss: 0.069 ... Validation loss: 0.210\r",
      "Progress: 63.5% ... Training loss: 0.063 ... Validation loss: 0.169\r",
      "Progress: 63.5% ... Training loss: 0.064 ... Validation loss: 0.200\r",
      "Progress: 63.5% ... Training loss: 0.061 ... Validation loss: 0.190\r",
      "Progress: 63.5% ... Training loss: 0.060 ... Validation loss: 0.188\r",
      "Progress: 63.5% ... Training loss: 0.061 ... Validation loss: 0.183\r",
      "Progress: 63.5% ... Training loss: 0.068 ... Validation loss: 0.207\r",
      "Progress: 63.6% ... Training loss: 0.062 ... Validation loss: 0.187\r",
      "Progress: 63.6% ... Training loss: 0.068 ... Validation loss: 0.217\r",
      "Progress: 63.6% ... Training loss: 0.067 ... Validation loss: 0.164\r",
      "Progress: 63.6% ... Training loss: 0.068 ... Validation loss: 0.207\r",
      "Progress: 63.6% ... Training loss: 0.065 ... Validation loss: 0.162\r",
      "Progress: 63.6% ... Training loss: 0.071 ... Validation loss: 0.213\r",
      "Progress: 63.6% ... Training loss: 0.077 ... Validation loss: 0.162\r",
      "Progress: 63.6% ... Training loss: 0.083 ... Validation loss: 0.211\r",
      "Progress: 63.7% ... Training loss: 0.061 ... Validation loss: 0.164\r",
      "Progress: 63.7% ... Training loss: 0.060 ... Validation loss: 0.175\r",
      "Progress: 63.7% ... Training loss: 0.061 ... Validation loss: 0.170\r",
      "Progress: 63.7% ... Training loss: 0.061 ... Validation loss: 0.184\r",
      "Progress: 63.7% ... Training loss: 0.063 ... Validation loss: 0.185\r",
      "Progress: 63.7% ... Training loss: 0.064 ... Validation loss: 0.162\r",
      "Progress: 63.7% ... Training loss: 0.063 ... Validation loss: 0.180\r",
      "Progress: 63.8% ... Training loss: 0.069 ... Validation loss: 0.157\r",
      "Progress: 63.8% ... Training loss: 0.063 ... Validation loss: 0.180\r",
      "Progress: 63.8% ... Training loss: 0.062 ... Validation loss: 0.157\r",
      "Progress: 63.8% ... Training loss: 0.062 ... Validation loss: 0.170\r",
      "Progress: 63.8% ... Training loss: 0.061 ... Validation loss: 0.162\r",
      "Progress: 63.8% ... Training loss: 0.060 ... Validation loss: 0.176\r",
      "Progress: 63.8% ... Training loss: 0.062 ... Validation loss: 0.175\r",
      "Progress: 63.8% ... Training loss: 0.065 ... Validation loss: 0.171\r",
      "Progress: 63.9% ... Training loss: 0.073 ... Validation loss: 0.160\r",
      "Progress: 63.9% ... Training loss: 0.082 ... Validation loss: 0.216\r",
      "Progress: 63.9% ... Training loss: 0.118 ... Validation loss: 0.160\r",
      "Progress: 63.9% ... Training loss: 0.088 ... Validation loss: 0.230\r",
      "Progress: 63.9% ... Training loss: 0.093 ... Validation loss: 0.166\r",
      "Progress: 63.9% ... Training loss: 0.087 ... Validation loss: 0.238\r",
      "Progress: 63.9% ... Training loss: 0.081 ... Validation loss: 0.164\r",
      "Progress: 63.9% ... Training loss: 0.063 ... Validation loss: 0.182\r",
      "Progress: 64.0% ... Training loss: 0.066 ... Validation loss: 0.160\r",
      "Progress: 64.0% ... Training loss: 0.065 ... Validation loss: 0.178\r",
      "Progress: 64.0% ... Training loss: 0.061 ... Validation loss: 0.172\r",
      "Progress: 64.0% ... Training loss: 0.062 ... Validation loss: 0.185\r",
      "Progress: 64.0% ... Training loss: 0.062 ... Validation loss: 0.159\r",
      "Progress: 64.0% ... Training loss: 0.062 ... Validation loss: 0.194\r",
      "Progress: 64.0% ... Training loss: 0.066 ... Validation loss: 0.171\r",
      "Progress: 64.0% ... Training loss: 0.061 ... Validation loss: 0.189\r",
      "Progress: 64.0% ... Training loss: 0.061 ... Validation loss: 0.175\r",
      "Progress: 64.1% ... Training loss: 0.061 ... Validation loss: 0.172\r",
      "Progress: 64.1% ... Training loss: 0.061 ... Validation loss: 0.173\r",
      "Progress: 64.1% ... Training loss: 0.060 ... Validation loss: 0.174\r",
      "Progress: 64.1% ... Training loss: 0.067 ... Validation loss: 0.200\r",
      "Progress: 64.1% ... Training loss: 0.065 ... Validation loss: 0.166\r",
      "Progress: 64.1% ... Training loss: 0.071 ... Validation loss: 0.209\r",
      "Progress: 64.1% ... Training loss: 0.063 ... Validation loss: 0.172\r",
      "Progress: 64.2% ... Training loss: 0.077 ... Validation loss: 0.229\r",
      "Progress: 64.2% ... Training loss: 0.077 ... Validation loss: 0.162\r",
      "Progress: 64.2% ... Training loss: 0.071 ... Validation loss: 0.221\r",
      "Progress: 64.2% ... Training loss: 0.067 ... Validation loss: 0.166\r",
      "Progress: 64.2% ... Training loss: 0.063 ... Validation loss: 0.198\r",
      "Progress: 64.2% ... Training loss: 0.067 ... Validation loss: 0.175\r",
      "Progress: 64.2% ... Training loss: 0.064 ... Validation loss: 0.199\r",
      "Progress: 64.2% ... Training loss: 0.060 ... Validation loss: 0.183\r",
      "Progress: 64.2% ... Training loss: 0.063 ... Validation loss: 0.181\r",
      "Progress: 64.3% ... Training loss: 0.064 ... Validation loss: 0.188\r",
      "Progress: 64.3% ... Training loss: 0.060 ... Validation loss: 0.190\r",
      "Progress: 64.3% ... Training loss: 0.060 ... Validation loss: 0.203\r",
      "Progress: 64.3% ... Training loss: 0.060 ... Validation loss: 0.182\r",
      "Progress: 64.3% ... Training loss: 0.060 ... Validation loss: 0.182\r",
      "Progress: 64.3% ... Training loss: 0.060 ... Validation loss: 0.179\r",
      "Progress: 64.3% ... Training loss: 0.061 ... Validation loss: 0.191\r",
      "Progress: 64.3% ... Training loss: 0.073 ... Validation loss: 0.170\r",
      "Progress: 64.4% ... Training loss: 0.080 ... Validation loss: 0.232\r",
      "Progress: 64.4% ... Training loss: 0.065 ... Validation loss: 0.156\r",
      "Progress: 64.4% ... Training loss: 0.062 ... Validation loss: 0.180\r",
      "Progress: 64.4% ... Training loss: 0.061 ... Validation loss: 0.163\r",
      "Progress: 64.4% ... Training loss: 0.060 ... Validation loss: 0.180\r",
      "Progress: 64.4% ... Training loss: 0.062 ... Validation loss: 0.186\r",
      "Progress: 64.4% ... Training loss: 0.060 ... Validation loss: 0.179\r",
      "Progress: 64.5% ... Training loss: 0.065 ... Validation loss: 0.163\r",
      "Progress: 64.5% ... Training loss: 0.060 ... Validation loss: 0.188\r",
      "Progress: 64.5% ... Training loss: 0.061 ... Validation loss: 0.175\r",
      "Progress: 64.5% ... Training loss: 0.060 ... Validation loss: 0.165\r",
      "Progress: 64.5% ... Training loss: 0.060 ... Validation loss: 0.174\r",
      "Progress: 64.5% ... Training loss: 0.061 ... Validation loss: 0.180\r",
      "Progress: 64.5% ... Training loss: 0.061 ... Validation loss: 0.191\r",
      "Progress: 64.5% ... Training loss: 0.061 ... Validation loss: 0.180\r",
      "Progress: 64.5% ... Training loss: 0.062 ... Validation loss: 0.172\r",
      "Progress: 64.6% ... Training loss: 0.069 ... Validation loss: 0.156\r",
      "Progress: 64.6% ... Training loss: 0.075 ... Validation loss: 0.195\r",
      "Progress: 64.6% ... Training loss: 0.076 ... Validation loss: 0.158\r",
      "Progress: 64.6% ... Training loss: 0.084 ... Validation loss: 0.207\r",
      "Progress: 64.6% ... Training loss: 0.083 ... Validation loss: 0.145\r",
      "Progress: 64.6% ... Training loss: 0.091 ... Validation loss: 0.215\r",
      "Progress: 64.6% ... Training loss: 0.063 ... Validation loss: 0.153\r",
      "Progress: 64.7% ... Training loss: 0.061 ... Validation loss: 0.160\r",
      "Progress: 64.7% ... Training loss: 0.061 ... Validation loss: 0.167\r",
      "Progress: 64.7% ... Training loss: 0.063 ... Validation loss: 0.177\r",
      "Progress: 64.7% ... Training loss: 0.063 ... Validation loss: 0.175\r",
      "Progress: 64.7% ... Training loss: 0.063 ... Validation loss: 0.156\r",
      "Progress: 64.7% ... Training loss: 0.068 ... Validation loss: 0.195\r",
      "Progress: 64.7% ... Training loss: 0.063 ... Validation loss: 0.165\r",
      "Progress: 64.7% ... Training loss: 0.062 ... Validation loss: 0.164\r",
      "Progress: 64.8% ... Training loss: 0.067 ... Validation loss: 0.197\r",
      "Progress: 64.8% ... Training loss: 0.060 ... Validation loss: 0.178\r",
      "Progress: 64.8% ... Training loss: 0.061 ... Validation loss: 0.182\r",
      "Progress: 64.8% ... Training loss: 0.061 ... Validation loss: 0.166\r",
      "Progress: 64.8% ... Training loss: 0.060 ... Validation loss: 0.177\r",
      "Progress: 64.8% ... Training loss: 0.061 ... Validation loss: 0.164\r",
      "Progress: 64.8% ... Training loss: 0.060 ... Validation loss: 0.175\r",
      "Progress: 64.8% ... Training loss: 0.061 ... Validation loss: 0.160\r",
      "Progress: 64.8% ... Training loss: 0.061 ... Validation loss: 0.167\r",
      "Progress: 64.9% ... Training loss: 0.064 ... Validation loss: 0.170\r",
      "Progress: 64.9% ... Training loss: 0.060 ... Validation loss: 0.161\r",
      "Progress: 64.9% ... Training loss: 0.061 ... Validation loss: 0.156\r",
      "Progress: 64.9% ... Training loss: 0.063 ... Validation loss: 0.181\r",
      "Progress: 64.9% ... Training loss: 0.061 ... Validation loss: 0.171\r",
      "Progress: 64.9% ... Training loss: 0.062 ... Validation loss: 0.178\r",
      "Progress: 64.9% ... Training loss: 0.062 ... Validation loss: 0.160\r",
      "Progress: 65.0% ... Training loss: 0.063 ... Validation loss: 0.171\r",
      "Progress: 65.0% ... Training loss: 0.065 ... Validation loss: 0.156\r",
      "Progress: 65.0% ... Training loss: 0.063 ... Validation loss: 0.187\r",
      "Progress: 65.0% ... Training loss: 0.063 ... Validation loss: 0.155\r",
      "Progress: 65.0% ... Training loss: 0.065 ... Validation loss: 0.186\r",
      "Progress: 65.0% ... Training loss: 0.065 ... Validation loss: 0.165\r",
      "Progress: 65.0% ... Training loss: 0.061 ... Validation loss: 0.174\r",
      "Progress: 65.0% ... Training loss: 0.063 ... Validation loss: 0.179\r",
      "Progress: 65.0% ... Training loss: 0.059 ... Validation loss: 0.184\r",
      "Progress: 65.1% ... Training loss: 0.061 ... Validation loss: 0.176\r",
      "Progress: 65.1% ... Training loss: 0.060 ... Validation loss: 0.176\r",
      "Progress: 65.1% ... Training loss: 0.059 ... Validation loss: 0.172\r",
      "Progress: 65.1% ... Training loss: 0.069 ... Validation loss: 0.200\r",
      "Progress: 65.1% ... Training loss: 0.061 ... Validation loss: 0.171\r",
      "Progress: 65.1% ... Training loss: 0.060 ... Validation loss: 0.178\r",
      "Progress: 65.1% ... Training loss: 0.061 ... Validation loss: 0.178\r",
      "Progress: 65.2% ... Training loss: 0.061 ... Validation loss: 0.165\r",
      "Progress: 65.2% ... Training loss: 0.060 ... Validation loss: 0.170\r",
      "Progress: 65.2% ... Training loss: 0.062 ... Validation loss: 0.167\r",
      "Progress: 65.2% ... Training loss: 0.063 ... Validation loss: 0.185\r",
      "Progress: 65.2% ... Training loss: 0.062 ... Validation loss: 0.165\r",
      "Progress: 65.2% ... Training loss: 0.069 ... Validation loss: 0.208\r",
      "Progress: 65.2% ... Training loss: 0.063 ... Validation loss: 0.160\r",
      "Progress: 65.2% ... Training loss: 0.060 ... Validation loss: 0.178\r",
      "Progress: 65.2% ... Training loss: 0.063 ... Validation loss: 0.167\r",
      "Progress: 65.3% ... Training loss: 0.060 ... Validation loss: 0.187\r",
      "Progress: 65.3% ... Training loss: 0.064 ... Validation loss: 0.164\r",
      "Progress: 65.3% ... Training loss: 0.066 ... Validation loss: 0.198\r",
      "Progress: 65.3% ... Training loss: 0.067 ... Validation loss: 0.159\r",
      "Progress: 65.3% ... Training loss: 0.059 ... Validation loss: 0.193\r",
      "Progress: 65.3% ... Training loss: 0.065 ... Validation loss: 0.197\r",
      "Progress: 65.3% ... Training loss: 0.064 ... Validation loss: 0.154\r",
      "Progress: 65.3% ... Training loss: 0.060 ... Validation loss: 0.171\r",
      "Progress: 65.4% ... Training loss: 0.063 ... Validation loss: 0.181\r",
      "Progress: 65.4% ... Training loss: 0.060 ... Validation loss: 0.178\r",
      "Progress: 65.4% ... Training loss: 0.060 ... Validation loss: 0.178\r",
      "Progress: 65.4% ... Training loss: 0.060 ... Validation loss: 0.183\r",
      "Progress: 65.4% ... Training loss: 0.059 ... Validation loss: 0.181\r",
      "Progress: 65.4% ... Training loss: 0.059 ... Validation loss: 0.186\r",
      "Progress: 65.4% ... Training loss: 0.059 ... Validation loss: 0.184\r",
      "Progress: 65.5% ... Training loss: 0.063 ... Validation loss: 0.175\r",
      "Progress: 65.5% ... Training loss: 0.060 ... Validation loss: 0.174\r",
      "Progress: 65.5% ... Training loss: 0.064 ... Validation loss: 0.177\r",
      "Progress: 65.5% ... Training loss: 0.066 ... Validation loss: 0.156\r",
      "Progress: 65.5% ... Training loss: 0.060 ... Validation loss: 0.181\r",
      "Progress: 65.5% ... Training loss: 0.059 ... Validation loss: 0.174\r",
      "Progress: 65.5% ... Training loss: 0.060 ... Validation loss: 0.163\r",
      "Progress: 65.5% ... Training loss: 0.061 ... Validation loss: 0.174\r",
      "Progress: 65.5% ... Training loss: 0.061 ... Validation loss: 0.183\r",
      "Progress: 65.6% ... Training loss: 0.064 ... Validation loss: 0.193\r",
      "Progress: 65.6% ... Training loss: 0.062 ... Validation loss: 0.173\r",
      "Progress: 65.6% ... Training loss: 0.066 ... Validation loss: 0.162\r",
      "Progress: 65.6% ... Training loss: 0.060 ... Validation loss: 0.167\r",
      "Progress: 65.6% ... Training loss: 0.064 ... Validation loss: 0.158\r",
      "Progress: 65.6% ... Training loss: 0.064 ... Validation loss: 0.179\r",
      "Progress: 65.6% ... Training loss: 0.063 ... Validation loss: 0.158\r",
      "Progress: 65.7% ... Training loss: 0.062 ... Validation loss: 0.185\r",
      "Progress: 65.7% ... Training loss: 0.061 ... Validation loss: 0.166\r",
      "Progress: 65.7% ... Training loss: 0.060 ... Validation loss: 0.168\r",
      "Progress: 65.7% ... Training loss: 0.061 ... Validation loss: 0.182\r",
      "Progress: 65.7% ... Training loss: 0.060 ... Validation loss: 0.167\r",
      "Progress: 65.7% ... Training loss: 0.064 ... Validation loss: 0.163\r",
      "Progress: 65.7% ... Training loss: 0.060 ... Validation loss: 0.180\r",
      "Progress: 65.7% ... Training loss: 0.060 ... Validation loss: 0.167\r",
      "Progress: 65.8% ... Training loss: 0.061 ... Validation loss: 0.168\r",
      "Progress: 65.8% ... Training loss: 0.063 ... Validation loss: 0.201\r",
      "Progress: 65.8% ... Training loss: 0.066 ... Validation loss: 0.165\r",
      "Progress: 65.8% ... Training loss: 0.066 ... Validation loss: 0.204\r",
      "Progress: 65.8% ... Training loss: 0.061 ... Validation loss: 0.174\r",
      "Progress: 65.8% ... Training loss: 0.060 ... Validation loss: 0.194\r",
      "Progress: 65.8% ... Training loss: 0.063 ... Validation loss: 0.176\r",
      "Progress: 65.8% ... Training loss: 0.071 ... Validation loss: 0.224\r",
      "Progress: 65.8% ... Training loss: 0.060 ... Validation loss: 0.178\r",
      "Progress: 65.9% ... Training loss: 0.067 ... Validation loss: 0.163\r",
      "Progress: 65.9% ... Training loss: 0.065 ... Validation loss: 0.195\r",
      "Progress: 65.9% ... Training loss: 0.061 ... Validation loss: 0.172\r",
      "Progress: 65.9% ... Training loss: 0.060 ... Validation loss: 0.182\r",
      "Progress: 65.9% ... Training loss: 0.060 ... Validation loss: 0.177\r",
      "Progress: 65.9% ... Training loss: 0.065 ... Validation loss: 0.162\r",
      "Progress: 65.9% ... Training loss: 0.063 ... Validation loss: 0.204\r",
      "Progress: 66.0% ... Training loss: 0.064 ... Validation loss: 0.193\r",
      "Progress: 66.0% ... Training loss: 0.067 ... Validation loss: 0.166\r",
      "Progress: 66.0% ... Training loss: 0.071 ... Validation loss: 0.214\r",
      "Progress: 66.0% ... Training loss: 0.079 ... Validation loss: 0.160\r",
      "Progress: 66.0% ... Training loss: 0.069 ... Validation loss: 0.221\r",
      "Progress: 66.0% ... Training loss: 0.072 ... Validation loss: 0.156\r",
      "Progress: 66.0% ... Training loss: 0.066 ... Validation loss: 0.212\r",
      "Progress: 66.0% ... Training loss: 0.071 ... Validation loss: 0.163\r",
      "Progress: 66.0% ... Training loss: 0.064 ... Validation loss: 0.205\r",
      "Progress: 66.1% ... Training loss: 0.061 ... Validation loss: 0.168\r",
      "Progress: 66.1% ... Training loss: 0.070 ... Validation loss: 0.156\r",
      "Progress: 66.1% ... Training loss: 0.065 ... Validation loss: 0.183\r",
      "Progress: 66.1% ... Training loss: 0.063 ... Validation loss: 0.185\r",
      "Progress: 66.1% ... Training loss: 0.087 ... Validation loss: 0.156\r",
      "Progress: 66.1% ... Training loss: 0.068 ... Validation loss: 0.215\r",
      "Progress: 66.1% ... Training loss: 0.070 ... Validation loss: 0.159\r",
      "Progress: 66.2% ... Training loss: 0.062 ... Validation loss: 0.194\r",
      "Progress: 66.2% ... Training loss: 0.060 ... Validation loss: 0.176\r",
      "Progress: 66.2% ... Training loss: 0.064 ... Validation loss: 0.194\r",
      "Progress: 66.2% ... Training loss: 0.059 ... Validation loss: 0.173\r",
      "Progress: 66.2% ... Training loss: 0.064 ... Validation loss: 0.162\r",
      "Progress: 66.2% ... Training loss: 0.063 ... Validation loss: 0.202\r",
      "Progress: 66.2% ... Training loss: 0.059 ... Validation loss: 0.180\r",
      "Progress: 66.2% ... Training loss: 0.060 ... Validation loss: 0.187\r",
      "Progress: 66.2% ... Training loss: 0.060 ... Validation loss: 0.170\r",
      "Progress: 66.3% ... Training loss: 0.060 ... Validation loss: 0.168\r",
      "Progress: 66.3% ... Training loss: 0.061 ... Validation loss: 0.186\r",
      "Progress: 66.3% ... Training loss: 0.077 ... Validation loss: 0.157\r",
      "Progress: 66.3% ... Training loss: 0.072 ... Validation loss: 0.218\r",
      "Progress: 66.3% ... Training loss: 0.071 ... Validation loss: 0.150\r",
      "Progress: 66.3% ... Training loss: 0.061 ... Validation loss: 0.183\r",
      "Progress: 66.3% ... Training loss: 0.070 ... Validation loss: 0.159\r",
      "Progress: 66.3% ... Training loss: 0.061 ... Validation loss: 0.198\r",
      "Progress: 66.4% ... Training loss: 0.060 ... Validation loss: 0.171\r",
      "Progress: 66.4% ... Training loss: 0.068 ... Validation loss: 0.212\r",
      "Progress: 66.4% ... Training loss: 0.074 ... Validation loss: 0.163\r",
      "Progress: 66.4% ... Training loss: 0.074 ... Validation loss: 0.219\r",
      "Progress: 66.4% ... Training loss: 0.080 ... Validation loss: 0.150\r",
      "Progress: 66.4% ... Training loss: 0.071 ... Validation loss: 0.200\r",
      "Progress: 66.4% ... Training loss: 0.062 ... Validation loss: 0.158\r",
      "Progress: 66.5% ... Training loss: 0.062 ... Validation loss: 0.186\r",
      "Progress: 66.5% ... Training loss: 0.063 ... Validation loss: 0.157\r",
      "Progress: 66.5% ... Training loss: 0.068 ... Validation loss: 0.205\r",
      "Progress: 66.5% ... Training loss: 0.064 ... Validation loss: 0.170\r",
      "Progress: 66.5% ... Training loss: 0.060 ... Validation loss: 0.172\r",
      "Progress: 66.5% ... Training loss: 0.070 ... Validation loss: 0.193\r",
      "Progress: 66.5% ... Training loss: 0.085 ... Validation loss: 0.158\r",
      "Progress: 66.5% ... Training loss: 0.064 ... Validation loss: 0.184\r",
      "Progress: 66.5% ... Training loss: 0.067 ... Validation loss: 0.158\r",
      "Progress: 66.6% ... Training loss: 0.061 ... Validation loss: 0.185\r",
      "Progress: 66.6% ... Training loss: 0.063 ... Validation loss: 0.160\r",
      "Progress: 66.6% ... Training loss: 0.059 ... Validation loss: 0.179\r",
      "Progress: 66.6% ... Training loss: 0.061 ... Validation loss: 0.163\r",
      "Progress: 66.6% ... Training loss: 0.060 ... Validation loss: 0.187\r",
      "Progress: 66.6% ... Training loss: 0.064 ... Validation loss: 0.177\r",
      "Progress: 66.6% ... Training loss: 0.061 ... Validation loss: 0.186\r",
      "Progress: 66.7% ... Training loss: 0.059 ... Validation loss: 0.179\r",
      "Progress: 66.7% ... Training loss: 0.059 ... Validation loss: 0.172\r",
      "Progress: 66.7% ... Training loss: 0.060 ... Validation loss: 0.175\r",
      "Progress: 66.7% ... Training loss: 0.061 ... Validation loss: 0.171\r",
      "Progress: 66.7% ... Training loss: 0.065 ... Validation loss: 0.205\r",
      "Progress: 66.7% ... Training loss: 0.076 ... Validation loss: 0.163\r",
      "Progress: 66.7% ... Training loss: 0.066 ... Validation loss: 0.203\r",
      "Progress: 66.7% ... Training loss: 0.067 ... Validation loss: 0.159\r",
      "Progress: 66.8% ... Training loss: 0.068 ... Validation loss: 0.219\r",
      "Progress: 66.8% ... Training loss: 0.064 ... Validation loss: 0.173\r",
      "Progress: 66.8% ... Training loss: 0.060 ... Validation loss: 0.186\r",
      "Progress: 66.8% ... Training loss: 0.061 ... Validation loss: 0.164\r",
      "Progress: 66.8% ... Training loss: 0.062 ... Validation loss: 0.179\r",
      "Progress: 66.8% ... Training loss: 0.060 ... Validation loss: 0.171\r",
      "Progress: 66.8% ... Training loss: 0.061 ... Validation loss: 0.176\r",
      "Progress: 66.8% ... Training loss: 0.063 ... Validation loss: 0.196\r",
      "Progress: 66.8% ... Training loss: 0.074 ... Validation loss: 0.162\r",
      "Progress: 66.9% ... Training loss: 0.087 ... Validation loss: 0.238\r",
      "Progress: 66.9% ... Training loss: 0.097 ... Validation loss: 0.154\r",
      "Progress: 66.9% ... Training loss: 0.078 ... Validation loss: 0.226\r",
      "Progress: 66.9% ... Training loss: 0.067 ... Validation loss: 0.159\r",
      "Progress: 66.9% ... Training loss: 0.064 ... Validation loss: 0.197\r",
      "Progress: 66.9% ... Training loss: 0.060 ... Validation loss: 0.166\r",
      "Progress: 66.9% ... Training loss: 0.064 ... Validation loss: 0.196\r",
      "Progress: 67.0% ... Training loss: 0.059 ... Validation loss: 0.169\r",
      "Progress: 67.0% ... Training loss: 0.059 ... Validation loss: 0.186\r",
      "Progress: 67.0% ... Training loss: 0.061 ... Validation loss: 0.166\r",
      "Progress: 67.0% ... Training loss: 0.066 ... Validation loss: 0.199\r",
      "Progress: 67.0% ... Training loss: 0.061 ... Validation loss: 0.161\r",
      "Progress: 67.0% ... Training loss: 0.069 ... Validation loss: 0.180\r",
      "Progress: 67.0% ... Training loss: 0.065 ... Validation loss: 0.148\r",
      "Progress: 67.0% ... Training loss: 0.059 ... Validation loss: 0.171\r",
      "Progress: 67.0% ... Training loss: 0.059 ... Validation loss: 0.165\r",
      "Progress: 67.1% ... Training loss: 0.059 ... Validation loss: 0.175\r",
      "Progress: 67.1% ... Training loss: 0.059 ... Validation loss: 0.174\r",
      "Progress: 67.1% ... Training loss: 0.059 ... Validation loss: 0.181\r",
      "Progress: 67.1% ... Training loss: 0.059 ... Validation loss: 0.183\r",
      "Progress: 67.1% ... Training loss: 0.060 ... Validation loss: 0.194\r",
      "Progress: 67.1% ... Training loss: 0.060 ... Validation loss: 0.168\r",
      "Progress: 67.1% ... Training loss: 0.059 ... Validation loss: 0.178\r",
      "Progress: 67.2% ... Training loss: 0.062 ... Validation loss: 0.193\r",
      "Progress: 67.2% ... Training loss: 0.061 ... Validation loss: 0.166\r",
      "Progress: 67.2% ... Training loss: 0.059 ... Validation loss: 0.178\r",
      "Progress: 67.2% ... Training loss: 0.062 ... Validation loss: 0.205\r",
      "Progress: 67.2% ... Training loss: 0.065 ... Validation loss: 0.163\r",
      "Progress: 67.2% ... Training loss: 0.070 ... Validation loss: 0.215\r",
      "Progress: 67.2% ... Training loss: 0.079 ... Validation loss: 0.158\r",
      "Progress: 67.2% ... Training loss: 0.086 ... Validation loss: 0.244\r",
      "Progress: 67.2% ... Training loss: 0.093 ... Validation loss: 0.160\r",
      "Progress: 67.3% ... Training loss: 0.084 ... Validation loss: 0.256\r",
      "Progress: 67.3% ... Training loss: 0.076 ... Validation loss: 0.166\r",
      "Progress: 67.3% ... Training loss: 0.063 ... Validation loss: 0.213\r",
      "Progress: 67.3% ... Training loss: 0.067 ... Validation loss: 0.170\r",
      "Progress: 67.3% ... Training loss: 0.071 ... Validation loss: 0.229\r",
      "Progress: 67.3% ... Training loss: 0.063 ... Validation loss: 0.177\r",
      "Progress: 67.3% ... Training loss: 0.074 ... Validation loss: 0.212\r",
      "Progress: 67.3% ... Training loss: 0.084 ... Validation loss: 0.164\r",
      "Progress: 67.4% ... Training loss: 0.062 ... Validation loss: 0.200\r",
      "Progress: 67.4% ... Training loss: 0.059 ... Validation loss: 0.187\r",
      "Progress: 67.4% ... Training loss: 0.062 ... Validation loss: 0.195\r",
      "Progress: 67.4% ... Training loss: 0.062 ... Validation loss: 0.194\r",
      "Progress: 67.4% ... Training loss: 0.060 ... Validation loss: 0.207\r",
      "Progress: 67.4% ... Training loss: 0.060 ... Validation loss: 0.204\r",
      "Progress: 67.4% ... Training loss: 0.060 ... Validation loss: 0.193\r",
      "Progress: 67.5% ... Training loss: 0.068 ... Validation loss: 0.231\r",
      "Progress: 67.5% ... Training loss: 0.064 ... Validation loss: 0.173\r",
      "Progress: 67.5% ... Training loss: 0.064 ... Validation loss: 0.205\r",
      "Progress: 67.5% ... Training loss: 0.075 ... Validation loss: 0.164\r",
      "Progress: 67.5% ... Training loss: 0.086 ... Validation loss: 0.243\r",
      "Progress: 67.5% ... Training loss: 0.065 ... Validation loss: 0.159\r",
      "Progress: 67.5% ... Training loss: 0.060 ... Validation loss: 0.189\r",
      "Progress: 67.5% ... Training loss: 0.059 ... Validation loss: 0.179\r",
      "Progress: 67.5% ... Training loss: 0.064 ... Validation loss: 0.206\r",
      "Progress: 67.6% ... Training loss: 0.074 ... Validation loss: 0.168\r",
      "Progress: 67.6% ... Training loss: 0.074 ... Validation loss: 0.218\r",
      "Progress: 67.6% ... Training loss: 0.074 ... Validation loss: 0.154\r",
      "Progress: 67.6% ... Training loss: 0.073 ... Validation loss: 0.203\r",
      "Progress: 67.6% ... Training loss: 0.067 ... Validation loss: 0.157\r",
      "Progress: 67.6% ... Training loss: 0.073 ... Validation loss: 0.192\r",
      "Progress: 67.6% ... Training loss: 0.064 ... Validation loss: 0.158\r",
      "Progress: 67.7% ... Training loss: 0.066 ... Validation loss: 0.194\r",
      "Progress: 67.7% ... Training loss: 0.068 ... Validation loss: 0.158\r",
      "Progress: 67.7% ... Training loss: 0.059 ... Validation loss: 0.178\r",
      "Progress: 67.7% ... Training loss: 0.060 ... Validation loss: 0.182\r",
      "Progress: 67.7% ... Training loss: 0.061 ... Validation loss: 0.197\r",
      "Progress: 67.7% ... Training loss: 0.061 ... Validation loss: 0.206\r",
      "Progress: 67.7% ... Training loss: 0.059 ... Validation loss: 0.197\r",
      "Progress: 67.7% ... Training loss: 0.059 ... Validation loss: 0.200\r",
      "Progress: 67.8% ... Training loss: 0.059 ... Validation loss: 0.184\r",
      "Progress: 67.8% ... Training loss: 0.062 ... Validation loss: 0.201\r",
      "Progress: 67.8% ... Training loss: 0.060 ... Validation loss: 0.173\r",
      "Progress: 67.8% ... Training loss: 0.060 ... Validation loss: 0.204\r",
      "Progress: 67.8% ... Training loss: 0.065 ... Validation loss: 0.178\r",
      "Progress: 67.8% ... Training loss: 0.064 ... Validation loss: 0.215\r",
      "Progress: 67.8% ... Training loss: 0.060 ... Validation loss: 0.190\r",
      "Progress: 67.8% ... Training loss: 0.059 ... Validation loss: 0.183\r",
      "Progress: 67.8% ... Training loss: 0.063 ... Validation loss: 0.167\r",
      "Progress: 67.9% ... Training loss: 0.060 ... Validation loss: 0.181\r",
      "Progress: 67.9% ... Training loss: 0.060 ... Validation loss: 0.198\r",
      "Progress: 67.9% ... Training loss: 0.059 ... Validation loss: 0.203\r",
      "Progress: 67.9% ... Training loss: 0.060 ... Validation loss: 0.192\r",
      "Progress: 67.9% ... Training loss: 0.068 ... Validation loss: 0.212\r",
      "Progress: 67.9% ... Training loss: 0.070 ... Validation loss: 0.168\r",
      "Progress: 67.9% ... Training loss: 0.063 ... Validation loss: 0.212\r",
      "Progress: 68.0% ... Training loss: 0.068 ... Validation loss: 0.175\r",
      "Progress: 68.0% ... Training loss: 0.064 ... Validation loss: 0.229\r",
      "Progress: 68.0% ... Training loss: 0.062 ... Validation loss: 0.186\r",
      "Progress: 68.0% ... Training loss: 0.059 ... Validation loss: 0.189\r",
      "Progress: 68.0% ... Training loss: 0.060 ... Validation loss: 0.182\r",
      "Progress: 68.0% ... Training loss: 0.059 ... Validation loss: 0.195\r",
      "Progress: 68.0% ... Training loss: 0.059 ... Validation loss: 0.197\r",
      "Progress: 68.0% ... Training loss: 0.059 ... Validation loss: 0.188\r",
      "Progress: 68.0% ... Training loss: 0.059 ... Validation loss: 0.190\r",
      "Progress: 68.1% ... Training loss: 0.060 ... Validation loss: 0.203\r",
      "Progress: 68.1% ... Training loss: 0.066 ... Validation loss: 0.229\r",
      "Progress: 68.1% ... Training loss: 0.073 ... Validation loss: 0.172\r",
      "Progress: 68.1% ... Training loss: 0.069 ... Validation loss: 0.223\r",
      "Progress: 68.1% ... Training loss: 0.065 ... Validation loss: 0.184\r",
      "Progress: 68.1% ... Training loss: 0.063 ... Validation loss: 0.225\r",
      "Progress: 68.1% ... Training loss: 0.067 ... Validation loss: 0.180\r",
      "Progress: 68.2% ... Training loss: 0.065 ... Validation loss: 0.240\r",
      "Progress: 68.2% ... Training loss: 0.065 ... Validation loss: 0.183\r",
      "Progress: 68.2% ... Training loss: 0.062 ... Validation loss: 0.202\r",
      "Progress: 68.2% ... Training loss: 0.062 ... Validation loss: 0.191\r",
      "Progress: 68.2% ... Training loss: 0.063 ... Validation loss: 0.217\r",
      "Progress: 68.2% ... Training loss: 0.068 ... Validation loss: 0.185\r",
      "Progress: 68.2% ... Training loss: 0.070 ... Validation loss: 0.235\r",
      "Progress: 68.2% ... Training loss: 0.065 ... Validation loss: 0.196\r",
      "Progress: 68.2% ... Training loss: 0.071 ... Validation loss: 0.265\r",
      "Progress: 68.3% ... Training loss: 0.061 ... Validation loss: 0.218\r",
      "Progress: 68.3% ... Training loss: 0.060 ... Validation loss: 0.229\r",
      "Progress: 68.3% ... Training loss: 0.061 ... Validation loss: 0.195\r",
      "Progress: 68.3% ... Training loss: 0.060 ... Validation loss: 0.208\r",
      "Progress: 68.3% ... Training loss: 0.059 ... Validation loss: 0.194\r",
      "Progress: 68.3% ... Training loss: 0.060 ... Validation loss: 0.207\r",
      "Progress: 68.3% ... Training loss: 0.062 ... Validation loss: 0.190\r",
      "Progress: 68.3% ... Training loss: 0.059 ... Validation loss: 0.204\r",
      "Progress: 68.4% ... Training loss: 0.059 ... Validation loss: 0.204\r",
      "Progress: 68.4% ... Training loss: 0.062 ... Validation loss: 0.185\r",
      "Progress: 68.4% ... Training loss: 0.065 ... Validation loss: 0.216\r",
      "Progress: 68.4% ... Training loss: 0.068 ... Validation loss: 0.169\r",
      "Progress: 68.4% ... Training loss: 0.061 ... Validation loss: 0.202\r",
      "Progress: 68.4% ... Training loss: 0.070 ... Validation loss: 0.175\r",
      "Progress: 68.4% ... Training loss: 0.064 ... Validation loss: 0.216\r",
      "Progress: 68.5% ... Training loss: 0.062 ... Validation loss: 0.179\r",
      "Progress: 68.5% ... Training loss: 0.064 ... Validation loss: 0.199\r",
      "Progress: 68.5% ... Training loss: 0.062 ... Validation loss: 0.181\r",
      "Progress: 68.5% ... Training loss: 0.063 ... Validation loss: 0.219\r",
      "Progress: 68.5% ... Training loss: 0.061 ... Validation loss: 0.187\r",
      "Progress: 68.5% ... Training loss: 0.060 ... Validation loss: 0.193\r",
      "Progress: 68.5% ... Training loss: 0.070 ... Validation loss: 0.166\r",
      "Progress: 68.5% ... Training loss: 0.069 ... Validation loss: 0.221\r",
      "Progress: 68.5% ... Training loss: 0.062 ... Validation loss: 0.183\r",
      "Progress: 68.6% ... Training loss: 0.059 ... Validation loss: 0.199\r",
      "Progress: 68.6% ... Training loss: 0.064 ... Validation loss: 0.177\r",
      "Progress: 68.6% ... Training loss: 0.063 ... Validation loss: 0.218\r",
      "Progress: 68.6% ... Training loss: 0.071 ... Validation loss: 0.172\r",
      "Progress: 68.6% ... Training loss: 0.061 ... Validation loss: 0.199\r",
      "Progress: 68.6% ... Training loss: 0.060 ... Validation loss: 0.188\r",
      "Progress: 68.6% ... Training loss: 0.063 ... Validation loss: 0.175\r",
      "Progress: 68.7% ... Training loss: 0.062 ... Validation loss: 0.197\r",
      "Progress: 68.7% ... Training loss: 0.059 ... Validation loss: 0.176\r",
      "Progress: 68.7% ... Training loss: 0.059 ... Validation loss: 0.178\r",
      "Progress: 68.7% ... Training loss: 0.061 ... Validation loss: 0.173\r",
      "Progress: 68.7% ... Training loss: 0.058 ... Validation loss: 0.193\r",
      "Progress: 68.7% ... Training loss: 0.060 ... Validation loss: 0.216\r",
      "Progress: 68.7% ... Training loss: 0.062 ... Validation loss: 0.191\r",
      "Progress: 68.7% ... Training loss: 0.059 ... Validation loss: 0.204\r",
      "Progress: 68.8% ... Training loss: 0.061 ... Validation loss: 0.194\r",
      "Progress: 68.8% ... Training loss: 0.063 ... Validation loss: 0.219\r",
      "Progress: 68.8% ... Training loss: 0.065 ... Validation loss: 0.178\r",
      "Progress: 68.8% ... Training loss: 0.058 ... Validation loss: 0.187\r",
      "Progress: 68.8% ... Training loss: 0.059 ... Validation loss: 0.201\r",
      "Progress: 68.8% ... Training loss: 0.060 ... Validation loss: 0.190\r",
      "Progress: 68.8% ... Training loss: 0.059 ... Validation loss: 0.208\r",
      "Progress: 68.8% ... Training loss: 0.059 ... Validation loss: 0.206\r",
      "Progress: 68.8% ... Training loss: 0.059 ... Validation loss: 0.214\r",
      "Progress: 68.9% ... Training loss: 0.058 ... Validation loss: 0.211\r",
      "Progress: 68.9% ... Training loss: 0.061 ... Validation loss: 0.194\r",
      "Progress: 68.9% ... Training loss: 0.060 ... Validation loss: 0.185\r",
      "Progress: 68.9% ... Training loss: 0.062 ... Validation loss: 0.176\r",
      "Progress: 68.9% ... Training loss: 0.065 ... Validation loss: 0.215\r",
      "Progress: 68.9% ... Training loss: 0.065 ... Validation loss: 0.171\r",
      "Progress: 68.9% ... Training loss: 0.065 ... Validation loss: 0.209\r",
      "Progress: 69.0% ... Training loss: 0.063 ... Validation loss: 0.176\r",
      "Progress: 69.0% ... Training loss: 0.062 ... Validation loss: 0.221\r",
      "Progress: 69.0% ... Training loss: 0.060 ... Validation loss: 0.189\r",
      "Progress: 69.0% ... Training loss: 0.058 ... Validation loss: 0.187\r",
      "Progress: 69.0% ... Training loss: 0.061 ... Validation loss: 0.185\r",
      "Progress: 69.0% ... Training loss: 0.071 ... Validation loss: 0.232\r",
      "Progress: 69.0% ... Training loss: 0.075 ... Validation loss: 0.165\r",
      "Progress: 69.0% ... Training loss: 0.070 ... Validation loss: 0.224\r",
      "Progress: 69.0% ... Training loss: 0.062 ... Validation loss: 0.178\r",
      "Progress: 69.1% ... Training loss: 0.060 ... Validation loss: 0.197\r",
      "Progress: 69.1% ... Training loss: 0.060 ... Validation loss: 0.182\r",
      "Progress: 69.1% ... Training loss: 0.062 ... Validation loss: 0.161\r",
      "Progress: 69.1% ... Training loss: 0.068 ... Validation loss: 0.207\r",
      "Progress: 69.1% ... Training loss: 0.071 ... Validation loss: 0.165\r",
      "Progress: 69.1% ... Training loss: 0.059 ... Validation loss: 0.186\r",
      "Progress: 69.1% ... Training loss: 0.059 ... Validation loss: 0.184\r",
      "Progress: 69.2% ... Training loss: 0.063 ... Validation loss: 0.208\r",
      "Progress: 69.2% ... Training loss: 0.059 ... Validation loss: 0.179\r",
      "Progress: 69.2% ... Training loss: 0.058 ... Validation loss: 0.181\r",
      "Progress: 69.2% ... Training loss: 0.059 ... Validation loss: 0.187\r",
      "Progress: 69.2% ... Training loss: 0.062 ... Validation loss: 0.172\r",
      "Progress: 69.2% ... Training loss: 0.060 ... Validation loss: 0.196\r",
      "Progress: 69.2% ... Training loss: 0.058 ... Validation loss: 0.184\r",
      "Progress: 69.2% ... Training loss: 0.058 ... Validation loss: 0.185\r",
      "Progress: 69.2% ... Training loss: 0.061 ... Validation loss: 0.169\r",
      "Progress: 69.3% ... Training loss: 0.060 ... Validation loss: 0.189\r",
      "Progress: 69.3% ... Training loss: 0.059 ... Validation loss: 0.177\r",
      "Progress: 69.3% ... Training loss: 0.059 ... Validation loss: 0.173\r",
      "Progress: 69.3% ... Training loss: 0.062 ... Validation loss: 0.190\r",
      "Progress: 69.3% ... Training loss: 0.061 ... Validation loss: 0.174\r",
      "Progress: 69.3% ... Training loss: 0.061 ... Validation loss: 0.198\r",
      "Progress: 69.3% ... Training loss: 0.073 ... Validation loss: 0.160\r",
      "Progress: 69.3% ... Training loss: 0.072 ... Validation loss: 0.241\r",
      "Progress: 69.4% ... Training loss: 0.068 ... Validation loss: 0.168\r",
      "Progress: 69.4% ... Training loss: 0.063 ... Validation loss: 0.218\r",
      "Progress: 69.4% ... Training loss: 0.068 ... Validation loss: 0.172\r",
      "Progress: 69.4% ... Training loss: 0.059 ... Validation loss: 0.182\r",
      "Progress: 69.4% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 69.4% ... Training loss: 0.062 ... Validation loss: 0.184\r",
      "Progress: 69.4% ... Training loss: 0.062 ... Validation loss: 0.198\r",
      "Progress: 69.5% ... Training loss: 0.061 ... Validation loss: 0.161\r",
      "Progress: 69.5% ... Training loss: 0.065 ... Validation loss: 0.197\r",
      "Progress: 69.5% ... Training loss: 0.079 ... Validation loss: 0.160\r",
      "Progress: 69.5% ... Training loss: 0.082 ... Validation loss: 0.225\r",
      "Progress: 69.5% ... Training loss: 0.077 ... Validation loss: 0.170\r",
      "Progress: 69.5% ... Training loss: 0.069 ... Validation loss: 0.214\r",
      "Progress: 69.5% ... Training loss: 0.073 ... Validation loss: 0.166\r",
      "Progress: 69.5% ... Training loss: 0.060 ... Validation loss: 0.206\r",
      "Progress: 69.5% ... Training loss: 0.069 ... Validation loss: 0.170\r",
      "Progress: 69.6% ... Training loss: 0.064 ... Validation loss: 0.212\r",
      "Progress: 69.6% ... Training loss: 0.059 ... Validation loss: 0.188\r",
      "Progress: 69.6% ... Training loss: 0.059 ... Validation loss: 0.184\r",
      "Progress: 69.6% ... Training loss: 0.059 ... Validation loss: 0.193\r",
      "Progress: 69.6% ... Training loss: 0.064 ... Validation loss: 0.183\r",
      "Progress: 69.6% ... Training loss: 0.072 ... Validation loss: 0.226\r",
      "Progress: 69.6% ... Training loss: 0.059 ... Validation loss: 0.178\r",
      "Progress: 69.7% ... Training loss: 0.062 ... Validation loss: 0.197\r",
      "Progress: 69.7% ... Training loss: 0.066 ... Validation loss: 0.164\r",
      "Progress: 69.7% ... Training loss: 0.075 ... Validation loss: 0.220\r",
      "Progress: 69.7% ... Training loss: 0.065 ... Validation loss: 0.167\r",
      "Progress: 69.7% ... Training loss: 0.080 ... Validation loss: 0.225\r",
      "Progress: 69.7% ... Training loss: 0.085 ... Validation loss: 0.160\r",
      "Progress: 69.7% ... Training loss: 0.070 ... Validation loss: 0.217\r",
      "Progress: 69.7% ... Training loss: 0.060 ... Validation loss: 0.173\r",
      "Progress: 69.8% ... Training loss: 0.058 ... Validation loss: 0.189\r",
      "Progress: 69.8% ... Training loss: 0.058 ... Validation loss: 0.182\r",
      "Progress: 69.8% ... Training loss: 0.059 ... Validation loss: 0.177\r",
      "Progress: 69.8% ... Training loss: 0.058 ... Validation loss: 0.193\r",
      "Progress: 69.8% ... Training loss: 0.058 ... Validation loss: 0.197\r",
      "Progress: 69.8% ... Training loss: 0.059 ... Validation loss: 0.168\r",
      "Progress: 69.8% ... Training loss: 0.058 ... Validation loss: 0.180\r",
      "Progress: 69.8% ... Training loss: 0.059 ... Validation loss: 0.189\r",
      "Progress: 69.8% ... Training loss: 0.058 ... Validation loss: 0.178\r",
      "Progress: 69.9% ... Training loss: 0.059 ... Validation loss: 0.186\r",
      "Progress: 69.9% ... Training loss: 0.059 ... Validation loss: 0.178\r",
      "Progress: 69.9% ... Training loss: 0.063 ... Validation loss: 0.201\r",
      "Progress: 69.9% ... Training loss: 0.061 ... Validation loss: 0.171\r",
      "Progress: 69.9% ... Training loss: 0.058 ... Validation loss: 0.184\r",
      "Progress: 69.9% ... Training loss: 0.059 ... Validation loss: 0.187\r",
      "Progress: 69.9% ... Training loss: 0.059 ... Validation loss: 0.184\r",
      "Progress: 70.0% ... Training loss: 0.059 ... Validation loss: 0.191\r",
      "Progress: 70.0% ... Training loss: 0.060 ... Validation loss: 0.189\r",
      "Progress: 70.0% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 70.0% ... Training loss: 0.059 ... Validation loss: 0.191\r",
      "Progress: 70.0% ... Training loss: 0.059 ... Validation loss: 0.186\r",
      "Progress: 70.0% ... Training loss: 0.060 ... Validation loss: 0.190\r",
      "Progress: 70.0% ... Training loss: 0.063 ... Validation loss: 0.221\r",
      "Progress: 70.0% ... Training loss: 0.060 ... Validation loss: 0.190\r",
      "Progress: 70.0% ... Training loss: 0.061 ... Validation loss: 0.206\r",
      "Progress: 70.1% ... Training loss: 0.058 ... Validation loss: 0.184\r",
      "Progress: 70.1% ... Training loss: 0.063 ... Validation loss: 0.205\r",
      "Progress: 70.1% ... Training loss: 0.073 ... Validation loss: 0.182\r",
      "Progress: 70.1% ... Training loss: 0.090 ... Validation loss: 0.278\r",
      "Progress: 70.1% ... Training loss: 0.074 ... Validation loss: 0.173\r",
      "Progress: 70.1% ... Training loss: 0.064 ... Validation loss: 0.217\r",
      "Progress: 70.1% ... Training loss: 0.073 ... Validation loss: 0.161\r",
      "Progress: 70.2% ... Training loss: 0.082 ... Validation loss: 0.240\r",
      "Progress: 70.2% ... Training loss: 0.099 ... Validation loss: 0.157\r",
      "Progress: 70.2% ... Training loss: 0.085 ... Validation loss: 0.231\r",
      "Progress: 70.2% ... Training loss: 0.068 ... Validation loss: 0.166\r",
      "Progress: 70.2% ... Training loss: 0.074 ... Validation loss: 0.223\r",
      "Progress: 70.2% ... Training loss: 0.065 ... Validation loss: 0.173\r",
      "Progress: 70.2% ... Training loss: 0.061 ... Validation loss: 0.209\r",
      "Progress: 70.2% ... Training loss: 0.061 ... Validation loss: 0.179\r",
      "Progress: 70.2% ... Training loss: 0.061 ... Validation loss: 0.203\r",
      "Progress: 70.3% ... Training loss: 0.066 ... Validation loss: 0.168\r",
      "Progress: 70.3% ... Training loss: 0.068 ... Validation loss: 0.211\r",
      "Progress: 70.3% ... Training loss: 0.062 ... Validation loss: 0.175\r",
      "Progress: 70.3% ... Training loss: 0.065 ... Validation loss: 0.221\r",
      "Progress: 70.3% ... Training loss: 0.059 ... Validation loss: 0.190\r",
      "Progress: 70.3% ... Training loss: 0.062 ... Validation loss: 0.211\r",
      "Progress: 70.3% ... Training loss: 0.060 ... Validation loss: 0.207\r",
      "Progress: 70.3% ... Training loss: 0.059 ... Validation loss: 0.199\r",
      "Progress: 70.4% ... Training loss: 0.059 ... Validation loss: 0.205\r",
      "Progress: 70.4% ... Training loss: 0.059 ... Validation loss: 0.176\r",
      "Progress: 70.4% ... Training loss: 0.066 ... Validation loss: 0.203\r",
      "Progress: 70.4% ... Training loss: 0.059 ... Validation loss: 0.187\r",
      "Progress: 70.4% ... Training loss: 0.063 ... Validation loss: 0.179\r",
      "Progress: 70.4% ... Training loss: 0.087 ... Validation loss: 0.163\r",
      "Progress: 70.4% ... Training loss: 0.063 ... Validation loss: 0.203\r",
      "Progress: 70.5% ... Training loss: 0.059 ... Validation loss: 0.183\r",
      "Progress: 70.5% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 70.5% ... Training loss: 0.060 ... Validation loss: 0.188\r",
      "Progress: 70.5% ... Training loss: 0.060 ... Validation loss: 0.168\r",
      "Progress: 70.5% ... Training loss: 0.066 ... Validation loss: 0.192\r",
      "Progress: 70.5% ... Training loss: 0.060 ... Validation loss: 0.167\r",
      "Progress: 70.5% ... Training loss: 0.060 ... Validation loss: 0.182\r",
      "Progress: 70.5% ... Training loss: 0.068 ... Validation loss: 0.159\r",
      "Progress: 70.5% ... Training loss: 0.076 ... Validation loss: 0.215\r",
      "Progress: 70.6% ... Training loss: 0.098 ... Validation loss: 0.161\r",
      "Progress: 70.6% ... Training loss: 0.069 ... Validation loss: 0.230\r",
      "Progress: 70.6% ... Training loss: 0.062 ... Validation loss: 0.187\r",
      "Progress: 70.6% ... Training loss: 0.061 ... Validation loss: 0.206\r",
      "Progress: 70.6% ... Training loss: 0.058 ... Validation loss: 0.180\r",
      "Progress: 70.6% ... Training loss: 0.074 ... Validation loss: 0.163\r",
      "Progress: 70.6% ... Training loss: 0.085 ... Validation loss: 0.257\r",
      "Progress: 70.7% ... Training loss: 0.094 ... Validation loss: 0.184\r",
      "Progress: 70.7% ... Training loss: 0.072 ... Validation loss: 0.250\r",
      "Progress: 70.7% ... Training loss: 0.061 ... Validation loss: 0.178\r",
      "Progress: 70.7% ... Training loss: 0.059 ... Validation loss: 0.187\r",
      "Progress: 70.7% ... Training loss: 0.058 ... Validation loss: 0.185\r",
      "Progress: 70.7% ... Training loss: 0.058 ... Validation loss: 0.194\r",
      "Progress: 70.7% ... Training loss: 0.059 ... Validation loss: 0.192\r",
      "Progress: 70.7% ... Training loss: 0.059 ... Validation loss: 0.197\r",
      "Progress: 70.8% ... Training loss: 0.058 ... Validation loss: 0.187\r",
      "Progress: 70.8% ... Training loss: 0.058 ... Validation loss: 0.192\r",
      "Progress: 70.8% ... Training loss: 0.060 ... Validation loss: 0.179\r",
      "Progress: 70.8% ... Training loss: 0.064 ... Validation loss: 0.209\r",
      "Progress: 70.8% ... Training loss: 0.064 ... Validation loss: 0.169\r",
      "Progress: 70.8% ... Training loss: 0.063 ... Validation loss: 0.210\r",
      "Progress: 70.8% ... Training loss: 0.061 ... Validation loss: 0.192\r",
      "Progress: 70.8% ... Training loss: 0.062 ... Validation loss: 0.185\r",
      "Progress: 70.8% ... Training loss: 0.060 ... Validation loss: 0.215\r",
      "Progress: 70.9% ... Training loss: 0.061 ... Validation loss: 0.184\r",
      "Progress: 70.9% ... Training loss: 0.062 ... Validation loss: 0.211\r",
      "Progress: 70.9% ... Training loss: 0.068 ... Validation loss: 0.173\r",
      "Progress: 70.9% ... Training loss: 0.084 ... Validation loss: 0.250\r",
      "Progress: 70.9% ... Training loss: 0.071 ... Validation loss: 0.159\r",
      "Progress: 70.9% ... Training loss: 0.082 ... Validation loss: 0.224\r",
      "Progress: 70.9% ... Training loss: 0.060 ... Validation loss: 0.173\r",
      "Progress: 71.0% ... Training loss: 0.062 ... Validation loss: 0.197\r",
      "Progress: 71.0% ... Training loss: 0.075 ... Validation loss: 0.165\r",
      "Progress: 71.0% ... Training loss: 0.082 ... Validation loss: 0.225\r",
      "Progress: 71.0% ... Training loss: 0.083 ... Validation loss: 0.160\r",
      "Progress: 71.0% ... Training loss: 0.074 ... Validation loss: 0.210\r",
      "Progress: 71.0% ... Training loss: 0.063 ... Validation loss: 0.171\r",
      "Progress: 71.0% ... Training loss: 0.073 ... Validation loss: 0.197\r",
      "Progress: 71.0% ... Training loss: 0.070 ... Validation loss: 0.174\r",
      "Progress: 71.0% ... Training loss: 0.064 ... Validation loss: 0.198\r",
      "Progress: 71.1% ... Training loss: 0.060 ... Validation loss: 0.163\r",
      "Progress: 71.1% ... Training loss: 0.061 ... Validation loss: 0.180\r",
      "Progress: 71.1% ... Training loss: 0.059 ... Validation loss: 0.171\r",
      "Progress: 71.1% ... Training loss: 0.064 ... Validation loss: 0.197\r",
      "Progress: 71.1% ... Training loss: 0.063 ... Validation loss: 0.172\r",
      "Progress: 71.1% ... Training loss: 0.059 ... Validation loss: 0.198\r",
      "Progress: 71.1% ... Training loss: 0.064 ... Validation loss: 0.174\r",
      "Progress: 71.2% ... Training loss: 0.059 ... Validation loss: 0.200\r",
      "Progress: 71.2% ... Training loss: 0.059 ... Validation loss: 0.195\r",
      "Progress: 71.2% ... Training loss: 0.059 ... Validation loss: 0.199\r",
      "Progress: 71.2% ... Training loss: 0.065 ... Validation loss: 0.174\r",
      "Progress: 71.2% ... Training loss: 0.062 ... Validation loss: 0.215\r",
      "Progress: 71.2% ... Training loss: 0.070 ... Validation loss: 0.166\r",
      "Progress: 71.2% ... Training loss: 0.069 ... Validation loss: 0.206\r",
      "Progress: 71.2% ... Training loss: 0.067 ... Validation loss: 0.159\r",
      "Progress: 71.2% ... Training loss: 0.064 ... Validation loss: 0.192\r",
      "Progress: 71.3% ... Training loss: 0.066 ... Validation loss: 0.168\r",
      "Progress: 71.3% ... Training loss: 0.066 ... Validation loss: 0.218\r",
      "Progress: 71.3% ... Training loss: 0.063 ... Validation loss: 0.170\r",
      "Progress: 71.3% ... Training loss: 0.066 ... Validation loss: 0.210\r",
      "Progress: 71.3% ... Training loss: 0.065 ... Validation loss: 0.174\r",
      "Progress: 71.3% ... Training loss: 0.058 ... Validation loss: 0.206\r",
      "Progress: 71.3% ... Training loss: 0.063 ... Validation loss: 0.184\r",
      "Progress: 71.3% ... Training loss: 0.060 ... Validation loss: 0.176\r",
      "Progress: 71.4% ... Training loss: 0.058 ... Validation loss: 0.181\r",
      "Progress: 71.4% ... Training loss: 0.059 ... Validation loss: 0.197\r",
      "Progress: 71.4% ... Training loss: 0.059 ... Validation loss: 0.203\r",
      "Progress: 71.4% ... Training loss: 0.061 ... Validation loss: 0.181\r",
      "Progress: 71.4% ... Training loss: 0.061 ... Validation loss: 0.212\r",
      "Progress: 71.4% ... Training loss: 0.060 ... Validation loss: 0.183\r",
      "Progress: 71.4% ... Training loss: 0.060 ... Validation loss: 0.178\r",
      "Progress: 71.5% ... Training loss: 0.058 ... Validation loss: 0.196\r",
      "Progress: 71.5% ... Training loss: 0.059 ... Validation loss: 0.189\r",
      "Progress: 71.5% ... Training loss: 0.072 ... Validation loss: 0.234\r",
      "Progress: 71.5% ... Training loss: 0.060 ... Validation loss: 0.186\r",
      "Progress: 71.5% ... Training loss: 0.059 ... Validation loss: 0.206\r",
      "Progress: 71.5% ... Training loss: 0.058 ... Validation loss: 0.201\r",
      "Progress: 71.5% ... Training loss: 0.058 ... Validation loss: 0.200\r",
      "Progress: 71.5% ... Training loss: 0.065 ... Validation loss: 0.225\r",
      "Progress: 71.5% ... Training loss: 0.065 ... Validation loss: 0.173\r",
      "Progress: 71.6% ... Training loss: 0.069 ... Validation loss: 0.232\r",
      "Progress: 71.6% ... Training loss: 0.065 ... Validation loss: 0.173\r",
      "Progress: 71.6% ... Training loss: 0.068 ... Validation loss: 0.225\r",
      "Progress: 71.6% ... Training loss: 0.058 ... Validation loss: 0.177\r",
      "Progress: 71.6% ... Training loss: 0.059 ... Validation loss: 0.175\r",
      "Progress: 71.6% ... Training loss: 0.060 ... Validation loss: 0.198\r",
      "Progress: 71.6% ... Training loss: 0.058 ... Validation loss: 0.185\r",
      "Progress: 71.7% ... Training loss: 0.061 ... Validation loss: 0.176\r",
      "Progress: 71.7% ... Training loss: 0.058 ... Validation loss: 0.187\r",
      "Progress: 71.7% ... Training loss: 0.059 ... Validation loss: 0.175\r",
      "Progress: 71.7% ... Training loss: 0.058 ... Validation loss: 0.183\r",
      "Progress: 71.7% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 71.7% ... Training loss: 0.059 ... Validation loss: 0.182\r",
      "Progress: 71.7% ... Training loss: 0.059 ... Validation loss: 0.170\r",
      "Progress: 71.7% ... Training loss: 0.059 ... Validation loss: 0.177\r",
      "Progress: 71.8% ... Training loss: 0.062 ... Validation loss: 0.202\r",
      "Progress: 71.8% ... Training loss: 0.066 ... Validation loss: 0.165\r",
      "Progress: 71.8% ... Training loss: 0.071 ... Validation loss: 0.220\r",
      "Progress: 71.8% ... Training loss: 0.066 ... Validation loss: 0.167\r",
      "Progress: 71.8% ... Training loss: 0.070 ... Validation loss: 0.204\r",
      "Progress: 71.8% ... Training loss: 0.060 ... Validation loss: 0.172\r",
      "Progress: 71.8% ... Training loss: 0.059 ... Validation loss: 0.175\r",
      "Progress: 71.8% ... Training loss: 0.059 ... Validation loss: 0.197\r",
      "Progress: 71.8% ... Training loss: 0.073 ... Validation loss: 0.163\r",
      "Progress: 71.9% ... Training loss: 0.073 ... Validation loss: 0.220\r",
      "Progress: 71.9% ... Training loss: 0.075 ... Validation loss: 0.163\r",
      "Progress: 71.9% ... Training loss: 0.059 ... Validation loss: 0.197\r",
      "Progress: 71.9% ... Training loss: 0.059 ... Validation loss: 0.184\r",
      "Progress: 71.9% ... Training loss: 0.060 ... Validation loss: 0.180\r",
      "Progress: 71.9% ... Training loss: 0.059 ... Validation loss: 0.199\r",
      "Progress: 71.9% ... Training loss: 0.059 ... Validation loss: 0.189\r",
      "Progress: 72.0% ... Training loss: 0.058 ... Validation loss: 0.197\r",
      "Progress: 72.0% ... Training loss: 0.058 ... Validation loss: 0.189\r",
      "Progress: 72.0% ... Training loss: 0.064 ... Validation loss: 0.206\r",
      "Progress: 72.0% ... Training loss: 0.062 ... Validation loss: 0.175\r",
      "Progress: 72.0% ... Training loss: 0.060 ... Validation loss: 0.204\r",
      "Progress: 72.0% ... Training loss: 0.061 ... Validation loss: 0.181\r",
      "Progress: 72.0% ... Training loss: 0.061 ... Validation loss: 0.190\r",
      "Progress: 72.0% ... Training loss: 0.065 ... Validation loss: 0.164\r",
      "Progress: 72.0% ... Training loss: 0.064 ... Validation loss: 0.203\r",
      "Progress: 72.1% ... Training loss: 0.067 ... Validation loss: 0.163\r",
      "Progress: 72.1% ... Training loss: 0.068 ... Validation loss: 0.207\r",
      "Progress: 72.1% ... Training loss: 0.082 ... Validation loss: 0.158\r",
      "Progress: 72.1% ... Training loss: 0.065 ... Validation loss: 0.198\r",
      "Progress: 72.1% ... Training loss: 0.058 ... Validation loss: 0.180\r",
      "Progress: 72.1% ... Training loss: 0.059 ... Validation loss: 0.174\r",
      "Progress: 72.1% ... Training loss: 0.059 ... Validation loss: 0.167\r",
      "Progress: 72.2% ... Training loss: 0.057 ... Validation loss: 0.173\r",
      "Progress: 72.2% ... Training loss: 0.059 ... Validation loss: 0.182\r",
      "Progress: 72.2% ... Training loss: 0.059 ... Validation loss: 0.183\r",
      "Progress: 72.2% ... Training loss: 0.061 ... Validation loss: 0.161\r",
      "Progress: 72.2% ... Training loss: 0.061 ... Validation loss: 0.192\r",
      "Progress: 72.2% ... Training loss: 0.076 ... Validation loss: 0.161\r",
      "Progress: 72.2% ... Training loss: 0.064 ... Validation loss: 0.202\r",
      "Progress: 72.2% ... Training loss: 0.064 ... Validation loss: 0.169\r",
      "Progress: 72.2% ... Training loss: 0.059 ... Validation loss: 0.184\r",
      "Progress: 72.3% ... Training loss: 0.058 ... Validation loss: 0.198\r",
      "Progress: 72.3% ... Training loss: 0.061 ... Validation loss: 0.208\r",
      "Progress: 72.3% ... Training loss: 0.061 ... Validation loss: 0.177\r",
      "Progress: 72.3% ... Training loss: 0.062 ... Validation loss: 0.195\r",
      "Progress: 72.3% ... Training loss: 0.062 ... Validation loss: 0.169\r",
      "Progress: 72.3% ... Training loss: 0.064 ... Validation loss: 0.206\r",
      "Progress: 72.3% ... Training loss: 0.076 ... Validation loss: 0.173\r",
      "Progress: 72.3% ... Training loss: 0.081 ... Validation loss: 0.248\r",
      "Progress: 72.4% ... Training loss: 0.083 ... Validation loss: 0.162\r",
      "Progress: 72.4% ... Training loss: 0.084 ... Validation loss: 0.263\r",
      "Progress: 72.4% ... Training loss: 0.069 ... Validation loss: 0.174\r",
      "Progress: 72.4% ... Training loss: 0.062 ... Validation loss: 0.213\r",
      "Progress: 72.4% ... Training loss: 0.062 ... Validation loss: 0.175\r",
      "Progress: 72.4% ... Training loss: 0.075 ... Validation loss: 0.230\r",
      "Progress: 72.4% ... Training loss: 0.093 ... Validation loss: 0.170\r",
      "Progress: 72.5% ... Training loss: 0.109 ... Validation loss: 0.311\r",
      "Progress: 72.5% ... Training loss: 0.087 ... Validation loss: 0.170\r",
      "Progress: 72.5% ... Training loss: 0.073 ... Validation loss: 0.257\r",
      "Progress: 72.5% ... Training loss: 0.077 ... Validation loss: 0.169\r",
      "Progress: 72.5% ... Training loss: 0.065 ... Validation loss: 0.227\r",
      "Progress: 72.5% ... Training loss: 0.062 ... Validation loss: 0.184\r",
      "Progress: 72.5% ... Training loss: 0.068 ... Validation loss: 0.236\r",
      "Progress: 72.5% ... Training loss: 0.072 ... Validation loss: 0.174\r",
      "Progress: 72.5% ... Training loss: 0.074 ... Validation loss: 0.240\r",
      "Progress: 72.6% ... Training loss: 0.085 ... Validation loss: 0.168\r",
      "Progress: 72.6% ... Training loss: 0.086 ... Validation loss: 0.272\r",
      "Progress: 72.6% ... Training loss: 0.075 ... Validation loss: 0.156\r",
      "Progress: 72.6% ... Training loss: 0.078 ... Validation loss: 0.217\r",
      "Progress: 72.6% ... Training loss: 0.084 ... Validation loss: 0.156\r",
      "Progress: 72.6% ... Training loss: 0.118 ... Validation loss: 0.260\r",
      "Progress: 72.6% ... Training loss: 0.155 ... Validation loss: 0.166\r",
      "Progress: 72.7% ... Training loss: 0.135 ... Validation loss: 0.298\r",
      "Progress: 72.7% ... Training loss: 0.082 ... Validation loss: 0.165\r",
      "Progress: 72.7% ... Training loss: 0.073 ... Validation loss: 0.218\r",
      "Progress: 72.7% ... Training loss: 0.072 ... Validation loss: 0.160\r",
      "Progress: 72.7% ... Training loss: 0.061 ... Validation loss: 0.190\r",
      "Progress: 72.7% ... Training loss: 0.060 ... Validation loss: 0.177\r",
      "Progress: 72.7% ... Training loss: 0.078 ... Validation loss: 0.215\r",
      "Progress: 72.7% ... Training loss: 0.066 ... Validation loss: 0.163\r",
      "Progress: 72.8% ... Training loss: 0.064 ... Validation loss: 0.219\r",
      "Progress: 72.8% ... Training loss: 0.058 ... Validation loss: 0.189\r",
      "Progress: 72.8% ... Training loss: 0.059 ... Validation loss: 0.193\r",
      "Progress: 72.8% ... Training loss: 0.059 ... Validation loss: 0.207\r",
      "Progress: 72.8% ... Training loss: 0.059 ... Validation loss: 0.198\r",
      "Progress: 72.8% ... Training loss: 0.058 ... Validation loss: 0.191\r",
      "Progress: 72.8% ... Training loss: 0.058 ... Validation loss: 0.191\r",
      "Progress: 72.8% ... Training loss: 0.060 ... Validation loss: 0.174\r",
      "Progress: 72.8% ... Training loss: 0.059 ... Validation loss: 0.194\r",
      "Progress: 72.9% ... Training loss: 0.059 ... Validation loss: 0.199\r",
      "Progress: 72.9% ... Training loss: 0.060 ... Validation loss: 0.186\r",
      "Progress: 72.9% ... Training loss: 0.057 ... Validation loss: 0.186\r",
      "Progress: 72.9% ... Training loss: 0.058 ... Validation loss: 0.190\r",
      "Progress: 72.9% ... Training loss: 0.059 ... Validation loss: 0.188\r",
      "Progress: 72.9% ... Training loss: 0.061 ... Validation loss: 0.178\r",
      "Progress: 72.9% ... Training loss: 0.059 ... Validation loss: 0.207\r",
      "Progress: 73.0% ... Training loss: 0.062 ... Validation loss: 0.179\r",
      "Progress: 73.0% ... Training loss: 0.059 ... Validation loss: 0.206\r",
      "Progress: 73.0% ... Training loss: 0.058 ... Validation loss: 0.189\r",
      "Progress: 73.0% ... Training loss: 0.058 ... Validation loss: 0.199\r",
      "Progress: 73.0% ... Training loss: 0.058 ... Validation loss: 0.201\r",
      "Progress: 73.0% ... Training loss: 0.059 ... Validation loss: 0.188\r",
      "Progress: 73.0% ... Training loss: 0.058 ... Validation loss: 0.185\r",
      "Progress: 73.0% ... Training loss: 0.058 ... Validation loss: 0.190\r",
      "Progress: 73.0% ... Training loss: 0.058 ... Validation loss: 0.181\r",
      "Progress: 73.1% ... Training loss: 0.059 ... Validation loss: 0.196\r",
      "Progress: 73.1% ... Training loss: 0.058 ... Validation loss: 0.184\r",
      "Progress: 73.1% ... Training loss: 0.057 ... Validation loss: 0.187\r",
      "Progress: 73.1% ... Training loss: 0.058 ... Validation loss: 0.192\r",
      "Progress: 73.1% ... Training loss: 0.060 ... Validation loss: 0.191\r",
      "Progress: 73.1% ... Training loss: 0.058 ... Validation loss: 0.170\r",
      "Progress: 73.1% ... Training loss: 0.058 ... Validation loss: 0.185\r",
      "Progress: 73.2% ... Training loss: 0.058 ... Validation loss: 0.184\r",
      "Progress: 73.2% ... Training loss: 0.061 ... Validation loss: 0.161\r",
      "Progress: 73.2% ... Training loss: 0.059 ... Validation loss: 0.186\r",
      "Progress: 73.2% ... Training loss: 0.070 ... Validation loss: 0.177\r",
      "Progress: 73.2% ... Training loss: 0.083 ... Validation loss: 0.254\r",
      "Progress: 73.2% ... Training loss: 0.073 ... Validation loss: 0.166\r",
      "Progress: 73.2% ... Training loss: 0.078 ... Validation loss: 0.239\r",
      "Progress: 73.2% ... Training loss: 0.065 ... Validation loss: 0.165\r",
      "Progress: 73.2% ... Training loss: 0.059 ... Validation loss: 0.202\r",
      "Progress: 73.3% ... Training loss: 0.065 ... Validation loss: 0.170\r",
      "Progress: 73.3% ... Training loss: 0.060 ... Validation loss: 0.186\r",
      "Progress: 73.3% ... Training loss: 0.061 ... Validation loss: 0.159\r",
      "Progress: 73.3% ... Training loss: 0.060 ... Validation loss: 0.195\r",
      "Progress: 73.3% ... Training loss: 0.068 ... Validation loss: 0.167\r",
      "Progress: 73.3% ... Training loss: 0.070 ... Validation loss: 0.208\r",
      "Progress: 73.3% ... Training loss: 0.064 ... Validation loss: 0.159\r",
      "Progress: 73.3% ... Training loss: 0.078 ... Validation loss: 0.212\r",
      "Progress: 73.4% ... Training loss: 0.073 ... Validation loss: 0.158\r",
      "Progress: 73.4% ... Training loss: 0.081 ... Validation loss: 0.228\r",
      "Progress: 73.4% ... Training loss: 0.063 ... Validation loss: 0.162\r",
      "Progress: 73.4% ... Training loss: 0.058 ... Validation loss: 0.204\r",
      "Progress: 73.4% ... Training loss: 0.060 ... Validation loss: 0.224\r",
      "Progress: 73.4% ... Training loss: 0.062 ... Validation loss: 0.186\r",
      "Progress: 73.4% ... Training loss: 0.059 ... Validation loss: 0.216\r",
      "Progress: 73.5% ... Training loss: 0.058 ... Validation loss: 0.182\r",
      "Progress: 73.5% ... Training loss: 0.059 ... Validation loss: 0.182\r",
      "Progress: 73.5% ... Training loss: 0.058 ... Validation loss: 0.198\r",
      "Progress: 73.5% ... Training loss: 0.058 ... Validation loss: 0.195\r",
      "Progress: 73.5% ... Training loss: 0.058 ... Validation loss: 0.183\r",
      "Progress: 73.5% ... Training loss: 0.059 ... Validation loss: 0.182\r",
      "Progress: 73.5% ... Training loss: 0.058 ... Validation loss: 0.192\r",
      "Progress: 73.5% ... Training loss: 0.061 ... Validation loss: 0.241\r",
      "Progress: 73.5% ... Training loss: 0.066 ... Validation loss: 0.178\r",
      "Progress: 73.6% ... Training loss: 0.071 ... Validation loss: 0.235\r",
      "Progress: 73.6% ... Training loss: 0.061 ... Validation loss: 0.171\r",
      "Progress: 73.6% ... Training loss: 0.059 ... Validation loss: 0.194\r",
      "Progress: 73.6% ... Training loss: 0.060 ... Validation loss: 0.212\r",
      "Progress: 73.6% ... Training loss: 0.061 ... Validation loss: 0.172\r",
      "Progress: 73.6% ... Training loss: 0.066 ... Validation loss: 0.214\r",
      "Progress: 73.6% ... Training loss: 0.074 ... Validation loss: 0.170\r",
      "Progress: 73.7% ... Training loss: 0.066 ... Validation loss: 0.240\r",
      "Progress: 73.7% ... Training loss: 0.070 ... Validation loss: 0.169\r",
      "Progress: 73.7% ... Training loss: 0.067 ... Validation loss: 0.240\r",
      "Progress: 73.7% ... Training loss: 0.058 ... Validation loss: 0.182\r",
      "Progress: 73.7% ... Training loss: 0.059 ... Validation loss: 0.210\r",
      "Progress: 73.7% ... Training loss: 0.059 ... Validation loss: 0.194\r",
      "Progress: 73.7% ... Training loss: 0.059 ... Validation loss: 0.200\r",
      "Progress: 73.7% ... Training loss: 0.061 ... Validation loss: 0.172\r",
      "Progress: 73.8% ... Training loss: 0.061 ... Validation loss: 0.201\r",
      "Progress: 73.8% ... Training loss: 0.059 ... Validation loss: 0.180\r",
      "Progress: 73.8% ... Training loss: 0.058 ... Validation loss: 0.194\r",
      "Progress: 73.8% ... Training loss: 0.058 ... Validation loss: 0.191\r",
      "Progress: 73.8% ... Training loss: 0.058 ... Validation loss: 0.184\r",
      "Progress: 73.8% ... Training loss: 0.058 ... Validation loss: 0.173\r",
      "Progress: 73.8% ... Training loss: 0.058 ... Validation loss: 0.187\r",
      "Progress: 73.8% ... Training loss: 0.058 ... Validation loss: 0.182\r",
      "Progress: 73.8% ... Training loss: 0.061 ... Validation loss: 0.195\r",
      "Progress: 73.9% ... Training loss: 0.058 ... Validation loss: 0.180\r",
      "Progress: 73.9% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 73.9% ... Training loss: 0.058 ... Validation loss: 0.184\r",
      "Progress: 73.9% ... Training loss: 0.058 ... Validation loss: 0.201\r",
      "Progress: 73.9% ... Training loss: 0.062 ... Validation loss: 0.211\r",
      "Progress: 73.9% ... Training loss: 0.059 ... Validation loss: 0.186\r",
      "Progress: 73.9% ... Training loss: 0.057 ... Validation loss: 0.187\r",
      "Progress: 74.0% ... Training loss: 0.058 ... Validation loss: 0.185\r",
      "Progress: 74.0% ... Training loss: 0.057 ... Validation loss: 0.196\r",
      "Progress: 74.0% ... Training loss: 0.060 ... Validation loss: 0.186\r",
      "Progress: 74.0% ... Training loss: 0.063 ... Validation loss: 0.212\r",
      "Progress: 74.0% ... Training loss: 0.062 ... Validation loss: 0.172\r",
      "Progress: 74.0% ... Training loss: 0.057 ... Validation loss: 0.184\r",
      "Progress: 74.0% ... Training loss: 0.059 ... Validation loss: 0.193\r",
      "Progress: 74.0% ... Training loss: 0.060 ... Validation loss: 0.177\r",
      "Progress: 74.0% ... Training loss: 0.060 ... Validation loss: 0.198\r",
      "Progress: 74.1% ... Training loss: 0.059 ... Validation loss: 0.194\r",
      "Progress: 74.1% ... Training loss: 0.060 ... Validation loss: 0.184\r",
      "Progress: 74.1% ... Training loss: 0.061 ... Validation loss: 0.214\r",
      "Progress: 74.1% ... Training loss: 0.059 ... Validation loss: 0.182\r",
      "Progress: 74.1% ... Training loss: 0.058 ... Validation loss: 0.196\r",
      "Progress: 74.1% ... Training loss: 0.057 ... Validation loss: 0.180\r",
      "Progress: 74.1% ... Training loss: 0.068 ... Validation loss: 0.171\r",
      "Progress: 74.2% ... Training loss: 0.058 ... Validation loss: 0.199\r",
      "Progress: 74.2% ... Training loss: 0.068 ... Validation loss: 0.165\r",
      "Progress: 74.2% ... Training loss: 0.079 ... Validation loss: 0.228\r",
      "Progress: 74.2% ... Training loss: 0.069 ... Validation loss: 0.162\r",
      "Progress: 74.2% ... Training loss: 0.077 ... Validation loss: 0.241\r",
      "Progress: 74.2% ... Training loss: 0.092 ... Validation loss: 0.161\r",
      "Progress: 74.2% ... Training loss: 0.070 ... Validation loss: 0.254\r",
      "Progress: 74.2% ... Training loss: 0.075 ... Validation loss: 0.173\r",
      "Progress: 74.2% ... Training loss: 0.063 ... Validation loss: 0.207\r",
      "Progress: 74.3% ... Training loss: 0.063 ... Validation loss: 0.166\r",
      "Progress: 74.3% ... Training loss: 0.069 ... Validation loss: 0.189\r",
      "Progress: 74.3% ... Training loss: 0.064 ... Validation loss: 0.168\r",
      "Progress: 74.3% ... Training loss: 0.063 ... Validation loss: 0.197\r",
      "Progress: 74.3% ... Training loss: 0.063 ... Validation loss: 0.160\r",
      "Progress: 74.3% ... Training loss: 0.061 ... Validation loss: 0.193\r",
      "Progress: 74.3% ... Training loss: 0.062 ... Validation loss: 0.161\r",
      "Progress: 74.3% ... Training loss: 0.059 ... Validation loss: 0.187\r",
      "Progress: 74.4% ... Training loss: 0.058 ... Validation loss: 0.166\r",
      "Progress: 74.4% ... Training loss: 0.059 ... Validation loss: 0.182\r",
      "Progress: 74.4% ... Training loss: 0.072 ... Validation loss: 0.160\r",
      "Progress: 74.4% ... Training loss: 0.065 ... Validation loss: 0.197\r",
      "Progress: 74.4% ... Training loss: 0.070 ... Validation loss: 0.165\r",
      "Progress: 74.4% ... Training loss: 0.058 ... Validation loss: 0.170\r",
      "Progress: 74.4% ... Training loss: 0.058 ... Validation loss: 0.171\r",
      "Progress: 74.5% ... Training loss: 0.060 ... Validation loss: 0.192\r",
      "Progress: 74.5% ... Training loss: 0.058 ... Validation loss: 0.180\r",
      "Progress: 74.5% ... Training loss: 0.072 ... Validation loss: 0.168\r",
      "Progress: 74.5% ... Training loss: 0.059 ... Validation loss: 0.205\r",
      "Progress: 74.5% ... Training loss: 0.059 ... Validation loss: 0.188\r",
      "Progress: 74.5% ... Training loss: 0.059 ... Validation loss: 0.199\r",
      "Progress: 74.5% ... Training loss: 0.059 ... Validation loss: 0.205\r",
      "Progress: 74.5% ... Training loss: 0.061 ... Validation loss: 0.195\r",
      "Progress: 74.5% ... Training loss: 0.057 ... Validation loss: 0.180\r",
      "Progress: 74.6% ... Training loss: 0.057 ... Validation loss: 0.181\r",
      "Progress: 74.6% ... Training loss: 0.060 ... Validation loss: 0.207\r",
      "Progress: 74.6% ... Training loss: 0.058 ... Validation loss: 0.184\r",
      "Progress: 74.6% ... Training loss: 0.061 ... Validation loss: 0.220\r",
      "Progress: 74.6% ... Training loss: 0.058 ... Validation loss: 0.191\r",
      "Progress: 74.6% ... Training loss: 0.060 ... Validation loss: 0.204\r",
      "Progress: 74.6% ... Training loss: 0.060 ... Validation loss: 0.199\r",
      "Progress: 74.7% ... Training loss: 0.058 ... Validation loss: 0.213\r",
      "Progress: 74.7% ... Training loss: 0.057 ... Validation loss: 0.208\r",
      "Progress: 74.7% ... Training loss: 0.057 ... Validation loss: 0.203\r",
      "Progress: 74.7% ... Training loss: 0.062 ... Validation loss: 0.185\r",
      "Progress: 74.7% ... Training loss: 0.073 ... Validation loss: 0.259\r",
      "Progress: 74.7% ... Training loss: 0.059 ... Validation loss: 0.205\r",
      "Progress: 74.7% ... Training loss: 0.059 ... Validation loss: 0.209\r",
      "Progress: 74.7% ... Training loss: 0.064 ... Validation loss: 0.180\r",
      "Progress: 74.8% ... Training loss: 0.060 ... Validation loss: 0.224\r",
      "Progress: 74.8% ... Training loss: 0.070 ... Validation loss: 0.173\r",
      "Progress: 74.8% ... Training loss: 0.063 ... Validation loss: 0.225\r",
      "Progress: 74.8% ... Training loss: 0.062 ... Validation loss: 0.167\r",
      "Progress: 74.8% ... Training loss: 0.062 ... Validation loss: 0.210\r",
      "Progress: 74.8% ... Training loss: 0.060 ... Validation loss: 0.172\r",
      "Progress: 74.8% ... Training loss: 0.063 ... Validation loss: 0.212\r",
      "Progress: 74.8% ... Training loss: 0.060 ... Validation loss: 0.172\r",
      "Progress: 74.8% ... Training loss: 0.061 ... Validation loss: 0.209\r",
      "Progress: 74.9% ... Training loss: 0.059 ... Validation loss: 0.173\r",
      "Progress: 74.9% ... Training loss: 0.059 ... Validation loss: 0.194\r",
      "Progress: 74.9% ... Training loss: 0.059 ... Validation loss: 0.178\r",
      "Progress: 74.9% ... Training loss: 0.060 ... Validation loss: 0.203\r",
      "Progress: 74.9% ... Training loss: 0.059 ... Validation loss: 0.189\r",
      "Progress: 74.9% ... Training loss: 0.060 ... Validation loss: 0.219\r",
      "Progress: 74.9% ... Training loss: 0.062 ... Validation loss: 0.176\r",
      "Progress: 75.0% ... Training loss: 0.060 ... Validation loss: 0.210\r",
      "Progress: 75.0% ... Training loss: 0.065 ... Validation loss: 0.172\r",
      "Progress: 75.0% ... Training loss: 0.060 ... Validation loss: 0.210\r",
      "Progress: 75.0% ... Training loss: 0.059 ... Validation loss: 0.190\r",
      "Progress: 75.0% ... Training loss: 0.058 ... Validation loss: 0.206\r",
      "Progress: 75.0% ... Training loss: 0.058 ... Validation loss: 0.192\r",
      "Progress: 75.0% ... Training loss: 0.058 ... Validation loss: 0.183\r",
      "Progress: 75.0% ... Training loss: 0.058 ... Validation loss: 0.183\r",
      "Progress: 75.0% ... Training loss: 0.058 ... Validation loss: 0.208\r",
      "Progress: 75.1% ... Training loss: 0.059 ... Validation loss: 0.210\r",
      "Progress: 75.1% ... Training loss: 0.058 ... Validation loss: 0.197\r",
      "Progress: 75.1% ... Training loss: 0.059 ... Validation loss: 0.211\r",
      "Progress: 75.1% ... Training loss: 0.058 ... Validation loss: 0.214\r",
      "Progress: 75.1% ... Training loss: 0.060 ... Validation loss: 0.191\r",
      "Progress: 75.1% ... Training loss: 0.059 ... Validation loss: 0.222\r",
      "Progress: 75.1% ... Training loss: 0.061 ... Validation loss: 0.179\r",
      "Progress: 75.2% ... Training loss: 0.067 ... Validation loss: 0.217\r",
      "Progress: 75.2% ... Training loss: 0.068 ... Validation loss: 0.174\r",
      "Progress: 75.2% ... Training loss: 0.078 ... Validation loss: 0.246\r",
      "Progress: 75.2% ... Training loss: 0.084 ... Validation loss: 0.160\r",
      "Progress: 75.2% ... Training loss: 0.078 ... Validation loss: 0.267\r",
      "Progress: 75.2% ... Training loss: 0.068 ... Validation loss: 0.175\r",
      "Progress: 75.2% ... Training loss: 0.075 ... Validation loss: 0.244\r",
      "Progress: 75.2% ... Training loss: 0.082 ... Validation loss: 0.164\r",
      "Progress: 75.2% ... Training loss: 0.072 ... Validation loss: 0.252\r",
      "Progress: 75.3% ... Training loss: 0.060 ... Validation loss: 0.170\r",
      "Progress: 75.3% ... Training loss: 0.058 ... Validation loss: 0.175\r",
      "Progress: 75.3% ... Training loss: 0.062 ... Validation loss: 0.175\r",
      "Progress: 75.3% ... Training loss: 0.064 ... Validation loss: 0.222\r",
      "Progress: 75.3% ... Training loss: 0.064 ... Validation loss: 0.175\r",
      "Progress: 75.3% ... Training loss: 0.060 ... Validation loss: 0.222\r",
      "Progress: 75.3% ... Training loss: 0.058 ... Validation loss: 0.187\r",
      "Progress: 75.3% ... Training loss: 0.058 ... Validation loss: 0.202\r",
      "Progress: 75.4% ... Training loss: 0.057 ... Validation loss: 0.191\r",
      "Progress: 75.4% ... Training loss: 0.058 ... Validation loss: 0.201\r",
      "Progress: 75.4% ... Training loss: 0.059 ... Validation loss: 0.201\r",
      "Progress: 75.4% ... Training loss: 0.058 ... Validation loss: 0.185\r",
      "Progress: 75.4% ... Training loss: 0.064 ... Validation loss: 0.217\r",
      "Progress: 75.4% ... Training loss: 0.062 ... Validation loss: 0.166\r",
      "Progress: 75.4% ... Training loss: 0.059 ... Validation loss: 0.198\r",
      "Progress: 75.5% ... Training loss: 0.065 ... Validation loss: 0.169\r",
      "Progress: 75.5% ... Training loss: 0.086 ... Validation loss: 0.255\r",
      "Progress: 75.5% ... Training loss: 0.092 ... Validation loss: 0.156\r",
      "Progress: 75.5% ... Training loss: 0.090 ... Validation loss: 0.241\r",
      "Progress: 75.5% ... Training loss: 0.069 ... Validation loss: 0.157\r",
      "Progress: 75.5% ... Training loss: 0.065 ... Validation loss: 0.190\r",
      "Progress: 75.5% ... Training loss: 0.081 ... Validation loss: 0.156\r",
      "Progress: 75.5% ... Training loss: 0.067 ... Validation loss: 0.208\r",
      "Progress: 75.5% ... Training loss: 0.067 ... Validation loss: 0.165\r",
      "Progress: 75.6% ... Training loss: 0.058 ... Validation loss: 0.175\r",
      "Progress: 75.6% ... Training loss: 0.057 ... Validation loss: 0.195\r",
      "Progress: 75.6% ... Training loss: 0.065 ... Validation loss: 0.217\r",
      "Progress: 75.6% ... Training loss: 0.061 ... Validation loss: 0.167\r",
      "Progress: 75.6% ... Training loss: 0.069 ... Validation loss: 0.213\r",
      "Progress: 75.6% ... Training loss: 0.068 ... Validation loss: 0.164\r",
      "Progress: 75.6% ... Training loss: 0.105 ... Validation loss: 0.257\r",
      "Progress: 75.7% ... Training loss: 0.098 ... Validation loss: 0.162\r",
      "Progress: 75.7% ... Training loss: 0.099 ... Validation loss: 0.250\r",
      "Progress: 75.7% ... Training loss: 0.073 ... Validation loss: 0.165\r",
      "Progress: 75.7% ... Training loss: 0.105 ... Validation loss: 0.263\r",
      "Progress: 75.7% ... Training loss: 0.075 ... Validation loss: 0.168\r",
      "Progress: 75.7% ... Training loss: 0.062 ... Validation loss: 0.206\r",
      "Progress: 75.7% ... Training loss: 0.059 ... Validation loss: 0.165\r",
      "Progress: 75.7% ... Training loss: 0.058 ... Validation loss: 0.175\r",
      "Progress: 75.8% ... Training loss: 0.059 ... Validation loss: 0.182\r",
      "Progress: 75.8% ... Training loss: 0.061 ... Validation loss: 0.168\r",
      "Progress: 75.8% ... Training loss: 0.062 ... Validation loss: 0.192\r",
      "Progress: 75.8% ... Training loss: 0.063 ... Validation loss: 0.157\r",
      "Progress: 75.8% ... Training loss: 0.059 ... Validation loss: 0.178\r",
      "Progress: 75.8% ... Training loss: 0.063 ... Validation loss: 0.163\r",
      "Progress: 75.8% ... Training loss: 0.057 ... Validation loss: 0.177\r",
      "Progress: 75.8% ... Training loss: 0.057 ... Validation loss: 0.170\r",
      "Progress: 75.8% ... Training loss: 0.065 ... Validation loss: 0.209\r",
      "Progress: 75.9% ... Training loss: 0.065 ... Validation loss: 0.165\r",
      "Progress: 75.9% ... Training loss: 0.080 ... Validation loss: 0.235\r",
      "Progress: 75.9% ... Training loss: 0.067 ... Validation loss: 0.167\r",
      "Progress: 75.9% ... Training loss: 0.058 ... Validation loss: 0.185\r",
      "Progress: 75.9% ... Training loss: 0.058 ... Validation loss: 0.172\r",
      "Progress: 75.9% ... Training loss: 0.059 ... Validation loss: 0.189\r",
      "Progress: 75.9% ... Training loss: 0.061 ... Validation loss: 0.164\r",
      "Progress: 76.0% ... Training loss: 0.060 ... Validation loss: 0.200\r",
      "Progress: 76.0% ... Training loss: 0.058 ... Validation loss: 0.193\r",
      "Progress: 76.0% ... Training loss: 0.058 ... Validation loss: 0.192\r",
      "Progress: 76.0% ... Training loss: 0.065 ... Validation loss: 0.171\r",
      "Progress: 76.0% ... Training loss: 0.060 ... Validation loss: 0.212\r",
      "Progress: 76.0% ... Training loss: 0.059 ... Validation loss: 0.190\r",
      "Progress: 76.0% ... Training loss: 0.058 ... Validation loss: 0.193\r",
      "Progress: 76.0% ... Training loss: 0.065 ... Validation loss: 0.227\r",
      "Progress: 76.0% ... Training loss: 0.061 ... Validation loss: 0.172\r",
      "Progress: 76.1% ... Training loss: 0.064 ... Validation loss: 0.208\r",
      "Progress: 76.1% ... Training loss: 0.058 ... Validation loss: 0.189\r",
      "Progress: 76.1% ... Training loss: 0.058 ... Validation loss: 0.180\r",
      "Progress: 76.1% ... Training loss: 0.069 ... Validation loss: 0.166\r",
      "Progress: 76.1% ... Training loss: 0.076 ... Validation loss: 0.250\r",
      "Progress: 76.1% ... Training loss: 0.059 ... Validation loss: 0.173\r",
      "Progress: 76.1% ... Training loss: 0.062 ... Validation loss: 0.199\r",
      "Progress: 76.2% ... Training loss: 0.060 ... Validation loss: 0.171\r",
      "Progress: 76.2% ... Training loss: 0.057 ... Validation loss: 0.184\r",
      "Progress: 76.2% ... Training loss: 0.058 ... Validation loss: 0.195\r",
      "Progress: 76.2% ... Training loss: 0.065 ... Validation loss: 0.170\r",
      "Progress: 76.2% ... Training loss: 0.063 ... Validation loss: 0.211\r",
      "Progress: 76.2% ... Training loss: 0.057 ... Validation loss: 0.180\r",
      "Progress: 76.2% ... Training loss: 0.065 ... Validation loss: 0.213\r",
      "Progress: 76.2% ... Training loss: 0.068 ... Validation loss: 0.175\r",
      "Progress: 76.2% ... Training loss: 0.069 ... Validation loss: 0.242\r",
      "Progress: 76.3% ... Training loss: 0.069 ... Validation loss: 0.164\r",
      "Progress: 76.3% ... Training loss: 0.073 ... Validation loss: 0.221\r",
      "Progress: 76.3% ... Training loss: 0.061 ... Validation loss: 0.171\r",
      "Progress: 76.3% ... Training loss: 0.065 ... Validation loss: 0.217\r",
      "Progress: 76.3% ... Training loss: 0.068 ... Validation loss: 0.167\r",
      "Progress: 76.3% ... Training loss: 0.072 ... Validation loss: 0.216\r",
      "Progress: 76.3% ... Training loss: 0.075 ... Validation loss: 0.164\r",
      "Progress: 76.3% ... Training loss: 0.076 ... Validation loss: 0.215\r",
      "Progress: 76.4% ... Training loss: 0.070 ... Validation loss: 0.154\r",
      "Progress: 76.4% ... Training loss: 0.064 ... Validation loss: 0.188\r",
      "Progress: 76.4% ... Training loss: 0.062 ... Validation loss: 0.161\r",
      "Progress: 76.4% ... Training loss: 0.064 ... Validation loss: 0.211\r",
      "Progress: 76.4% ... Training loss: 0.062 ... Validation loss: 0.165\r",
      "Progress: 76.4% ... Training loss: 0.066 ... Validation loss: 0.217\r",
      "Progress: 76.4% ... Training loss: 0.063 ... Validation loss: 0.170\r",
      "Progress: 76.5% ... Training loss: 0.061 ... Validation loss: 0.206\r",
      "Progress: 76.5% ... Training loss: 0.072 ... Validation loss: 0.162\r",
      "Progress: 76.5% ... Training loss: 0.082 ... Validation loss: 0.237\r",
      "Progress: 76.5% ... Training loss: 0.069 ... Validation loss: 0.172\r",
      "Progress: 76.5% ... Training loss: 0.076 ... Validation loss: 0.238\r",
      "Progress: 76.5% ... Training loss: 0.062 ... Validation loss: 0.173\r",
      "Progress: 76.5% ... Training loss: 0.058 ... Validation loss: 0.192\r",
      "Progress: 76.5% ... Training loss: 0.062 ... Validation loss: 0.175\r",
      "Progress: 76.5% ... Training loss: 0.058 ... Validation loss: 0.196\r",
      "Progress: 76.6% ... Training loss: 0.059 ... Validation loss: 0.181\r",
      "Progress: 76.6% ... Training loss: 0.060 ... Validation loss: 0.187\r",
      "Progress: 76.6% ... Training loss: 0.057 ... Validation loss: 0.187\r",
      "Progress: 76.6% ... Training loss: 0.065 ... Validation loss: 0.214\r",
      "Progress: 76.6% ... Training loss: 0.071 ... Validation loss: 0.161\r",
      "Progress: 76.6% ... Training loss: 0.065 ... Validation loss: 0.203\r",
      "Progress: 76.6% ... Training loss: 0.057 ... Validation loss: 0.171\r",
      "Progress: 76.7% ... Training loss: 0.057 ... Validation loss: 0.191\r",
      "Progress: 76.7% ... Training loss: 0.059 ... Validation loss: 0.170\r",
      "Progress: 76.7% ... Training loss: 0.058 ... Validation loss: 0.191\r",
      "Progress: 76.7% ... Training loss: 0.060 ... Validation loss: 0.170\r",
      "Progress: 76.7% ... Training loss: 0.069 ... Validation loss: 0.215\r",
      "Progress: 76.7% ... Training loss: 0.062 ... Validation loss: 0.167\r",
      "Progress: 76.7% ... Training loss: 0.058 ... Validation loss: 0.192\r",
      "Progress: 76.7% ... Training loss: 0.057 ... Validation loss: 0.203\r",
      "Progress: 76.8% ... Training loss: 0.063 ... Validation loss: 0.180\r",
      "Progress: 76.8% ... Training loss: 0.063 ... Validation loss: 0.221\r",
      "Progress: 76.8% ... Training loss: 0.069 ... Validation loss: 0.161\r",
      "Progress: 76.8% ... Training loss: 0.093 ... Validation loss: 0.262\r",
      "Progress: 76.8% ... Training loss: 0.087 ... Validation loss: 0.157\r",
      "Progress: 76.8% ... Training loss: 0.094 ... Validation loss: 0.272\r",
      "Progress: 76.8% ... Training loss: 0.093 ... Validation loss: 0.162\r",
      "Progress: 76.8% ... Training loss: 0.090 ... Validation loss: 0.279\r",
      "Progress: 76.8% ... Training loss: 0.071 ... Validation loss: 0.175\r",
      "Progress: 76.9% ... Training loss: 0.063 ... Validation loss: 0.220\r",
      "Progress: 76.9% ... Training loss: 0.070 ... Validation loss: 0.169\r",
      "Progress: 76.9% ... Training loss: 0.066 ... Validation loss: 0.225\r",
      "Progress: 76.9% ... Training loss: 0.075 ... Validation loss: 0.162\r",
      "Progress: 76.9% ... Training loss: 0.086 ... Validation loss: 0.251\r",
      "Progress: 76.9% ... Training loss: 0.101 ... Validation loss: 0.157\r",
      "Progress: 76.9% ... Training loss: 0.087 ... Validation loss: 0.258\r",
      "Progress: 77.0% ... Training loss: 0.083 ... Validation loss: 0.163\r",
      "Progress: 77.0% ... Training loss: 0.092 ... Validation loss: 0.275\r",
      "Progress: 77.0% ... Training loss: 0.100 ... Validation loss: 0.158\r",
      "Progress: 77.0% ... Training loss: 0.107 ... Validation loss: 0.274\r",
      "Progress: 77.0% ... Training loss: 0.106 ... Validation loss: 0.153\r",
      "Progress: 77.0% ... Training loss: 0.075 ... Validation loss: 0.229\r",
      "Progress: 77.0% ... Training loss: 0.079 ... Validation loss: 0.152\r",
      "Progress: 77.0% ... Training loss: 0.064 ... Validation loss: 0.198\r",
      "Progress: 77.0% ... Training loss: 0.062 ... Validation loss: 0.166\r",
      "Progress: 77.1% ... Training loss: 0.059 ... Validation loss: 0.179\r",
      "Progress: 77.1% ... Training loss: 0.059 ... Validation loss: 0.167\r",
      "Progress: 77.1% ... Training loss: 0.060 ... Validation loss: 0.163\r",
      "Progress: 77.1% ... Training loss: 0.058 ... Validation loss: 0.177\r",
      "Progress: 77.1% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 77.1% ... Training loss: 0.058 ... Validation loss: 0.200\r",
      "Progress: 77.1% ... Training loss: 0.058 ... Validation loss: 0.178\r",
      "Progress: 77.2% ... Training loss: 0.061 ... Validation loss: 0.184\r",
      "Progress: 77.2% ... Training loss: 0.065 ... Validation loss: 0.154\r",
      "Progress: 77.2% ... Training loss: 0.059 ... Validation loss: 0.174\r",
      "Progress: 77.2% ... Training loss: 0.059 ... Validation loss: 0.165\r",
      "Progress: 77.2% ... Training loss: 0.057 ... Validation loss: 0.173\r",
      "Progress: 77.2% ... Training loss: 0.060 ... Validation loss: 0.200\r",
      "Progress: 77.2% ... Training loss: 0.060 ... Validation loss: 0.171\r",
      "Progress: 77.2% ... Training loss: 0.061 ... Validation loss: 0.209\r",
      "Progress: 77.2% ... Training loss: 0.060 ... Validation loss: 0.167\r",
      "Progress: 77.3% ... Training loss: 0.058 ... Validation loss: 0.179\r",
      "Progress: 77.3% ... Training loss: 0.058 ... Validation loss: 0.176\r",
      "Progress: 77.3% ... Training loss: 0.062 ... Validation loss: 0.196\r",
      "Progress: 77.3% ... Training loss: 0.061 ... Validation loss: 0.188\r",
      "Progress: 77.3% ... Training loss: 0.063 ... Validation loss: 0.221\r",
      "Progress: 77.3% ... Training loss: 0.065 ... Validation loss: 0.180\r",
      "Progress: 77.3% ... Training loss: 0.068 ... Validation loss: 0.243\r",
      "Progress: 77.3% ... Training loss: 0.059 ... Validation loss: 0.188\r",
      "Progress: 77.4% ... Training loss: 0.060 ... Validation loss: 0.208\r",
      "Progress: 77.4% ... Training loss: 0.075 ... Validation loss: 0.168\r",
      "Progress: 77.4% ... Training loss: 0.077 ... Validation loss: 0.230\r",
      "Progress: 77.4% ... Training loss: 0.068 ... Validation loss: 0.163\r",
      "Progress: 77.4% ... Training loss: 0.059 ... Validation loss: 0.202\r",
      "Progress: 77.4% ... Training loss: 0.058 ... Validation loss: 0.188\r",
      "Progress: 77.4% ... Training loss: 0.059 ... Validation loss: 0.187\r",
      "Progress: 77.5% ... Training loss: 0.061 ... Validation loss: 0.165\r",
      "Progress: 77.5% ... Training loss: 0.062 ... Validation loss: 0.200\r",
      "Progress: 77.5% ... Training loss: 0.073 ... Validation loss: 0.153\r",
      "Progress: 77.5% ... Training loss: 0.057 ... Validation loss: 0.190\r",
      "Progress: 77.5% ... Training loss: 0.058 ... Validation loss: 0.187\r",
      "Progress: 77.5% ... Training loss: 0.058 ... Validation loss: 0.182\r",
      "Progress: 77.5% ... Training loss: 0.058 ... Validation loss: 0.191\r",
      "Progress: 77.5% ... Training loss: 0.057 ... Validation loss: 0.189\r",
      "Progress: 77.5% ... Training loss: 0.060 ... Validation loss: 0.170\r",
      "Progress: 77.6% ... Training loss: 0.058 ... Validation loss: 0.199\r",
      "Progress: 77.6% ... Training loss: 0.057 ... Validation loss: 0.188\r",
      "Progress: 77.6% ... Training loss: 0.059 ... Validation loss: 0.185\r",
      "Progress: 77.6% ... Training loss: 0.061 ... Validation loss: 0.221\r",
      "Progress: 77.6% ... Training loss: 0.057 ... Validation loss: 0.190\r",
      "Progress: 77.6% ... Training loss: 0.058 ... Validation loss: 0.187\r",
      "Progress: 77.6% ... Training loss: 0.059 ... Validation loss: 0.191\r",
      "Progress: 77.7% ... Training loss: 0.057 ... Validation loss: 0.179\r",
      "Progress: 77.7% ... Training loss: 0.057 ... Validation loss: 0.197\r",
      "Progress: 77.7% ... Training loss: 0.058 ... Validation loss: 0.195\r",
      "Progress: 77.7% ... Training loss: 0.062 ... Validation loss: 0.206\r",
      "Progress: 77.7% ... Training loss: 0.059 ... Validation loss: 0.184\r",
      "Progress: 77.7% ... Training loss: 0.059 ... Validation loss: 0.195\r",
      "Progress: 77.7% ... Training loss: 0.058 ... Validation loss: 0.176\r",
      "Progress: 77.7% ... Training loss: 0.057 ... Validation loss: 0.187\r",
      "Progress: 77.8% ... Training loss: 0.059 ... Validation loss: 0.171\r",
      "Progress: 77.8% ... Training loss: 0.062 ... Validation loss: 0.203\r",
      "Progress: 77.8% ... Training loss: 0.062 ... Validation loss: 0.166\r",
      "Progress: 77.8% ... Training loss: 0.060 ... Validation loss: 0.206\r",
      "Progress: 77.8% ... Training loss: 0.059 ... Validation loss: 0.174\r",
      "Progress: 77.8% ... Training loss: 0.059 ... Validation loss: 0.202\r",
      "Progress: 77.8% ... Training loss: 0.062 ... Validation loss: 0.164\r",
      "Progress: 77.8% ... Training loss: 0.063 ... Validation loss: 0.202\r",
      "Progress: 77.8% ... Training loss: 0.062 ... Validation loss: 0.168\r",
      "Progress: 77.9% ... Training loss: 0.060 ... Validation loss: 0.196\r",
      "Progress: 77.9% ... Training loss: 0.062 ... Validation loss: 0.172\r",
      "Progress: 77.9% ... Training loss: 0.070 ... Validation loss: 0.213\r",
      "Progress: 77.9% ... Training loss: 0.063 ... Validation loss: 0.171\r",
      "Progress: 77.9% ... Training loss: 0.058 ... Validation loss: 0.190\r",
      "Progress: 77.9% ... Training loss: 0.066 ... Validation loss: 0.177\r",
      "Progress: 77.9% ... Training loss: 0.058 ... Validation loss: 0.202\r",
      "Progress: 78.0% ... Training loss: 0.057 ... Validation loss: 0.179\r",
      "Progress: 78.0% ... Training loss: 0.063 ... Validation loss: 0.214\r",
      "Progress: 78.0% ... Training loss: 0.058 ... Validation loss: 0.199\r",
      "Progress: 78.0% ... Training loss: 0.057 ... Validation loss: 0.197\r",
      "Progress: 78.0% ... Training loss: 0.057 ... Validation loss: 0.189\r",
      "Progress: 78.0% ... Training loss: 0.059 ... Validation loss: 0.208\r",
      "Progress: 78.0% ... Training loss: 0.059 ... Validation loss: 0.183\r",
      "Progress: 78.0% ... Training loss: 0.057 ... Validation loss: 0.195\r",
      "Progress: 78.0% ... Training loss: 0.060 ... Validation loss: 0.217\r",
      "Progress: 78.1% ... Training loss: 0.069 ... Validation loss: 0.175\r",
      "Progress: 78.1% ... Training loss: 0.066 ... Validation loss: 0.217\r",
      "Progress: 78.1% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 78.1% ... Training loss: 0.064 ... Validation loss: 0.220\r",
      "Progress: 78.1% ... Training loss: 0.061 ... Validation loss: 0.169\r",
      "Progress: 78.1% ... Training loss: 0.059 ... Validation loss: 0.209\r",
      "Progress: 78.1% ... Training loss: 0.060 ... Validation loss: 0.221\r",
      "Progress: 78.2% ... Training loss: 0.065 ... Validation loss: 0.161\r",
      "Progress: 78.2% ... Training loss: 0.060 ... Validation loss: 0.199\r",
      "Progress: 78.2% ... Training loss: 0.057 ... Validation loss: 0.183\r",
      "Progress: 78.2% ... Training loss: 0.057 ... Validation loss: 0.182\r",
      "Progress: 78.2% ... Training loss: 0.059 ... Validation loss: 0.171\r",
      "Progress: 78.2% ... Training loss: 0.058 ... Validation loss: 0.177\r",
      "Progress: 78.2% ... Training loss: 0.057 ... Validation loss: 0.175\r",
      "Progress: 78.2% ... Training loss: 0.059 ... Validation loss: 0.206\r",
      "Progress: 78.2% ... Training loss: 0.066 ... Validation loss: 0.169\r",
      "Progress: 78.3% ... Training loss: 0.070 ... Validation loss: 0.223\r",
      "Progress: 78.3% ... Training loss: 0.062 ... Validation loss: 0.178\r",
      "Progress: 78.3% ... Training loss: 0.057 ... Validation loss: 0.187\r",
      "Progress: 78.3% ... Training loss: 0.059 ... Validation loss: 0.200\r",
      "Progress: 78.3% ... Training loss: 0.058 ... Validation loss: 0.195\r",
      "Progress: 78.3% ... Training loss: 0.057 ... Validation loss: 0.196\r",
      "Progress: 78.3% ... Training loss: 0.059 ... Validation loss: 0.196\r",
      "Progress: 78.3% ... Training loss: 0.067 ... Validation loss: 0.165\r",
      "Progress: 78.4% ... Training loss: 0.059 ... Validation loss: 0.208\r",
      "Progress: 78.4% ... Training loss: 0.059 ... Validation loss: 0.179\r",
      "Progress: 78.4% ... Training loss: 0.057 ... Validation loss: 0.183\r",
      "Progress: 78.4% ... Training loss: 0.059 ... Validation loss: 0.186\r",
      "Progress: 78.4% ... Training loss: 0.060 ... Validation loss: 0.207\r",
      "Progress: 78.4% ... Training loss: 0.059 ... Validation loss: 0.188\r",
      "Progress: 78.4% ... Training loss: 0.058 ... Validation loss: 0.210\r",
      "Progress: 78.5% ... Training loss: 0.058 ... Validation loss: 0.181\r",
      "Progress: 78.5% ... Training loss: 0.058 ... Validation loss: 0.177\r",
      "Progress: 78.5% ... Training loss: 0.061 ... Validation loss: 0.210\r",
      "Progress: 78.5% ... Training loss: 0.059 ... Validation loss: 0.181\r",
      "Progress: 78.5% ... Training loss: 0.059 ... Validation loss: 0.197\r",
      "Progress: 78.5% ... Training loss: 0.059 ... Validation loss: 0.178\r",
      "Progress: 78.5% ... Training loss: 0.057 ... Validation loss: 0.188\r",
      "Progress: 78.5% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 78.5% ... Training loss: 0.059 ... Validation loss: 0.203\r",
      "Progress: 78.6% ... Training loss: 0.057 ... Validation loss: 0.187\r",
      "Progress: 78.6% ... Training loss: 0.062 ... Validation loss: 0.169\r",
      "Progress: 78.6% ... Training loss: 0.068 ... Validation loss: 0.225\r",
      "Progress: 78.6% ... Training loss: 0.076 ... Validation loss: 0.164\r",
      "Progress: 78.6% ... Training loss: 0.070 ... Validation loss: 0.220\r",
      "Progress: 78.6% ... Training loss: 0.064 ... Validation loss: 0.166\r",
      "Progress: 78.6% ... Training loss: 0.060 ... Validation loss: 0.197\r",
      "Progress: 78.7% ... Training loss: 0.069 ... Validation loss: 0.162\r",
      "Progress: 78.7% ... Training loss: 0.068 ... Validation loss: 0.212\r",
      "Progress: 78.7% ... Training loss: 0.092 ... Validation loss: 0.158\r",
      "Progress: 78.7% ... Training loss: 0.077 ... Validation loss: 0.245\r",
      "Progress: 78.7% ... Training loss: 0.071 ... Validation loss: 0.160\r",
      "Progress: 78.7% ... Training loss: 0.069 ... Validation loss: 0.217\r",
      "Progress: 78.7% ... Training loss: 0.059 ... Validation loss: 0.184\r",
      "Progress: 78.7% ... Training loss: 0.062 ... Validation loss: 0.221\r",
      "Progress: 78.8% ... Training loss: 0.056 ... Validation loss: 0.191\r",
      "Progress: 78.8% ... Training loss: 0.057 ... Validation loss: 0.188\r",
      "Progress: 78.8% ... Training loss: 0.057 ... Validation loss: 0.188\r",
      "Progress: 78.8% ... Training loss: 0.058 ... Validation loss: 0.207\r",
      "Progress: 78.8% ... Training loss: 0.057 ... Validation loss: 0.194\r",
      "Progress: 78.8% ... Training loss: 0.056 ... Validation loss: 0.185\r",
      "Progress: 78.8% ... Training loss: 0.062 ... Validation loss: 0.182\r",
      "Progress: 78.8% ... Training loss: 0.063 ... Validation loss: 0.229\r",
      "Progress: 78.8% ... Training loss: 0.057 ... Validation loss: 0.190\r",
      "Progress: 78.9% ... Training loss: 0.056 ... Validation loss: 0.196\r",
      "Progress: 78.9% ... Training loss: 0.057 ... Validation loss: 0.176\r",
      "Progress: 78.9% ... Training loss: 0.065 ... Validation loss: 0.214\r",
      "Progress: 78.9% ... Training loss: 0.062 ... Validation loss: 0.171\r",
      "Progress: 78.9% ... Training loss: 0.059 ... Validation loss: 0.207\r",
      "Progress: 78.9% ... Training loss: 0.069 ... Validation loss: 0.167\r",
      "Progress: 78.9% ... Training loss: 0.068 ... Validation loss: 0.225\r",
      "Progress: 79.0% ... Training loss: 0.062 ... Validation loss: 0.179\r",
      "Progress: 79.0% ... Training loss: 0.069 ... Validation loss: 0.238\r",
      "Progress: 79.0% ... Training loss: 0.062 ... Validation loss: 0.186\r",
      "Progress: 79.0% ... Training loss: 0.060 ... Validation loss: 0.230\r",
      "Progress: 79.0% ... Training loss: 0.062 ... Validation loss: 0.182\r",
      "Progress: 79.0% ... Training loss: 0.062 ... Validation loss: 0.217\r",
      "Progress: 79.0% ... Training loss: 0.060 ... Validation loss: 0.185\r",
      "Progress: 79.0% ... Training loss: 0.060 ... Validation loss: 0.226\r",
      "Progress: 79.0% ... Training loss: 0.057 ... Validation loss: 0.213\r",
      "Progress: 79.1% ... Training loss: 0.058 ... Validation loss: 0.211\r",
      "Progress: 79.1% ... Training loss: 0.057 ... Validation loss: 0.194\r",
      "Progress: 79.1% ... Training loss: 0.062 ... Validation loss: 0.231\r",
      "Progress: 79.1% ... Training loss: 0.061 ... Validation loss: 0.180\r",
      "Progress: 79.1% ... Training loss: 0.063 ... Validation loss: 0.213\r",
      "Progress: 79.1% ... Training loss: 0.072 ... Validation loss: 0.166\r",
      "Progress: 79.1% ... Training loss: 0.066 ... Validation loss: 0.231\r",
      "Progress: 79.2% ... Training loss: 0.061 ... Validation loss: 0.185\r",
      "Progress: 79.2% ... Training loss: 0.060 ... Validation loss: 0.209\r",
      "Progress: 79.2% ... Training loss: 0.062 ... Validation loss: 0.176\r",
      "Progress: 79.2% ... Training loss: 0.058 ... Validation loss: 0.213\r",
      "Progress: 79.2% ... Training loss: 0.062 ... Validation loss: 0.180\r",
      "Progress: 79.2% ... Training loss: 0.059 ... Validation loss: 0.228\r",
      "Progress: 79.2% ... Training loss: 0.059 ... Validation loss: 0.193\r",
      "Progress: 79.2% ... Training loss: 0.057 ... Validation loss: 0.202\r",
      "Progress: 79.2% ... Training loss: 0.058 ... Validation loss: 0.191\r",
      "Progress: 79.3% ... Training loss: 0.058 ... Validation loss: 0.217\r",
      "Progress: 79.3% ... Training loss: 0.059 ... Validation loss: 0.210\r",
      "Progress: 79.3% ... Training loss: 0.057 ... Validation loss: 0.174\r",
      "Progress: 79.3% ... Training loss: 0.058 ... Validation loss: 0.202\r",
      "Progress: 79.3% ... Training loss: 0.059 ... Validation loss: 0.172\r",
      "Progress: 79.3% ... Training loss: 0.063 ... Validation loss: 0.204\r",
      "Progress: 79.3% ... Training loss: 0.061 ... Validation loss: 0.169\r",
      "Progress: 79.3% ... Training loss: 0.060 ... Validation loss: 0.198\r",
      "Progress: 79.4% ... Training loss: 0.058 ... Validation loss: 0.180\r",
      "Progress: 79.4% ... Training loss: 0.059 ... Validation loss: 0.205\r",
      "Progress: 79.4% ... Training loss: 0.057 ... Validation loss: 0.183\r",
      "Progress: 79.4% ... Training loss: 0.062 ... Validation loss: 0.207\r",
      "Progress: 79.4% ... Training loss: 0.058 ... Validation loss: 0.183\r",
      "Progress: 79.4% ... Training loss: 0.061 ... Validation loss: 0.219\r",
      "Progress: 79.4% ... Training loss: 0.070 ... Validation loss: 0.179\r",
      "Progress: 79.5% ... Training loss: 0.058 ... Validation loss: 0.214\r",
      "Progress: 79.5% ... Training loss: 0.056 ... Validation loss: 0.194\r",
      "Progress: 79.5% ... Training loss: 0.062 ... Validation loss: 0.224\r",
      "Progress: 79.5% ... Training loss: 0.065 ... Validation loss: 0.176\r",
      "Progress: 79.5% ... Training loss: 0.077 ... Validation loss: 0.265\r",
      "Progress: 79.5% ... Training loss: 0.063 ... Validation loss: 0.182\r",
      "Progress: 79.5% ... Training loss: 0.064 ... Validation loss: 0.239\r",
      "Progress: 79.5% ... Training loss: 0.059 ... Validation loss: 0.173\r",
      "Progress: 79.5% ... Training loss: 0.061 ... Validation loss: 0.200\r",
      "Progress: 79.6% ... Training loss: 0.059 ... Validation loss: 0.172\r",
      "Progress: 79.6% ... Training loss: 0.064 ... Validation loss: 0.208\r",
      "Progress: 79.6% ... Training loss: 0.057 ... Validation loss: 0.186\r",
      "Progress: 79.6% ... Training loss: 0.077 ... Validation loss: 0.237\r",
      "Progress: 79.6% ... Training loss: 0.073 ... Validation loss: 0.170\r",
      "Progress: 79.6% ... Training loss: 0.075 ... Validation loss: 0.245\r",
      "Progress: 79.6% ... Training loss: 0.077 ... Validation loss: 0.162\r",
      "Progress: 79.7% ... Training loss: 0.071 ... Validation loss: 0.216\r",
      "Progress: 79.7% ... Training loss: 0.072 ... Validation loss: 0.164\r",
      "Progress: 79.7% ... Training loss: 0.068 ... Validation loss: 0.231\r",
      "Progress: 79.7% ... Training loss: 0.074 ... Validation loss: 0.160\r",
      "Progress: 79.7% ... Training loss: 0.079 ... Validation loss: 0.236\r",
      "Progress: 79.7% ... Training loss: 0.072 ... Validation loss: 0.162\r",
      "Progress: 79.7% ... Training loss: 0.092 ... Validation loss: 0.252\r",
      "Progress: 79.7% ... Training loss: 0.073 ... Validation loss: 0.169\r",
      "Progress: 79.8% ... Training loss: 0.066 ... Validation loss: 0.243\r",
      "Progress: 79.8% ... Training loss: 0.061 ... Validation loss: 0.189\r",
      "Progress: 79.8% ... Training loss: 0.058 ... Validation loss: 0.209\r",
      "Progress: 79.8% ... Training loss: 0.064 ... Validation loss: 0.175\r",
      "Progress: 79.8% ... Training loss: 0.057 ... Validation loss: 0.204\r",
      "Progress: 79.8% ... Training loss: 0.056 ... Validation loss: 0.188\r",
      "Progress: 79.8% ... Training loss: 0.057 ... Validation loss: 0.197\r",
      "Progress: 79.8% ... Training loss: 0.060 ... Validation loss: 0.184\r",
      "Progress: 79.8% ... Training loss: 0.066 ... Validation loss: 0.233\r",
      "Progress: 79.9% ... Training loss: 0.061 ... Validation loss: 0.184\r",
      "Progress: 79.9% ... Training loss: 0.061 ... Validation loss: 0.226\r",
      "Progress: 79.9% ... Training loss: 0.061 ... Validation loss: 0.172\r",
      "Progress: 79.9% ... Training loss: 0.058 ... Validation loss: 0.194\r",
      "Progress: 79.9% ... Training loss: 0.057 ... Validation loss: 0.189\r",
      "Progress: 79.9% ... Training loss: 0.058 ... Validation loss: 0.211\r",
      "Progress: 79.9% ... Training loss: 0.063 ... Validation loss: 0.187\r",
      "Progress: 80.0% ... Training loss: 0.059 ... Validation loss: 0.210\r",
      "Progress: 80.0% ... Training loss: 0.066 ... Validation loss: 0.175\r",
      "Progress: 80.0% ... Training loss: 0.074 ... Validation loss: 0.238\r",
      "Progress: 80.0% ... Training loss: 0.065 ... Validation loss: 0.169\r",
      "Progress: 80.0% ... Training loss: 0.060 ... Validation loss: 0.217\r",
      "Progress: 80.0% ... Training loss: 0.058 ... Validation loss: 0.188\r",
      "Progress: 80.0% ... Training loss: 0.056 ... Validation loss: 0.202\r",
      "Progress: 80.0% ... Training loss: 0.058 ... Validation loss: 0.188\r",
      "Progress: 80.0% ... Training loss: 0.063 ... Validation loss: 0.222\r",
      "Progress: 80.1% ... Training loss: 0.063 ... Validation loss: 0.175\r",
      "Progress: 80.1% ... Training loss: 0.057 ... Validation loss: 0.212\r",
      "Progress: 80.1% ... Training loss: 0.059 ... Validation loss: 0.181\r",
      "Progress: 80.1% ... Training loss: 0.057 ... Validation loss: 0.188\r",
      "Progress: 80.1% ... Training loss: 0.058 ... Validation loss: 0.195\r",
      "Progress: 80.1% ... Training loss: 0.057 ... Validation loss: 0.185\r",
      "Progress: 80.1% ... Training loss: 0.057 ... Validation loss: 0.198\r",
      "Progress: 80.2% ... Training loss: 0.056 ... Validation loss: 0.188\r",
      "Progress: 80.2% ... Training loss: 0.056 ... Validation loss: 0.187\r",
      "Progress: 80.2% ... Training loss: 0.059 ... Validation loss: 0.169\r",
      "Progress: 80.2% ... Training loss: 0.059 ... Validation loss: 0.180\r",
      "Progress: 80.2% ... Training loss: 0.058 ... Validation loss: 0.163\r",
      "Progress: 80.2% ... Training loss: 0.061 ... Validation loss: 0.195\r",
      "Progress: 80.2% ... Training loss: 0.057 ... Validation loss: 0.173\r",
      "Progress: 80.2% ... Training loss: 0.057 ... Validation loss: 0.176\r",
      "Progress: 80.2% ... Training loss: 0.057 ... Validation loss: 0.174\r",
      "Progress: 80.3% ... Training loss: 0.058 ... Validation loss: 0.178\r",
      "Progress: 80.3% ... Training loss: 0.060 ... Validation loss: 0.189\r",
      "Progress: 80.3% ... Training loss: 0.081 ... Validation loss: 0.159\r",
      "Progress: 80.3% ... Training loss: 0.063 ... Validation loss: 0.193\r",
      "Progress: 80.3% ... Training loss: 0.060 ... Validation loss: 0.170\r",
      "Progress: 80.3% ... Training loss: 0.057 ... Validation loss: 0.179\r",
      "Progress: 80.3% ... Training loss: 0.058 ... Validation loss: 0.190\r",
      "Progress: 80.3% ... Training loss: 0.059 ... Validation loss: 0.188\r",
      "Progress: 80.4% ... Training loss: 0.058 ... Validation loss: 0.180\r",
      "Progress: 80.4% ... Training loss: 0.064 ... Validation loss: 0.207\r",
      "Progress: 80.4% ... Training loss: 0.057 ... Validation loss: 0.183\r",
      "Progress: 80.4% ... Training loss: 0.059 ... Validation loss: 0.166\r",
      "Progress: 80.4% ... Training loss: 0.059 ... Validation loss: 0.176\r",
      "Progress: 80.4% ... Training loss: 0.063 ... Validation loss: 0.162\r",
      "Progress: 80.4% ... Training loss: 0.057 ... Validation loss: 0.185\r",
      "Progress: 80.5% ... Training loss: 0.056 ... Validation loss: 0.173\r",
      "Progress: 80.5% ... Training loss: 0.056 ... Validation loss: 0.173\r",
      "Progress: 80.5% ... Training loss: 0.063 ... Validation loss: 0.154\r",
      "Progress: 80.5% ... Training loss: 0.059 ... Validation loss: 0.172\r",
      "Progress: 80.5% ... Training loss: 0.059 ... Validation loss: 0.159\r",
      "Progress: 80.5% ... Training loss: 0.060 ... Validation loss: 0.191\r",
      "Progress: 80.5% ... Training loss: 0.066 ... Validation loss: 0.159\r",
      "Progress: 80.5% ... Training loss: 0.059 ... Validation loss: 0.189\r",
      "Progress: 80.5% ... Training loss: 0.058 ... Validation loss: 0.183\r",
      "Progress: 80.6% ... Training loss: 0.059 ... Validation loss: 0.165\r",
      "Progress: 80.6% ... Training loss: 0.057 ... Validation loss: 0.184\r",
      "Progress: 80.6% ... Training loss: 0.056 ... Validation loss: 0.184\r",
      "Progress: 80.6% ... Training loss: 0.057 ... Validation loss: 0.179\r",
      "Progress: 80.6% ... Training loss: 0.056 ... Validation loss: 0.175\r",
      "Progress: 80.6% ... Training loss: 0.065 ... Validation loss: 0.209\r",
      "Progress: 80.6% ... Training loss: 0.073 ... Validation loss: 0.163\r",
      "Progress: 80.7% ... Training loss: 0.058 ... Validation loss: 0.190\r",
      "Progress: 80.7% ... Training loss: 0.056 ... Validation loss: 0.186\r",
      "Progress: 80.7% ... Training loss: 0.056 ... Validation loss: 0.178\r",
      "Progress: 80.7% ... Training loss: 0.058 ... Validation loss: 0.182\r",
      "Progress: 80.7% ... Training loss: 0.059 ... Validation loss: 0.167\r",
      "Progress: 80.7% ... Training loss: 0.057 ... Validation loss: 0.192\r",
      "Progress: 80.7% ... Training loss: 0.058 ... Validation loss: 0.202\r",
      "Progress: 80.7% ... Training loss: 0.059 ... Validation loss: 0.184\r",
      "Progress: 80.8% ... Training loss: 0.071 ... Validation loss: 0.232\r",
      "Progress: 80.8% ... Training loss: 0.073 ... Validation loss: 0.173\r",
      "Progress: 80.8% ... Training loss: 0.066 ... Validation loss: 0.226\r",
      "Progress: 80.8% ... Training loss: 0.069 ... Validation loss: 0.163\r",
      "Progress: 80.8% ... Training loss: 0.064 ... Validation loss: 0.211\r",
      "Progress: 80.8% ... Training loss: 0.061 ... Validation loss: 0.164\r",
      "Progress: 80.8% ... Training loss: 0.061 ... Validation loss: 0.194\r",
      "Progress: 80.8% ... Training loss: 0.059 ... Validation loss: 0.174\r",
      "Progress: 80.8% ... Training loss: 0.060 ... Validation loss: 0.208\r",
      "Progress: 80.9% ... Training loss: 0.063 ... Validation loss: 0.169\r",
      "Progress: 80.9% ... Training loss: 0.065 ... Validation loss: 0.199\r",
      "Progress: 80.9% ... Training loss: 0.059 ... Validation loss: 0.157\r",
      "Progress: 80.9% ... Training loss: 0.060 ... Validation loss: 0.195\r",
      "Progress: 80.9% ... Training loss: 0.064 ... Validation loss: 0.164\r",
      "Progress: 80.9% ... Training loss: 0.079 ... Validation loss: 0.216\r",
      "Progress: 80.9% ... Training loss: 0.064 ... Validation loss: 0.161\r",
      "Progress: 81.0% ... Training loss: 0.078 ... Validation loss: 0.228\r",
      "Progress: 81.0% ... Training loss: 0.062 ... Validation loss: 0.171\r",
      "Progress: 81.0% ... Training loss: 0.063 ... Validation loss: 0.212\r",
      "Progress: 81.0% ... Training loss: 0.059 ... Validation loss: 0.170\r",
      "Progress: 81.0% ... Training loss: 0.063 ... Validation loss: 0.180\r",
      "Progress: 81.0% ... Training loss: 0.067 ... Validation loss: 0.155\r",
      "Progress: 81.0% ... Training loss: 0.068 ... Validation loss: 0.206\r",
      "Progress: 81.0% ... Training loss: 0.059 ... Validation loss: 0.167\r",
      "Progress: 81.0% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 81.1% ... Training loss: 0.057 ... Validation loss: 0.179\r",
      "Progress: 81.1% ... Training loss: 0.059 ... Validation loss: 0.168\r",
      "Progress: 81.1% ... Training loss: 0.060 ... Validation loss: 0.194\r",
      "Progress: 81.1% ... Training loss: 0.057 ... Validation loss: 0.176\r",
      "Progress: 81.1% ... Training loss: 0.058 ... Validation loss: 0.180\r",
      "Progress: 81.1% ... Training loss: 0.057 ... Validation loss: 0.183\r",
      "Progress: 81.1% ... Training loss: 0.056 ... Validation loss: 0.178\r",
      "Progress: 81.2% ... Training loss: 0.059 ... Validation loss: 0.167\r",
      "Progress: 81.2% ... Training loss: 0.063 ... Validation loss: 0.191\r",
      "Progress: 81.2% ... Training loss: 0.059 ... Validation loss: 0.168\r",
      "Progress: 81.2% ... Training loss: 0.081 ... Validation loss: 0.239\r",
      "Progress: 81.2% ... Training loss: 0.084 ... Validation loss: 0.166\r",
      "Progress: 81.2% ... Training loss: 0.075 ... Validation loss: 0.260\r",
      "Progress: 81.2% ... Training loss: 0.065 ... Validation loss: 0.183\r",
      "Progress: 81.2% ... Training loss: 0.091 ... Validation loss: 0.247\r",
      "Progress: 81.2% ... Training loss: 0.093 ... Validation loss: 0.159\r",
      "Progress: 81.3% ... Training loss: 0.065 ... Validation loss: 0.218\r",
      "Progress: 81.3% ... Training loss: 0.072 ... Validation loss: 0.165\r",
      "Progress: 81.3% ... Training loss: 0.066 ... Validation loss: 0.218\r",
      "Progress: 81.3% ... Training loss: 0.068 ... Validation loss: 0.167\r",
      "Progress: 81.3% ... Training loss: 0.057 ... Validation loss: 0.195\r",
      "Progress: 81.3% ... Training loss: 0.057 ... Validation loss: 0.207\r",
      "Progress: 81.3% ... Training loss: 0.069 ... Validation loss: 0.165\r",
      "Progress: 81.3% ... Training loss: 0.059 ... Validation loss: 0.202\r",
      "Progress: 81.4% ... Training loss: 0.056 ... Validation loss: 0.183\r",
      "Progress: 81.4% ... Training loss: 0.060 ... Validation loss: 0.198\r",
      "Progress: 81.4% ... Training loss: 0.058 ... Validation loss: 0.174\r",
      "Progress: 81.4% ... Training loss: 0.059 ... Validation loss: 0.195\r",
      "Progress: 81.4% ... Training loss: 0.057 ... Validation loss: 0.171\r",
      "Progress: 81.4% ... Training loss: 0.058 ... Validation loss: 0.170\r",
      "Progress: 81.4% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 81.5% ... Training loss: 0.059 ... Validation loss: 0.200\r",
      "Progress: 81.5% ... Training loss: 0.068 ... Validation loss: 0.168\r",
      "Progress: 81.5% ... Training loss: 0.072 ... Validation loss: 0.215\r",
      "Progress: 81.5% ... Training loss: 0.068 ... Validation loss: 0.156\r",
      "Progress: 81.5% ... Training loss: 0.070 ... Validation loss: 0.212\r",
      "Progress: 81.5% ... Training loss: 0.064 ... Validation loss: 0.162\r",
      "Progress: 81.5% ... Training loss: 0.058 ... Validation loss: 0.195\r",
      "Progress: 81.5% ... Training loss: 0.058 ... Validation loss: 0.176\r",
      "Progress: 81.5% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 81.6% ... Training loss: 0.059 ... Validation loss: 0.172\r",
      "Progress: 81.6% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 81.6% ... Training loss: 0.061 ... Validation loss: 0.201\r",
      "Progress: 81.6% ... Training loss: 0.059 ... Validation loss: 0.170\r",
      "Progress: 81.6% ... Training loss: 0.057 ... Validation loss: 0.186\r",
      "Progress: 81.6% ... Training loss: 0.056 ... Validation loss: 0.183\r",
      "Progress: 81.6% ... Training loss: 0.067 ... Validation loss: 0.192\r",
      "Progress: 81.7% ... Training loss: 0.059 ... Validation loss: 0.162\r",
      "Progress: 81.7% ... Training loss: 0.057 ... Validation loss: 0.163\r",
      "Progress: 81.7% ... Training loss: 0.057 ... Validation loss: 0.181\r",
      "Progress: 81.7% ... Training loss: 0.056 ... Validation loss: 0.166\r",
      "Progress: 81.7% ... Training loss: 0.057 ... Validation loss: 0.179\r",
      "Progress: 81.7% ... Training loss: 0.057 ... Validation loss: 0.170\r",
      "Progress: 81.7% ... Training loss: 0.056 ... Validation loss: 0.178\r",
      "Progress: 81.7% ... Training loss: 0.057 ... Validation loss: 0.173\r",
      "Progress: 81.8% ... Training loss: 0.058 ... Validation loss: 0.174\r",
      "Progress: 81.8% ... Training loss: 0.057 ... Validation loss: 0.173\r",
      "Progress: 81.8% ... Training loss: 0.062 ... Validation loss: 0.198\r",
      "Progress: 81.8% ... Training loss: 0.059 ... Validation loss: 0.161\r",
      "Progress: 81.8% ... Training loss: 0.059 ... Validation loss: 0.182\r",
      "Progress: 81.8% ... Training loss: 0.056 ... Validation loss: 0.171\r",
      "Progress: 81.8% ... Training loss: 0.057 ... Validation loss: 0.180\r",
      "Progress: 81.8% ... Training loss: 0.062 ... Validation loss: 0.192\r",
      "Progress: 81.8% ... Training loss: 0.061 ... Validation loss: 0.165\r",
      "Progress: 81.9% ... Training loss: 0.060 ... Validation loss: 0.182\r",
      "Progress: 81.9% ... Training loss: 0.057 ... Validation loss: 0.178\r",
      "Progress: 81.9% ... Training loss: 0.058 ... Validation loss: 0.190\r",
      "Progress: 81.9% ... Training loss: 0.058 ... Validation loss: 0.179\r",
      "Progress: 81.9% ... Training loss: 0.057 ... Validation loss: 0.194\r",
      "Progress: 81.9% ... Training loss: 0.057 ... Validation loss: 0.202\r",
      "Progress: 81.9% ... Training loss: 0.061 ... Validation loss: 0.169\r",
      "Progress: 82.0% ... Training loss: 0.061 ... Validation loss: 0.182\r",
      "Progress: 82.0% ... Training loss: 0.062 ... Validation loss: 0.165\r",
      "Progress: 82.0% ... Training loss: 0.057 ... Validation loss: 0.174\r",
      "Progress: 82.0% ... Training loss: 0.057 ... Validation loss: 0.180\r",
      "Progress: 82.0% ... Training loss: 0.056 ... Validation loss: 0.182\r",
      "Progress: 82.0% ... Training loss: 0.057 ... Validation loss: 0.181\r",
      "Progress: 82.0% ... Training loss: 0.060 ... Validation loss: 0.199\r",
      "Progress: 82.0% ... Training loss: 0.068 ... Validation loss: 0.173\r",
      "Progress: 82.0% ... Training loss: 0.063 ... Validation loss: 0.218\r",
      "Progress: 82.1% ... Training loss: 0.063 ... Validation loss: 0.176\r",
      "Progress: 82.1% ... Training loss: 0.057 ... Validation loss: 0.201\r",
      "Progress: 82.1% ... Training loss: 0.060 ... Validation loss: 0.206\r",
      "Progress: 82.1% ... Training loss: 0.058 ... Validation loss: 0.181\r",
      "Progress: 82.1% ... Training loss: 0.056 ... Validation loss: 0.194\r",
      "Progress: 82.1% ... Training loss: 0.056 ... Validation loss: 0.190\r",
      "Progress: 82.1% ... Training loss: 0.056 ... Validation loss: 0.178\r",
      "Progress: 82.2% ... Training loss: 0.056 ... Validation loss: 0.184\r",
      "Progress: 82.2% ... Training loss: 0.056 ... Validation loss: 0.183\r",
      "Progress: 82.2% ... Training loss: 0.056 ... Validation loss: 0.176\r",
      "Progress: 82.2% ... Training loss: 0.057 ... Validation loss: 0.187\r",
      "Progress: 82.2% ... Training loss: 0.058 ... Validation loss: 0.189\r",
      "Progress: 82.2% ... Training loss: 0.056 ... Validation loss: 0.175\r",
      "Progress: 82.2% ... Training loss: 0.056 ... Validation loss: 0.176\r",
      "Progress: 82.2% ... Training loss: 0.060 ... Validation loss: 0.201\r",
      "Progress: 82.2% ... Training loss: 0.062 ... Validation loss: 0.167\r",
      "Progress: 82.3% ... Training loss: 0.058 ... Validation loss: 0.177\r",
      "Progress: 82.3% ... Training loss: 0.056 ... Validation loss: 0.170\r",
      "Progress: 82.3% ... Training loss: 0.056 ... Validation loss: 0.169\r",
      "Progress: 82.3% ... Training loss: 0.062 ... Validation loss: 0.161\r",
      "Progress: 82.3% ... Training loss: 0.057 ... Validation loss: 0.202\r",
      "Progress: 82.3% ... Training loss: 0.062 ... Validation loss: 0.173\r",
      "Progress: 82.3% ... Training loss: 0.061 ... Validation loss: 0.204\r",
      "Progress: 82.3% ... Training loss: 0.061 ... Validation loss: 0.166\r",
      "Progress: 82.4% ... Training loss: 0.059 ... Validation loss: 0.180\r",
      "Progress: 82.4% ... Training loss: 0.060 ... Validation loss: 0.163\r",
      "Progress: 82.4% ... Training loss: 0.058 ... Validation loss: 0.192\r",
      "Progress: 82.4% ... Training loss: 0.057 ... Validation loss: 0.178\r",
      "Progress: 82.4% ... Training loss: 0.064 ... Validation loss: 0.215\r",
      "Progress: 82.4% ... Training loss: 0.058 ... Validation loss: 0.164\r",
      "Progress: 82.4% ... Training loss: 0.059 ... Validation loss: 0.185\r",
      "Progress: 82.5% ... Training loss: 0.065 ... Validation loss: 0.164\r",
      "Progress: 82.5% ... Training loss: 0.057 ... Validation loss: 0.188\r",
      "Progress: 82.5% ... Training loss: 0.056 ... Validation loss: 0.193\r",
      "Progress: 82.5% ... Training loss: 0.056 ... Validation loss: 0.204\r",
      "Progress: 82.5% ... Training loss: 0.059 ... Validation loss: 0.193\r",
      "Progress: 82.5% ... Training loss: 0.059 ... Validation loss: 0.223\r",
      "Progress: 82.5% ... Training loss: 0.056 ... Validation loss: 0.196\r",
      "Progress: 82.5% ... Training loss: 0.056 ... Validation loss: 0.190\r",
      "Progress: 82.5% ... Training loss: 0.059 ... Validation loss: 0.205\r",
      "Progress: 82.6% ... Training loss: 0.057 ... Validation loss: 0.185\r",
      "Progress: 82.6% ... Training loss: 0.057 ... Validation loss: 0.195\r",
      "Progress: 82.6% ... Training loss: 0.058 ... Validation loss: 0.199\r",
      "Progress: 82.6% ... Training loss: 0.057 ... Validation loss: 0.212\r",
      "Progress: 82.6% ... Training loss: 0.057 ... Validation loss: 0.200\r",
      "Progress: 82.6% ... Training loss: 0.061 ... Validation loss: 0.227\r",
      "Progress: 82.6% ... Training loss: 0.064 ... Validation loss: 0.180\r",
      "Progress: 82.7% ... Training loss: 0.072 ... Validation loss: 0.242\r",
      "Progress: 82.7% ... Training loss: 0.058 ... Validation loss: 0.188\r",
      "Progress: 82.7% ... Training loss: 0.060 ... Validation loss: 0.218\r",
      "Progress: 82.7% ... Training loss: 0.068 ... Validation loss: 0.167\r",
      "Progress: 82.7% ... Training loss: 0.069 ... Validation loss: 0.231\r",
      "Progress: 82.7% ... Training loss: 0.063 ... Validation loss: 0.166\r",
      "Progress: 82.7% ... Training loss: 0.078 ... Validation loss: 0.234\r",
      "Progress: 82.7% ... Training loss: 0.102 ... Validation loss: 0.166\r",
      "Progress: 82.8% ... Training loss: 0.147 ... Validation loss: 0.314\r",
      "Progress: 82.8% ... Training loss: 0.116 ... Validation loss: 0.155\r",
      "Progress: 82.8% ... Training loss: 0.088 ... Validation loss: 0.240\r",
      "Progress: 82.8% ... Training loss: 0.064 ... Validation loss: 0.161\r",
      "Progress: 82.8% ... Training loss: 0.063 ... Validation loss: 0.222\r",
      "Progress: 82.8% ... Training loss: 0.063 ... Validation loss: 0.181\r",
      "Progress: 82.8% ... Training loss: 0.059 ... Validation loss: 0.214\r",
      "Progress: 82.8% ... Training loss: 0.059 ... Validation loss: 0.188\r",
      "Progress: 82.8% ... Training loss: 0.074 ... Validation loss: 0.232\r",
      "Progress: 82.9% ... Training loss: 0.070 ... Validation loss: 0.161\r",
      "Progress: 82.9% ... Training loss: 0.075 ... Validation loss: 0.246\r",
      "Progress: 82.9% ... Training loss: 0.073 ... Validation loss: 0.181\r",
      "Progress: 82.9% ... Training loss: 0.061 ... Validation loss: 0.233\r",
      "Progress: 82.9% ... Training loss: 0.056 ... Validation loss: 0.195\r",
      "Progress: 82.9% ... Training loss: 0.060 ... Validation loss: 0.180\r",
      "Progress: 82.9% ... Training loss: 0.063 ... Validation loss: 0.227\r",
      "Progress: 83.0% ... Training loss: 0.056 ... Validation loss: 0.189\r",
      "Progress: 83.0% ... Training loss: 0.058 ... Validation loss: 0.172\r",
      "Progress: 83.0% ... Training loss: 0.056 ... Validation loss: 0.191\r",
      "Progress: 83.0% ... Training loss: 0.056 ... Validation loss: 0.203\r",
      "Progress: 83.0% ... Training loss: 0.059 ... Validation loss: 0.179\r",
      "Progress: 83.0% ... Training loss: 0.056 ... Validation loss: 0.194\r",
      "Progress: 83.0% ... Training loss: 0.056 ... Validation loss: 0.184\r",
      "Progress: 83.0% ... Training loss: 0.058 ... Validation loss: 0.206\r",
      "Progress: 83.0% ... Training loss: 0.061 ... Validation loss: 0.182\r",
      "Progress: 83.1% ... Training loss: 0.062 ... Validation loss: 0.223\r",
      "Progress: 83.1% ... Training loss: 0.057 ... Validation loss: 0.188\r",
      "Progress: 83.1% ... Training loss: 0.059 ... Validation loss: 0.195\r",
      "Progress: 83.1% ... Training loss: 0.058 ... Validation loss: 0.182\r",
      "Progress: 83.1% ... Training loss: 0.058 ... Validation loss: 0.209\r",
      "Progress: 83.1% ... Training loss: 0.056 ... Validation loss: 0.196\r",
      "Progress: 83.1% ... Training loss: 0.057 ... Validation loss: 0.205\r",
      "Progress: 83.2% ... Training loss: 0.057 ... Validation loss: 0.178\r",
      "Progress: 83.2% ... Training loss: 0.057 ... Validation loss: 0.186\r",
      "Progress: 83.2% ... Training loss: 0.056 ... Validation loss: 0.182\r",
      "Progress: 83.2% ... Training loss: 0.057 ... Validation loss: 0.186\r",
      "Progress: 83.2% ... Training loss: 0.062 ... Validation loss: 0.167\r",
      "Progress: 83.2% ... Training loss: 0.072 ... Validation loss: 0.215\r",
      "Progress: 83.2% ... Training loss: 0.073 ... Validation loss: 0.161\r",
      "Progress: 83.2% ... Training loss: 0.068 ... Validation loss: 0.216\r",
      "Progress: 83.2% ... Training loss: 0.060 ... Validation loss: 0.169\r",
      "Progress: 83.3% ... Training loss: 0.056 ... Validation loss: 0.174\r",
      "Progress: 83.3% ... Training loss: 0.056 ... Validation loss: 0.185\r",
      "Progress: 83.3% ... Training loss: 0.057 ... Validation loss: 0.184\r",
      "Progress: 83.3% ... Training loss: 0.056 ... Validation loss: 0.175\r",
      "Progress: 83.3% ... Training loss: 0.057 ... Validation loss: 0.191\r",
      "Progress: 83.3% ... Training loss: 0.059 ... Validation loss: 0.171\r",
      "Progress: 83.3% ... Training loss: 0.063 ... Validation loss: 0.200\r",
      "Progress: 83.3% ... Training loss: 0.057 ... Validation loss: 0.174\r",
      "Progress: 83.4% ... Training loss: 0.062 ... Validation loss: 0.214\r",
      "Progress: 83.4% ... Training loss: 0.060 ... Validation loss: 0.170\r",
      "Progress: 83.4% ... Training loss: 0.056 ... Validation loss: 0.187\r",
      "Progress: 83.4% ... Training loss: 0.056 ... Validation loss: 0.194\r",
      "Progress: 83.4% ... Training loss: 0.058 ... Validation loss: 0.176\r",
      "Progress: 83.4% ... Training loss: 0.059 ... Validation loss: 0.197\r",
      "Progress: 83.4% ... Training loss: 0.060 ... Validation loss: 0.186\r",
      "Progress: 83.5% ... Training loss: 0.059 ... Validation loss: 0.199\r",
      "Progress: 83.5% ... Training loss: 0.059 ... Validation loss: 0.164\r",
      "Progress: 83.5% ... Training loss: 0.058 ... Validation loss: 0.202\r",
      "Progress: 83.5% ... Training loss: 0.061 ... Validation loss: 0.166\r",
      "Progress: 83.5% ... Training loss: 0.061 ... Validation loss: 0.197\r",
      "Progress: 83.5% ... Training loss: 0.059 ... Validation loss: 0.164\r",
      "Progress: 83.5% ... Training loss: 0.056 ... Validation loss: 0.181\r",
      "Progress: 83.5% ... Training loss: 0.056 ... Validation loss: 0.183\r",
      "Progress: 83.5% ... Training loss: 0.057 ... Validation loss: 0.178\r",
      "Progress: 83.6% ... Training loss: 0.058 ... Validation loss: 0.165\r",
      "Progress: 83.6% ... Training loss: 0.056 ... Validation loss: 0.179\r",
      "Progress: 83.6% ... Training loss: 0.062 ... Validation loss: 0.207\r",
      "Progress: 83.6% ... Training loss: 0.057 ... Validation loss: 0.174\r",
      "Progress: 83.6% ... Training loss: 0.057 ... Validation loss: 0.192\r",
      "Progress: 83.6% ... Training loss: 0.058 ... Validation loss: 0.198\r",
      "Progress: 83.6% ... Training loss: 0.066 ... Validation loss: 0.212\r",
      "Progress: 83.7% ... Training loss: 0.057 ... Validation loss: 0.179\r",
      "Progress: 83.7% ... Training loss: 0.062 ... Validation loss: 0.204\r",
      "Progress: 83.7% ... Training loss: 0.060 ... Validation loss: 0.167\r",
      "Progress: 83.7% ... Training loss: 0.062 ... Validation loss: 0.205\r",
      "Progress: 83.7% ... Training loss: 0.062 ... Validation loss: 0.162\r",
      "Progress: 83.7% ... Training loss: 0.058 ... Validation loss: 0.207\r",
      "Progress: 83.7% ... Training loss: 0.057 ... Validation loss: 0.176\r",
      "Progress: 83.7% ... Training loss: 0.058 ... Validation loss: 0.201\r",
      "Progress: 83.8% ... Training loss: 0.058 ... Validation loss: 0.182\r",
      "Progress: 83.8% ... Training loss: 0.057 ... Validation loss: 0.181\r",
      "Progress: 83.8% ... Training loss: 0.060 ... Validation loss: 0.190\r",
      "Progress: 83.8% ... Training loss: 0.056 ... Validation loss: 0.179\r",
      "Progress: 83.8% ... Training loss: 0.058 ... Validation loss: 0.183\r",
      "Progress: 83.8% ... Training loss: 0.057 ... Validation loss: 0.184\r",
      "Progress: 83.8% ... Training loss: 0.063 ... Validation loss: 0.164\r",
      "Progress: 83.8% ... Training loss: 0.056 ... Validation loss: 0.186\r",
      "Progress: 83.8% ... Training loss: 0.057 ... Validation loss: 0.184\r",
      "Progress: 83.9% ... Training loss: 0.057 ... Validation loss: 0.217\r",
      "Progress: 83.9% ... Training loss: 0.057 ... Validation loss: 0.192\r",
      "Progress: 83.9% ... Training loss: 0.056 ... Validation loss: 0.203\r",
      "Progress: 83.9% ... Training loss: 0.056 ... Validation loss: 0.184\r",
      "Progress: 83.9% ... Training loss: 0.058 ... Validation loss: 0.173\r",
      "Progress: 83.9% ... Training loss: 0.056 ... Validation loss: 0.179\r",
      "Progress: 83.9% ... Training loss: 0.056 ... Validation loss: 0.189\r",
      "Progress: 84.0% ... Training loss: 0.058 ... Validation loss: 0.210\r",
      "Progress: 84.0% ... Training loss: 0.056 ... Validation loss: 0.192\r",
      "Progress: 84.0% ... Training loss: 0.057 ... Validation loss: 0.194\r",
      "Progress: 84.0% ... Training loss: 0.057 ... Validation loss: 0.181\r",
      "Progress: 84.0% ... Training loss: 0.056 ... Validation loss: 0.193\r",
      "Progress: 84.0% ... Training loss: 0.057 ... Validation loss: 0.193\r",
      "Progress: 84.0% ... Training loss: 0.056 ... Validation loss: 0.171\r",
      "Progress: 84.0% ... Training loss: 0.057 ... Validation loss: 0.188\r",
      "Progress: 84.0% ... Training loss: 0.056 ... Validation loss: 0.182\r",
      "Progress: 84.1% ... Training loss: 0.056 ... Validation loss: 0.181\r",
      "Progress: 84.1% ... Training loss: 0.056 ... Validation loss: 0.181\r",
      "Progress: 84.1% ... Training loss: 0.058 ... Validation loss: 0.168\r",
      "Progress: 84.1% ... Training loss: 0.071 ... Validation loss: 0.208\r",
      "Progress: 84.1% ... Training loss: 0.059 ... Validation loss: 0.171\r",
      "Progress: 84.1% ... Training loss: 0.057 ... Validation loss: 0.182\r",
      "Progress: 84.1% ... Training loss: 0.056 ... Validation loss: 0.187\r",
      "Progress: 84.2% ... Training loss: 0.058 ... Validation loss: 0.190\r",
      "Progress: 84.2% ... Training loss: 0.056 ... Validation loss: 0.175\r",
      "Progress: 84.2% ... Training loss: 0.056 ... Validation loss: 0.177\r",
      "Progress: 84.2% ... Training loss: 0.057 ... Validation loss: 0.182\r",
      "Progress: 84.2% ... Training loss: 0.057 ... Validation loss: 0.185\r",
      "Progress: 84.2% ... Training loss: 0.055 ... Validation loss: 0.179\r",
      "Progress: 84.2% ... Training loss: 0.056 ... Validation loss: 0.190\r",
      "Progress: 84.2% ... Training loss: 0.056 ... Validation loss: 0.176\r",
      "Progress: 84.2% ... Training loss: 0.056 ... Validation loss: 0.174\r",
      "Progress: 84.3% ... Training loss: 0.057 ... Validation loss: 0.173\r",
      "Progress: 84.3% ... Training loss: 0.057 ... Validation loss: 0.183\r",
      "Progress: 84.3% ... Training loss: 0.057 ... Validation loss: 0.173\r",
      "Progress: 84.3% ... Training loss: 0.056 ... Validation loss: 0.191\r",
      "Progress: 84.3% ... Training loss: 0.061 ... Validation loss: 0.197\r",
      "Progress: 84.3% ... Training loss: 0.058 ... Validation loss: 0.170\r",
      "Progress: 84.3% ... Training loss: 0.057 ... Validation loss: 0.184\r",
      "Progress: 84.3% ... Training loss: 0.057 ... Validation loss: 0.183\r",
      "Progress: 84.4% ... Training loss: 0.056 ... Validation loss: 0.181\r",
      "Progress: 84.4% ... Training loss: 0.055 ... Validation loss: 0.190\r",
      "Progress: 84.4% ... Training loss: 0.057 ... Validation loss: 0.206\r",
      "Progress: 84.4% ... Training loss: 0.057 ... Validation loss: 0.179\r",
      "Progress: 84.4% ... Training loss: 0.064 ... Validation loss: 0.210\r",
      "Progress: 84.4% ... Training loss: 0.064 ... Validation loss: 0.164\r",
      "Progress: 84.4% ... Training loss: 0.070 ... Validation loss: 0.202\r",
      "Progress: 84.5% ... Training loss: 0.061 ... Validation loss: 0.166\r",
      "Progress: 84.5% ... Training loss: 0.079 ... Validation loss: 0.214\r",
      "Progress: 84.5% ... Training loss: 0.084 ... Validation loss: 0.161\r",
      "Progress: 84.5% ... Training loss: 0.081 ... Validation loss: 0.236\r",
      "Progress: 84.5% ... Training loss: 0.061 ... Validation loss: 0.180\r",
      "Progress: 84.5% ... Training loss: 0.063 ... Validation loss: 0.225\r",
      "Progress: 84.5% ... Training loss: 0.060 ... Validation loss: 0.183\r",
      "Progress: 84.5% ... Training loss: 0.061 ... Validation loss: 0.213\r",
      "Progress: 84.5% ... Training loss: 0.056 ... Validation loss: 0.187\r",
      "Progress: 84.6% ... Training loss: 0.062 ... Validation loss: 0.222\r",
      "Progress: 84.6% ... Training loss: 0.058 ... Validation loss: 0.182\r",
      "Progress: 84.6% ... Training loss: 0.058 ... Validation loss: 0.173\r",
      "Progress: 84.6% ... Training loss: 0.059 ... Validation loss: 0.198\r",
      "Progress: 84.6% ... Training loss: 0.057 ... Validation loss: 0.175\r",
      "Progress: 84.6% ... Training loss: 0.057 ... Validation loss: 0.190\r",
      "Progress: 84.6% ... Training loss: 0.057 ... Validation loss: 0.185\r",
      "Progress: 84.7% ... Training loss: 0.056 ... Validation loss: 0.174\r",
      "Progress: 84.7% ... Training loss: 0.056 ... Validation loss: 0.171\r",
      "Progress: 84.7% ... Training loss: 0.056 ... Validation loss: 0.191\r",
      "Progress: 84.7% ... Training loss: 0.057 ... Validation loss: 0.180\r",
      "Progress: 84.7% ... Training loss: 0.057 ... Validation loss: 0.175\r",
      "Progress: 84.7% ... Training loss: 0.062 ... Validation loss: 0.202\r",
      "Progress: 84.7% ... Training loss: 0.069 ... Validation loss: 0.163\r",
      "Progress: 84.7% ... Training loss: 0.066 ... Validation loss: 0.210\r",
      "Progress: 84.8% ... Training loss: 0.063 ... Validation loss: 0.163\r",
      "Progress: 84.8% ... Training loss: 0.071 ... Validation loss: 0.211\r",
      "Progress: 84.8% ... Training loss: 0.067 ... Validation loss: 0.152\r",
      "Progress: 84.8% ... Training loss: 0.065 ... Validation loss: 0.174\r",
      "Progress: 84.8% ... Training loss: 0.066 ... Validation loss: 0.150\r",
      "Progress: 84.8% ... Training loss: 0.056 ... Validation loss: 0.176\r",
      "Progress: 84.8% ... Training loss: 0.058 ... Validation loss: 0.170\r",
      "Progress: 84.8% ... Training loss: 0.061 ... Validation loss: 0.200\r",
      "Progress: 84.8% ... Training loss: 0.057 ... Validation loss: 0.161\r",
      "Progress: 84.9% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 84.9% ... Training loss: 0.058 ... Validation loss: 0.164\r",
      "Progress: 84.9% ... Training loss: 0.056 ... Validation loss: 0.168\r",
      "Progress: 84.9% ... Training loss: 0.056 ... Validation loss: 0.171\r",
      "Progress: 84.9% ... Training loss: 0.060 ... Validation loss: 0.182\r",
      "Progress: 84.9% ... Training loss: 0.057 ... Validation loss: 0.159\r",
      "Progress: 84.9% ... Training loss: 0.063 ... Validation loss: 0.199\r",
      "Progress: 85.0% ... Training loss: 0.062 ... Validation loss: 0.160\r",
      "Progress: 85.0% ... Training loss: 0.059 ... Validation loss: 0.184\r",
      "Progress: 85.0% ... Training loss: 0.056 ... Validation loss: 0.169\r",
      "Progress: 85.0% ... Training loss: 0.056 ... Validation loss: 0.184\r",
      "Progress: 85.0% ... Training loss: 0.057 ... Validation loss: 0.195\r",
      "Progress: 85.0% ... Training loss: 0.064 ... Validation loss: 0.165\r",
      "Progress: 85.0% ... Training loss: 0.067 ... Validation loss: 0.227\r",
      "Progress: 85.0% ... Training loss: 0.061 ... Validation loss: 0.168\r",
      "Progress: 85.0% ... Training loss: 0.056 ... Validation loss: 0.175\r",
      "Progress: 85.1% ... Training loss: 0.057 ... Validation loss: 0.175\r",
      "Progress: 85.1% ... Training loss: 0.056 ... Validation loss: 0.176\r",
      "Progress: 85.1% ... Training loss: 0.060 ... Validation loss: 0.185\r",
      "Progress: 85.1% ... Training loss: 0.057 ... Validation loss: 0.169\r",
      "Progress: 85.1% ... Training loss: 0.057 ... Validation loss: 0.188\r",
      "Progress: 85.1% ... Training loss: 0.060 ... Validation loss: 0.165\r",
      "Progress: 85.1% ... Training loss: 0.057 ... Validation loss: 0.188\r",
      "Progress: 85.2% ... Training loss: 0.056 ... Validation loss: 0.177\r",
      "Progress: 85.2% ... Training loss: 0.057 ... Validation loss: 0.170\r",
      "Progress: 85.2% ... Training loss: 0.057 ... Validation loss: 0.173\r",
      "Progress: 85.2% ... Training loss: 0.062 ... Validation loss: 0.201\r",
      "Progress: 85.2% ... Training loss: 0.057 ... Validation loss: 0.173\r",
      "Progress: 85.2% ... Training loss: 0.061 ... Validation loss: 0.206\r",
      "Progress: 85.2% ... Training loss: 0.065 ... Validation loss: 0.159\r",
      "Progress: 85.2% ... Training loss: 0.058 ... Validation loss: 0.203\r",
      "Progress: 85.2% ... Training loss: 0.060 ... Validation loss: 0.174\r",
      "Progress: 85.3% ... Training loss: 0.056 ... Validation loss: 0.199\r",
      "Progress: 85.3% ... Training loss: 0.057 ... Validation loss: 0.188\r",
      "Progress: 85.3% ... Training loss: 0.061 ... Validation loss: 0.184\r",
      "Progress: 85.3% ... Training loss: 0.075 ... Validation loss: 0.241\r",
      "Progress: 85.3% ... Training loss: 0.074 ... Validation loss: 0.160\r",
      "Progress: 85.3% ... Training loss: 0.061 ... Validation loss: 0.202\r",
      "Progress: 85.3% ... Training loss: 0.058 ... Validation loss: 0.169\r",
      "Progress: 85.3% ... Training loss: 0.061 ... Validation loss: 0.199\r",
      "Progress: 85.4% ... Training loss: 0.059 ... Validation loss: 0.172\r",
      "Progress: 85.4% ... Training loss: 0.056 ... Validation loss: 0.194\r",
      "Progress: 85.4% ... Training loss: 0.056 ... Validation loss: 0.201\r",
      "Progress: 85.4% ... Training loss: 0.057 ... Validation loss: 0.217\r",
      "Progress: 85.4% ... Training loss: 0.060 ... Validation loss: 0.179\r",
      "Progress: 85.4% ... Training loss: 0.066 ... Validation loss: 0.222\r",
      "Progress: 85.4% ... Training loss: 0.058 ... Validation loss: 0.185\r",
      "Progress: 85.5% ... Training loss: 0.056 ... Validation loss: 0.184\r",
      "Progress: 85.5% ... Training loss: 0.056 ... Validation loss: 0.189\r",
      "Progress: 85.5% ... Training loss: 0.065 ... Validation loss: 0.210\r",
      "Progress: 85.5% ... Training loss: 0.059 ... Validation loss: 0.188\r",
      "Progress: 85.5% ... Training loss: 0.060 ... Validation loss: 0.222\r",
      "Progress: 85.5% ... Training loss: 0.065 ... Validation loss: 0.173\r",
      "Progress: 85.5% ... Training loss: 0.076 ... Validation loss: 0.239\r",
      "Progress: 85.5% ... Training loss: 0.069 ... Validation loss: 0.165\r",
      "Progress: 85.5% ... Training loss: 0.070 ... Validation loss: 0.222\r",
      "Progress: 85.6% ... Training loss: 0.095 ... Validation loss: 0.156\r",
      "Progress: 85.6% ... Training loss: 0.097 ... Validation loss: 0.257\r",
      "Progress: 85.6% ... Training loss: 0.102 ... Validation loss: 0.160\r",
      "Progress: 85.6% ... Training loss: 0.109 ... Validation loss: 0.255\r",
      "Progress: 85.6% ... Training loss: 0.121 ... Validation loss: 0.159\r",
      "Progress: 85.6% ... Training loss: 0.125 ... Validation loss: 0.267\r",
      "Progress: 85.6% ... Training loss: 0.077 ... Validation loss: 0.160\r",
      "Progress: 85.7% ... Training loss: 0.070 ... Validation loss: 0.223\r",
      "Progress: 85.7% ... Training loss: 0.063 ... Validation loss: 0.164\r",
      "Progress: 85.7% ... Training loss: 0.057 ... Validation loss: 0.182\r",
      "Progress: 85.7% ... Training loss: 0.057 ... Validation loss: 0.172\r",
      "Progress: 85.7% ... Training loss: 0.058 ... Validation loss: 0.176\r",
      "Progress: 85.7% ... Training loss: 0.057 ... Validation loss: 0.192\r",
      "Progress: 85.7% ... Training loss: 0.057 ... Validation loss: 0.173\r",
      "Progress: 85.7% ... Training loss: 0.060 ... Validation loss: 0.207\r",
      "Progress: 85.8% ... Training loss: 0.056 ... Validation loss: 0.184\r",
      "Progress: 85.8% ... Training loss: 0.059 ... Validation loss: 0.200\r",
      "Progress: 85.8% ... Training loss: 0.056 ... Validation loss: 0.185\r",
      "Progress: 85.8% ... Training loss: 0.057 ... Validation loss: 0.204\r",
      "Progress: 85.8% ... Training loss: 0.056 ... Validation loss: 0.187\r",
      "Progress: 85.8% ... Training loss: 0.057 ... Validation loss: 0.181\r",
      "Progress: 85.8% ... Training loss: 0.056 ... Validation loss: 0.190\r",
      "Progress: 85.8% ... Training loss: 0.059 ... Validation loss: 0.187\r",
      "Progress: 85.8% ... Training loss: 0.060 ... Validation loss: 0.171\r",
      "Progress: 85.9% ... Training loss: 0.057 ... Validation loss: 0.196\r",
      "Progress: 85.9% ... Training loss: 0.057 ... Validation loss: 0.198\r",
      "Progress: 85.9% ... Training loss: 0.056 ... Validation loss: 0.176\r",
      "Progress: 85.9% ... Training loss: 0.064 ... Validation loss: 0.208\r",
      "Progress: 85.9% ... Training loss: 0.060 ... Validation loss: 0.171\r",
      "Progress: 85.9% ... Training loss: 0.069 ... Validation loss: 0.231\r",
      "Progress: 85.9% ... Training loss: 0.062 ... Validation loss: 0.178\r",
      "Progress: 86.0% ... Training loss: 0.058 ... Validation loss: 0.207\r",
      "Progress: 86.0% ... Training loss: 0.056 ... Validation loss: 0.182\r",
      "Progress: 86.0% ... Training loss: 0.057 ... Validation loss: 0.161\r",
      "Progress: 86.0% ... Training loss: 0.056 ... Validation loss: 0.175\r",
      "Progress: 86.0% ... Training loss: 0.057 ... Validation loss: 0.164\r",
      "Progress: 86.0% ... Training loss: 0.056 ... Validation loss: 0.177\r",
      "Progress: 86.0% ... Training loss: 0.055 ... Validation loss: 0.178\r",
      "Progress: 86.0% ... Training loss: 0.056 ... Validation loss: 0.191\r",
      "Progress: 86.0% ... Training loss: 0.056 ... Validation loss: 0.180\r",
      "Progress: 86.1% ... Training loss: 0.056 ... Validation loss: 0.185\r",
      "Progress: 86.1% ... Training loss: 0.058 ... Validation loss: 0.181\r",
      "Progress: 86.1% ... Training loss: 0.057 ... Validation loss: 0.174\r",
      "Progress: 86.1% ... Training loss: 0.057 ... Validation loss: 0.197\r",
      "Progress: 86.1% ... Training loss: 0.056 ... Validation loss: 0.169\r",
      "Progress: 86.1% ... Training loss: 0.060 ... Validation loss: 0.163\r",
      "Progress: 86.1% ... Training loss: 0.056 ... Validation loss: 0.183\r",
      "Progress: 86.2% ... Training loss: 0.058 ... Validation loss: 0.174\r",
      "Progress: 86.2% ... Training loss: 0.057 ... Validation loss: 0.187\r",
      "Progress: 86.2% ... Training loss: 0.056 ... Validation loss: 0.170\r",
      "Progress: 86.2% ... Training loss: 0.061 ... Validation loss: 0.194\r",
      "Progress: 86.2% ... Training loss: 0.061 ... Validation loss: 0.160\r",
      "Progress: 86.2% ... Training loss: 0.056 ... Validation loss: 0.193\r",
      "Progress: 86.2% ... Training loss: 0.057 ... Validation loss: 0.172\r",
      "Progress: 86.2% ... Training loss: 0.066 ... Validation loss: 0.213\r",
      "Progress: 86.2% ... Training loss: 0.059 ... Validation loss: 0.170\r",
      "Progress: 86.3% ... Training loss: 0.056 ... Validation loss: 0.189\r",
      "Progress: 86.3% ... Training loss: 0.076 ... Validation loss: 0.153\r",
      "Progress: 86.3% ... Training loss: 0.057 ... Validation loss: 0.176\r",
      "Progress: 86.3% ... Training loss: 0.062 ... Validation loss: 0.195\r",
      "Progress: 86.3% ... Training loss: 0.068 ... Validation loss: 0.160\r",
      "Progress: 86.3% ... Training loss: 0.056 ... Validation loss: 0.165\r",
      "Progress: 86.3% ... Training loss: 0.056 ... Validation loss: 0.179\r",
      "Progress: 86.3% ... Training loss: 0.056 ... Validation loss: 0.173\r",
      "Progress: 86.4% ... Training loss: 0.056 ... Validation loss: 0.193\r",
      "Progress: 86.4% ... Training loss: 0.057 ... Validation loss: 0.172\r",
      "Progress: 86.4% ... Training loss: 0.057 ... Validation loss: 0.182\r",
      "Progress: 86.4% ... Training loss: 0.057 ... Validation loss: 0.185\r",
      "Progress: 86.4% ... Training loss: 0.058 ... Validation loss: 0.180\r",
      "Progress: 86.4% ... Training loss: 0.060 ... Validation loss: 0.161\r",
      "Progress: 86.4% ... Training loss: 0.059 ... Validation loss: 0.191\r",
      "Progress: 86.5% ... Training loss: 0.059 ... Validation loss: 0.167\r",
      "Progress: 86.5% ... Training loss: 0.058 ... Validation loss: 0.179\r",
      "Progress: 86.5% ... Training loss: 0.055 ... Validation loss: 0.179\r",
      "Progress: 86.5% ... Training loss: 0.056 ... Validation loss: 0.167\r",
      "Progress: 86.5% ... Training loss: 0.059 ... Validation loss: 0.191\r",
      "Progress: 86.5% ... Training loss: 0.059 ... Validation loss: 0.173\r",
      "Progress: 86.5% ... Training loss: 0.056 ... Validation loss: 0.181\r",
      "Progress: 86.5% ... Training loss: 0.055 ... Validation loss: 0.186\r",
      "Progress: 86.5% ... Training loss: 0.055 ... Validation loss: 0.185\r",
      "Progress: 86.6% ... Training loss: 0.056 ... Validation loss: 0.184\r",
      "Progress: 86.6% ... Training loss: 0.063 ... Validation loss: 0.180\r",
      "Progress: 86.6% ... Training loss: 0.065 ... Validation loss: 0.160\r",
      "Progress: 86.6% ... Training loss: 0.058 ... Validation loss: 0.192\r",
      "Progress: 86.6% ... Training loss: 0.057 ... Validation loss: 0.199\r",
      "Progress: 86.6% ... Training loss: 0.064 ... Validation loss: 0.166\r",
      "Progress: 86.6% ... Training loss: 0.058 ... Validation loss: 0.204\r",
      "Progress: 86.7% ... Training loss: 0.055 ... Validation loss: 0.178\r",
      "Progress: 86.7% ... Training loss: 0.056 ... Validation loss: 0.171\r",
      "Progress: 86.7% ... Training loss: 0.057 ... Validation loss: 0.171\r",
      "Progress: 86.7% ... Training loss: 0.056 ... Validation loss: 0.166\r",
      "Progress: 86.7% ... Training loss: 0.057 ... Validation loss: 0.172\r",
      "Progress: 86.7% ... Training loss: 0.060 ... Validation loss: 0.184\r",
      "Progress: 86.7% ... Training loss: 0.062 ... Validation loss: 0.157\r",
      "Progress: 86.7% ... Training loss: 0.060 ... Validation loss: 0.182\r",
      "Progress: 86.8% ... Training loss: 0.071 ... Validation loss: 0.150\r",
      "Progress: 86.8% ... Training loss: 0.061 ... Validation loss: 0.197\r",
      "Progress: 86.8% ... Training loss: 0.067 ... Validation loss: 0.153\r",
      "Progress: 86.8% ... Training loss: 0.078 ... Validation loss: 0.233\r",
      "Progress: 86.8% ... Training loss: 0.065 ... Validation loss: 0.150\r",
      "Progress: 86.8% ... Training loss: 0.066 ... Validation loss: 0.206\r",
      "Progress: 86.8% ... Training loss: 0.061 ... Validation loss: 0.154\r",
      "Progress: 86.8% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 86.8% ... Training loss: 0.057 ... Validation loss: 0.163\r",
      "Progress: 86.9% ... Training loss: 0.057 ... Validation loss: 0.182\r",
      "Progress: 86.9% ... Training loss: 0.060 ... Validation loss: 0.166\r",
      "Progress: 86.9% ... Training loss: 0.056 ... Validation loss: 0.180\r",
      "Progress: 86.9% ... Training loss: 0.056 ... Validation loss: 0.186\r",
      "Progress: 86.9% ... Training loss: 0.056 ... Validation loss: 0.170\r",
      "Progress: 86.9% ... Training loss: 0.058 ... Validation loss: 0.163\r",
      "Progress: 86.9% ... Training loss: 0.056 ... Validation loss: 0.188\r",
      "Progress: 87.0% ... Training loss: 0.055 ... Validation loss: 0.174\r",
      "Progress: 87.0% ... Training loss: 0.055 ... Validation loss: 0.171\r",
      "Progress: 87.0% ... Training loss: 0.057 ... Validation loss: 0.170\r",
      "Progress: 87.0% ... Training loss: 0.055 ... Validation loss: 0.173\r",
      "Progress: 87.0% ... Training loss: 0.055 ... Validation loss: 0.176\r",
      "Progress: 87.0% ... Training loss: 0.055 ... Validation loss: 0.173\r",
      "Progress: 87.0% ... Training loss: 0.056 ... Validation loss: 0.169\r",
      "Progress: 87.0% ... Training loss: 0.056 ... Validation loss: 0.177\r",
      "Progress: 87.0% ... Training loss: 0.056 ... Validation loss: 0.174\r",
      "Progress: 87.1% ... Training loss: 0.056 ... Validation loss: 0.164\r",
      "Progress: 87.1% ... Training loss: 0.060 ... Validation loss: 0.193\r",
      "Progress: 87.1% ... Training loss: 0.057 ... Validation loss: 0.190\r",
      "Progress: 87.1% ... Training loss: 0.057 ... Validation loss: 0.194\r",
      "Progress: 87.1% ... Training loss: 0.058 ... Validation loss: 0.174\r",
      "Progress: 87.1% ... Training loss: 0.058 ... Validation loss: 0.182\r",
      "Progress: 87.1% ... Training loss: 0.058 ... Validation loss: 0.166\r",
      "Progress: 87.2% ... Training loss: 0.078 ... Validation loss: 0.220\r",
      "Progress: 87.2% ... Training loss: 0.067 ... Validation loss: 0.157\r",
      "Progress: 87.2% ... Training loss: 0.057 ... Validation loss: 0.186\r",
      "Progress: 87.2% ... Training loss: 0.056 ... Validation loss: 0.173\r",
      "Progress: 87.2% ... Training loss: 0.055 ... Validation loss: 0.191\r",
      "Progress: 87.2% ... Training loss: 0.055 ... Validation loss: 0.187\r",
      "Progress: 87.2% ... Training loss: 0.056 ... Validation loss: 0.175\r",
      "Progress: 87.2% ... Training loss: 0.057 ... Validation loss: 0.163\r",
      "Progress: 87.2% ... Training loss: 0.072 ... Validation loss: 0.212\r",
      "Progress: 87.3% ... Training loss: 0.123 ... Validation loss: 0.159\r",
      "Progress: 87.3% ... Training loss: 0.133 ... Validation loss: 0.283\r",
      "Progress: 87.3% ... Training loss: 0.110 ... Validation loss: 0.160\r",
      "Progress: 87.3% ... Training loss: 0.103 ... Validation loss: 0.253\r",
      "Progress: 87.3% ... Training loss: 0.099 ... Validation loss: 0.158\r",
      "Progress: 87.3% ... Training loss: 0.078 ... Validation loss: 0.253\r",
      "Progress: 87.3% ... Training loss: 0.073 ... Validation loss: 0.168\r",
      "Progress: 87.3% ... Training loss: 0.077 ... Validation loss: 0.238\r",
      "Progress: 87.4% ... Training loss: 0.066 ... Validation loss: 0.168\r",
      "Progress: 87.4% ... Training loss: 0.071 ... Validation loss: 0.236\r",
      "Progress: 87.4% ... Training loss: 0.067 ... Validation loss: 0.173\r",
      "Progress: 87.4% ... Training loss: 0.061 ... Validation loss: 0.217\r",
      "Progress: 87.4% ... Training loss: 0.057 ... Validation loss: 0.186\r",
      "Progress: 87.4% ... Training loss: 0.061 ... Validation loss: 0.207\r",
      "Progress: 87.4% ... Training loss: 0.063 ... Validation loss: 0.175\r",
      "Progress: 87.5% ... Training loss: 0.065 ... Validation loss: 0.216\r",
      "Progress: 87.5% ... Training loss: 0.059 ... Validation loss: 0.167\r",
      "Progress: 87.5% ... Training loss: 0.058 ... Validation loss: 0.202\r",
      "Progress: 87.5% ... Training loss: 0.063 ... Validation loss: 0.163\r",
      "Progress: 87.5% ... Training loss: 0.056 ... Validation loss: 0.195\r",
      "Progress: 87.5% ... Training loss: 0.059 ... Validation loss: 0.193\r",
      "Progress: 87.5% ... Training loss: 0.063 ... Validation loss: 0.224\r",
      "Progress: 87.5% ... Training loss: 0.063 ... Validation loss: 0.170\r",
      "Progress: 87.5% ... Training loss: 0.063 ... Validation loss: 0.216\r",
      "Progress: 87.6% ... Training loss: 0.065 ... Validation loss: 0.164\r",
      "Progress: 87.6% ... Training loss: 0.074 ... Validation loss: 0.220\r",
      "Progress: 87.6% ... Training loss: 0.079 ... Validation loss: 0.160\r",
      "Progress: 87.6% ... Training loss: 0.094 ... Validation loss: 0.254\r",
      "Progress: 87.6% ... Training loss: 0.110 ... Validation loss: 0.166\r",
      "Progress: 87.6% ... Training loss: 0.086 ... Validation loss: 0.255\r",
      "Progress: 87.6% ... Training loss: 0.061 ... Validation loss: 0.164\r",
      "Progress: 87.7% ... Training loss: 0.064 ... Validation loss: 0.201\r",
      "Progress: 87.7% ... Training loss: 0.057 ... Validation loss: 0.184\r",
      "Progress: 87.7% ... Training loss: 0.056 ... Validation loss: 0.171\r",
      "Progress: 87.7% ... Training loss: 0.056 ... Validation loss: 0.177\r",
      "Progress: 87.7% ... Training loss: 0.056 ... Validation loss: 0.190\r",
      "Progress: 87.7% ... Training loss: 0.057 ... Validation loss: 0.179\r",
      "Progress: 87.7% ... Training loss: 0.057 ... Validation loss: 0.183\r",
      "Progress: 87.7% ... Training loss: 0.056 ... Validation loss: 0.178\r",
      "Progress: 87.8% ... Training loss: 0.055 ... Validation loss: 0.187\r",
      "Progress: 87.8% ... Training loss: 0.075 ... Validation loss: 0.158\r",
      "Progress: 87.8% ... Training loss: 0.095 ... Validation loss: 0.225\r",
      "Progress: 87.8% ... Training loss: 0.066 ... Validation loss: 0.156\r",
      "Progress: 87.8% ... Training loss: 0.057 ... Validation loss: 0.187\r",
      "Progress: 87.8% ... Training loss: 0.059 ... Validation loss: 0.170\r",
      "Progress: 87.8% ... Training loss: 0.059 ... Validation loss: 0.203\r",
      "Progress: 87.8% ... Training loss: 0.057 ... Validation loss: 0.176\r",
      "Progress: 87.8% ... Training loss: 0.056 ... Validation loss: 0.184\r",
      "Progress: 87.9% ... Training loss: 0.062 ... Validation loss: 0.195\r",
      "Progress: 87.9% ... Training loss: 0.060 ... Validation loss: 0.164\r",
      "Progress: 87.9% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 87.9% ... Training loss: 0.058 ... Validation loss: 0.171\r",
      "Progress: 87.9% ... Training loss: 0.056 ... Validation loss: 0.191\r",
      "Progress: 87.9% ... Training loss: 0.057 ... Validation loss: 0.175\r",
      "Progress: 87.9% ... Training loss: 0.056 ... Validation loss: 0.187\r",
      "Progress: 88.0% ... Training loss: 0.057 ... Validation loss: 0.194\r",
      "Progress: 88.0% ... Training loss: 0.056 ... Validation loss: 0.174\r",
      "Progress: 88.0% ... Training loss: 0.055 ... Validation loss: 0.188\r",
      "Progress: 88.0% ... Training loss: 0.056 ... Validation loss: 0.172\r",
      "Progress: 88.0% ... Training loss: 0.056 ... Validation loss: 0.171\r",
      "Progress: 88.0% ... Training loss: 0.056 ... Validation loss: 0.185\r",
      "Progress: 88.0% ... Training loss: 0.059 ... Validation loss: 0.203\r",
      "Progress: 88.0% ... Training loss: 0.058 ... Validation loss: 0.179\r",
      "Progress: 88.0% ... Training loss: 0.059 ... Validation loss: 0.216\r",
      "Progress: 88.1% ... Training loss: 0.055 ... Validation loss: 0.203\r",
      "Progress: 88.1% ... Training loss: 0.057 ... Validation loss: 0.210\r",
      "Progress: 88.1% ... Training loss: 0.056 ... Validation loss: 0.185\r",
      "Progress: 88.1% ... Training loss: 0.058 ... Validation loss: 0.201\r",
      "Progress: 88.1% ... Training loss: 0.065 ... Validation loss: 0.164\r",
      "Progress: 88.1% ... Training loss: 0.063 ... Validation loss: 0.218\r",
      "Progress: 88.1% ... Training loss: 0.065 ... Validation loss: 0.175\r",
      "Progress: 88.2% ... Training loss: 0.057 ... Validation loss: 0.211\r",
      "Progress: 88.2% ... Training loss: 0.056 ... Validation loss: 0.203\r",
      "Progress: 88.2% ... Training loss: 0.056 ... Validation loss: 0.201\r",
      "Progress: 88.2% ... Training loss: 0.056 ... Validation loss: 0.186\r",
      "Progress: 88.2% ... Training loss: 0.055 ... Validation loss: 0.183\r",
      "Progress: 88.2% ... Training loss: 0.056 ... Validation loss: 0.205\r",
      "Progress: 88.2% ... Training loss: 0.055 ... Validation loss: 0.178\r",
      "Progress: 88.2% ... Training loss: 0.056 ... Validation loss: 0.185\r",
      "Progress: 88.2% ... Training loss: 0.055 ... Validation loss: 0.181\r",
      "Progress: 88.3% ... Training loss: 0.056 ... Validation loss: 0.173\r",
      "Progress: 88.3% ... Training loss: 0.065 ... Validation loss: 0.216\r",
      "Progress: 88.3% ... Training loss: 0.061 ... Validation loss: 0.174\r",
      "Progress: 88.3% ... Training loss: 0.067 ... Validation loss: 0.212\r",
      "Progress: 88.3% ... Training loss: 0.078 ... Validation loss: 0.158\r",
      "Progress: 88.3% ... Training loss: 0.061 ... Validation loss: 0.203\r",
      "Progress: 88.3% ... Training loss: 0.063 ... Validation loss: 0.171\r",
      "Progress: 88.3% ... Training loss: 0.057 ... Validation loss: 0.204\r",
      "Progress: 88.4% ... Training loss: 0.061 ... Validation loss: 0.167\r",
      "Progress: 88.4% ... Training loss: 0.062 ... Validation loss: 0.192\r",
      "Progress: 88.4% ... Training loss: 0.057 ... Validation loss: 0.172\r",
      "Progress: 88.4% ... Training loss: 0.057 ... Validation loss: 0.189\r",
      "Progress: 88.4% ... Training loss: 0.061 ... Validation loss: 0.167\r",
      "Progress: 88.4% ... Training loss: 0.070 ... Validation loss: 0.229\r",
      "Progress: 88.4% ... Training loss: 0.064 ... Validation loss: 0.163\r",
      "Progress: 88.5% ... Training loss: 0.067 ... Validation loss: 0.215\r",
      "Progress: 88.5% ... Training loss: 0.069 ... Validation loss: 0.160\r",
      "Progress: 88.5% ... Training loss: 0.065 ... Validation loss: 0.217\r",
      "Progress: 88.5% ... Training loss: 0.080 ... Validation loss: 0.160\r",
      "Progress: 88.5% ... Training loss: 0.065 ... Validation loss: 0.212\r",
      "Progress: 88.5% ... Training loss: 0.059 ... Validation loss: 0.163\r",
      "Progress: 88.5% ... Training loss: 0.056 ... Validation loss: 0.181\r",
      "Progress: 88.5% ... Training loss: 0.056 ... Validation loss: 0.199\r",
      "Progress: 88.5% ... Training loss: 0.055 ... Validation loss: 0.188\r",
      "Progress: 88.6% ... Training loss: 0.061 ... Validation loss: 0.182\r",
      "Progress: 88.6% ... Training loss: 0.061 ... Validation loss: 0.223\r",
      "Progress: 88.6% ... Training loss: 0.056 ... Validation loss: 0.183\r",
      "Progress: 88.6% ... Training loss: 0.062 ... Validation loss: 0.213\r",
      "Progress: 88.6% ... Training loss: 0.056 ... Validation loss: 0.174\r",
      "Progress: 88.6% ... Training loss: 0.059 ... Validation loss: 0.196\r",
      "Progress: 88.6% ... Training loss: 0.060 ... Validation loss: 0.168\r",
      "Progress: 88.7% ... Training loss: 0.057 ... Validation loss: 0.192\r",
      "Progress: 88.7% ... Training loss: 0.063 ... Validation loss: 0.170\r",
      "Progress: 88.7% ... Training loss: 0.061 ... Validation loss: 0.205\r",
      "Progress: 88.7% ... Training loss: 0.060 ... Validation loss: 0.175\r",
      "Progress: 88.7% ... Training loss: 0.056 ... Validation loss: 0.198\r",
      "Progress: 88.7% ... Training loss: 0.056 ... Validation loss: 0.205\r",
      "Progress: 88.7% ... Training loss: 0.063 ... Validation loss: 0.175\r",
      "Progress: 88.7% ... Training loss: 0.060 ... Validation loss: 0.209\r",
      "Progress: 88.8% ... Training loss: 0.061 ... Validation loss: 0.171\r",
      "Progress: 88.8% ... Training loss: 0.067 ... Validation loss: 0.231\r",
      "Progress: 88.8% ... Training loss: 0.063 ... Validation loss: 0.172\r",
      "Progress: 88.8% ... Training loss: 0.058 ... Validation loss: 0.201\r",
      "Progress: 88.8% ... Training loss: 0.056 ... Validation loss: 0.182\r",
      "Progress: 88.8% ... Training loss: 0.056 ... Validation loss: 0.163\r",
      "Progress: 88.8% ... Training loss: 0.056 ... Validation loss: 0.161\r",
      "Progress: 88.8% ... Training loss: 0.058 ... Validation loss: 0.185\r",
      "Progress: 88.8% ... Training loss: 0.057 ... Validation loss: 0.176\r",
      "Progress: 88.9% ... Training loss: 0.056 ... Validation loss: 0.171\r",
      "Progress: 88.9% ... Training loss: 0.058 ... Validation loss: 0.190\r",
      "Progress: 88.9% ... Training loss: 0.082 ... Validation loss: 0.157\r",
      "Progress: 88.9% ... Training loss: 0.062 ... Validation loss: 0.201\r",
      "Progress: 88.9% ... Training loss: 0.070 ... Validation loss: 0.158\r",
      "Progress: 88.9% ... Training loss: 0.072 ... Validation loss: 0.228\r",
      "Progress: 88.9% ... Training loss: 0.076 ... Validation loss: 0.170\r",
      "Progress: 89.0% ... Training loss: 0.067 ... Validation loss: 0.219\r",
      "Progress: 89.0% ... Training loss: 0.063 ... Validation loss: 0.165\r",
      "Progress: 89.0% ... Training loss: 0.055 ... Validation loss: 0.190\r",
      "Progress: 89.0% ... Training loss: 0.056 ... Validation loss: 0.184\r",
      "Progress: 89.0% ... Training loss: 0.057 ... Validation loss: 0.190\r",
      "Progress: 89.0% ... Training loss: 0.055 ... Validation loss: 0.175\r",
      "Progress: 89.0% ... Training loss: 0.060 ... Validation loss: 0.164\r",
      "Progress: 89.0% ... Training loss: 0.070 ... Validation loss: 0.220\r",
      "Progress: 89.0% ... Training loss: 0.074 ... Validation loss: 0.167\r",
      "Progress: 89.1% ... Training loss: 0.069 ... Validation loss: 0.238\r",
      "Progress: 89.1% ... Training loss: 0.060 ... Validation loss: 0.177\r",
      "Progress: 89.1% ... Training loss: 0.056 ... Validation loss: 0.204\r",
      "Progress: 89.1% ... Training loss: 0.060 ... Validation loss: 0.170\r",
      "Progress: 89.1% ... Training loss: 0.058 ... Validation loss: 0.214\r",
      "Progress: 89.1% ... Training loss: 0.057 ... Validation loss: 0.176\r",
      "Progress: 89.1% ... Training loss: 0.066 ... Validation loss: 0.168\r",
      "Progress: 89.2% ... Training loss: 0.066 ... Validation loss: 0.215\r",
      "Progress: 89.2% ... Training loss: 0.063 ... Validation loss: 0.161\r",
      "Progress: 89.2% ... Training loss: 0.057 ... Validation loss: 0.185\r",
      "Progress: 89.2% ... Training loss: 0.056 ... Validation loss: 0.172\r",
      "Progress: 89.2% ... Training loss: 0.060 ... Validation loss: 0.188\r",
      "Progress: 89.2% ... Training loss: 0.064 ... Validation loss: 0.156\r",
      "Progress: 89.2% ... Training loss: 0.060 ... Validation loss: 0.185\r",
      "Progress: 89.2% ... Training loss: 0.061 ... Validation loss: 0.160\r",
      "Progress: 89.2% ... Training loss: 0.056 ... Validation loss: 0.185\r",
      "Progress: 89.3% ... Training loss: 0.056 ... Validation loss: 0.179\r",
      "Progress: 89.3% ... Training loss: 0.056 ... Validation loss: 0.171\r",
      "Progress: 89.3% ... Training loss: 0.056 ... Validation loss: 0.174\r",
      "Progress: 89.3% ... Training loss: 0.057 ... Validation loss: 0.181\r",
      "Progress: 89.3% ... Training loss: 0.056 ... Validation loss: 0.189\r",
      "Progress: 89.3% ... Training loss: 0.063 ... Validation loss: 0.166\r",
      "Progress: 89.3% ... Training loss: 0.056 ... Validation loss: 0.204\r",
      "Progress: 89.3% ... Training loss: 0.058 ... Validation loss: 0.211\r",
      "Progress: 89.4% ... Training loss: 0.057 ... Validation loss: 0.171\r",
      "Progress: 89.4% ... Training loss: 0.055 ... Validation loss: 0.189\r",
      "Progress: 89.4% ... Training loss: 0.058 ... Validation loss: 0.198\r",
      "Progress: 89.4% ... Training loss: 0.055 ... Validation loss: 0.184\r",
      "Progress: 89.4% ... Training loss: 0.055 ... Validation loss: 0.182\r",
      "Progress: 89.4% ... Training loss: 0.057 ... Validation loss: 0.167\r",
      "Progress: 89.4% ... Training loss: 0.056 ... Validation loss: 0.175\r",
      "Progress: 89.5% ... Training loss: 0.056 ... Validation loss: 0.168\r",
      "Progress: 89.5% ... Training loss: 0.062 ... Validation loss: 0.195\r",
      "Progress: 89.5% ... Training loss: 0.056 ... Validation loss: 0.181\r",
      "Progress: 89.5% ... Training loss: 0.057 ... Validation loss: 0.175\r",
      "Progress: 89.5% ... Training loss: 0.056 ... Validation loss: 0.176\r",
      "Progress: 89.5% ... Training loss: 0.065 ... Validation loss: 0.205\r",
      "Progress: 89.5% ... Training loss: 0.057 ... Validation loss: 0.171\r",
      "Progress: 89.5% ... Training loss: 0.058 ... Validation loss: 0.195\r",
      "Progress: 89.5% ... Training loss: 0.063 ... Validation loss: 0.161\r",
      "Progress: 89.6% ... Training loss: 0.056 ... Validation loss: 0.183\r",
      "Progress: 89.6% ... Training loss: 0.055 ... Validation loss: 0.189\r",
      "Progress: 89.6% ... Training loss: 0.055 ... Validation loss: 0.182\r",
      "Progress: 89.6% ... Training loss: 0.056 ... Validation loss: 0.187\r",
      "Progress: 89.6% ... Training loss: 0.063 ... Validation loss: 0.174\r",
      "Progress: 89.6% ... Training loss: 0.058 ... Validation loss: 0.198\r",
      "Progress: 89.6% ... Training loss: 0.070 ... Validation loss: 0.168\r",
      "Progress: 89.7% ... Training loss: 0.073 ... Validation loss: 0.230\r",
      "Progress: 89.7% ... Training loss: 0.066 ... Validation loss: 0.165\r",
      "Progress: 89.7% ... Training loss: 0.065 ... Validation loss: 0.203\r",
      "Progress: 89.7% ... Training loss: 0.057 ... Validation loss: 0.182\r",
      "Progress: 89.7% ... Training loss: 0.056 ... Validation loss: 0.175\r",
      "Progress: 89.7% ... Training loss: 0.057 ... Validation loss: 0.179\r",
      "Progress: 89.7% ... Training loss: 0.062 ... Validation loss: 0.167\r",
      "Progress: 89.7% ... Training loss: 0.061 ... Validation loss: 0.200\r",
      "Progress: 89.8% ... Training loss: 0.057 ... Validation loss: 0.165\r",
      "Progress: 89.8% ... Training loss: 0.056 ... Validation loss: 0.174\r",
      "Progress: 89.8% ... Training loss: 0.064 ... Validation loss: 0.189\r",
      "Progress: 89.8% ... Training loss: 0.061 ... Validation loss: 0.160\r",
      "Progress: 89.8% ... Training loss: 0.067 ... Validation loss: 0.208\r",
      "Progress: 89.8% ... Training loss: 0.057 ... Validation loss: 0.172\r",
      "Progress: 89.8% ... Training loss: 0.056 ... Validation loss: 0.185\r",
      "Progress: 89.8% ... Training loss: 0.056 ... Validation loss: 0.189\r",
      "Progress: 89.8% ... Training loss: 0.058 ... Validation loss: 0.183\r",
      "Progress: 89.9% ... Training loss: 0.066 ... Validation loss: 0.211\r",
      "Progress: 89.9% ... Training loss: 0.071 ... Validation loss: 0.167\r",
      "Progress: 89.9% ... Training loss: 0.056 ... Validation loss: 0.194\r",
      "Progress: 89.9% ... Training loss: 0.061 ... Validation loss: 0.208\r",
      "Progress: 89.9% ... Training loss: 0.070 ... Validation loss: 0.170\r",
      "Progress: 89.9% ... Training loss: 0.082 ... Validation loss: 0.239\r",
      "Progress: 89.9% ... Training loss: 0.075 ... Validation loss: 0.164\r",
      "Progress: 90.0% ... Training loss: 0.057 ... Validation loss: 0.199\r",
      "Progress: 90.0% ... Training loss: 0.056 ... Validation loss: 0.194\r",
      "Progress: 90.0% ... Training loss: 0.056 ... Validation loss: 0.186\r",
      "Progress: 90.0% ... Training loss: 0.056 ... Validation loss: 0.176\r",
      "Progress: 90.0% ... Training loss: 0.056 ... Validation loss: 0.197\r",
      "Progress: 90.0% ... Training loss: 0.056 ... Validation loss: 0.177\r",
      "Progress: 90.0% ... Training loss: 0.057 ... Validation loss: 0.200\r",
      "Progress: 90.0% ... Training loss: 0.057 ... Validation loss: 0.189\r",
      "Progress: 90.0% ... Training loss: 0.063 ... Validation loss: 0.225\r",
      "Progress: 90.1% ... Training loss: 0.062 ... Validation loss: 0.188\r",
      "Progress: 90.1% ... Training loss: 0.063 ... Validation loss: 0.222\r",
      "Progress: 90.1% ... Training loss: 0.065 ... Validation loss: 0.168\r",
      "Progress: 90.1% ... Training loss: 0.058 ... Validation loss: 0.212\r",
      "Progress: 90.1% ... Training loss: 0.056 ... Validation loss: 0.182\r",
      "Progress: 90.1% ... Training loss: 0.055 ... Validation loss: 0.187\r",
      "Progress: 90.1% ... Training loss: 0.055 ... Validation loss: 0.194\r",
      "Progress: 90.2% ... Training loss: 0.058 ... Validation loss: 0.212\r",
      "Progress: 90.2% ... Training loss: 0.055 ... Validation loss: 0.192\r",
      "Progress: 90.2% ... Training loss: 0.058 ... Validation loss: 0.185\r",
      "Progress: 90.2% ... Training loss: 0.055 ... Validation loss: 0.195\r",
      "Progress: 90.2% ... Training loss: 0.055 ... Validation loss: 0.193\r",
      "Progress: 90.2% ... Training loss: 0.058 ... Validation loss: 0.190\r",
      "Progress: 90.2% ... Training loss: 0.058 ... Validation loss: 0.208\r",
      "Progress: 90.2% ... Training loss: 0.068 ... Validation loss: 0.167\r",
      "Progress: 90.2% ... Training loss: 0.058 ... Validation loss: 0.205\r",
      "Progress: 90.3% ... Training loss: 0.056 ... Validation loss: 0.198\r",
      "Progress: 90.3% ... Training loss: 0.056 ... Validation loss: 0.189\r",
      "Progress: 90.3% ... Training loss: 0.055 ... Validation loss: 0.178\r",
      "Progress: 90.3% ... Training loss: 0.055 ... Validation loss: 0.189\r",
      "Progress: 90.3% ... Training loss: 0.055 ... Validation loss: 0.185\r",
      "Progress: 90.3% ... Training loss: 0.055 ... Validation loss: 0.191\r",
      "Progress: 90.3% ... Training loss: 0.055 ... Validation loss: 0.187\r",
      "Progress: 90.3% ... Training loss: 0.056 ... Validation loss: 0.172\r",
      "Progress: 90.4% ... Training loss: 0.055 ... Validation loss: 0.179\r",
      "Progress: 90.4% ... Training loss: 0.058 ... Validation loss: 0.161\r",
      "Progress: 90.4% ... Training loss: 0.057 ... Validation loss: 0.188\r",
      "Progress: 90.4% ... Training loss: 0.061 ... Validation loss: 0.164\r",
      "Progress: 90.4% ... Training loss: 0.059 ... Validation loss: 0.162\r",
      "Progress: 90.4% ... Training loss: 0.055 ... Validation loss: 0.175\r",
      "Progress: 90.4% ... Training loss: 0.056 ... Validation loss: 0.172\r",
      "Progress: 90.5% ... Training loss: 0.055 ... Validation loss: 0.174\r",
      "Progress: 90.5% ... Training loss: 0.056 ... Validation loss: 0.189\r",
      "Progress: 90.5% ... Training loss: 0.057 ... Validation loss: 0.185\r",
      "Progress: 90.5% ... Training loss: 0.057 ... Validation loss: 0.182\r",
      "Progress: 90.5% ... Training loss: 0.055 ... Validation loss: 0.195\r",
      "Progress: 90.5% ... Training loss: 0.057 ... Validation loss: 0.184\r",
      "Progress: 90.5% ... Training loss: 0.061 ... Validation loss: 0.220\r",
      "Progress: 90.5% ... Training loss: 0.059 ... Validation loss: 0.180\r",
      "Progress: 90.5% ... Training loss: 0.059 ... Validation loss: 0.213\r",
      "Progress: 90.6% ... Training loss: 0.055 ... Validation loss: 0.186\r",
      "Progress: 90.6% ... Training loss: 0.061 ... Validation loss: 0.207\r",
      "Progress: 90.6% ... Training loss: 0.058 ... Validation loss: 0.180\r",
      "Progress: 90.6% ... Training loss: 0.059 ... Validation loss: 0.185\r",
      "Progress: 90.6% ... Training loss: 0.059 ... Validation loss: 0.220\r",
      "Progress: 90.6% ... Training loss: 0.056 ... Validation loss: 0.180\r",
      "Progress: 90.6% ... Training loss: 0.055 ... Validation loss: 0.180\r",
      "Progress: 90.7% ... Training loss: 0.056 ... Validation loss: 0.183\r",
      "Progress: 90.7% ... Training loss: 0.056 ... Validation loss: 0.189\r",
      "Progress: 90.7% ... Training loss: 0.055 ... Validation loss: 0.181\r",
      "Progress: 90.7% ... Training loss: 0.056 ... Validation loss: 0.187\r",
      "Progress: 90.7% ... Training loss: 0.055 ... Validation loss: 0.175\r",
      "Progress: 90.7% ... Training loss: 0.074 ... Validation loss: 0.216\r",
      "Progress: 90.7% ... Training loss: 0.063 ... Validation loss: 0.165\r",
      "Progress: 90.7% ... Training loss: 0.062 ... Validation loss: 0.207\r",
      "Progress: 90.8% ... Training loss: 0.062 ... Validation loss: 0.158\r",
      "Progress: 90.8% ... Training loss: 0.055 ... Validation loss: 0.188\r",
      "Progress: 90.8% ... Training loss: 0.055 ... Validation loss: 0.195\r",
      "Progress: 90.8% ... Training loss: 0.055 ... Validation loss: 0.180\r",
      "Progress: 90.8% ... Training loss: 0.057 ... Validation loss: 0.201\r",
      "Progress: 90.8% ... Training loss: 0.057 ... Validation loss: 0.179\r",
      "Progress: 90.8% ... Training loss: 0.058 ... Validation loss: 0.207\r",
      "Progress: 90.8% ... Training loss: 0.058 ... Validation loss: 0.177\r",
      "Progress: 90.8% ... Training loss: 0.058 ... Validation loss: 0.192\r",
      "Progress: 90.9% ... Training loss: 0.059 ... Validation loss: 0.175\r",
      "Progress: 90.9% ... Training loss: 0.059 ... Validation loss: 0.223\r",
      "Progress: 90.9% ... Training loss: 0.061 ... Validation loss: 0.169\r",
      "Progress: 90.9% ... Training loss: 0.058 ... Validation loss: 0.204\r",
      "Progress: 90.9% ... Training loss: 0.057 ... Validation loss: 0.186\r",
      "Progress: 90.9% ... Training loss: 0.055 ... Validation loss: 0.205\r",
      "Progress: 90.9% ... Training loss: 0.056 ... Validation loss: 0.194\r",
      "Progress: 91.0% ... Training loss: 0.060 ... Validation loss: 0.222\r",
      "Progress: 91.0% ... Training loss: 0.057 ... Validation loss: 0.182\r",
      "Progress: 91.0% ... Training loss: 0.056 ... Validation loss: 0.195\r",
      "Progress: 91.0% ... Training loss: 0.056 ... Validation loss: 0.207\r",
      "Progress: 91.0% ... Training loss: 0.056 ... Validation loss: 0.201\r",
      "Progress: 91.0% ... Training loss: 0.060 ... Validation loss: 0.187\r",
      "Progress: 91.0% ... Training loss: 0.067 ... Validation loss: 0.239\r",
      "Progress: 91.0% ... Training loss: 0.060 ... Validation loss: 0.184\r",
      "Progress: 91.0% ... Training loss: 0.063 ... Validation loss: 0.253\r",
      "Progress: 91.1% ... Training loss: 0.058 ... Validation loss: 0.185\r",
      "Progress: 91.1% ... Training loss: 0.057 ... Validation loss: 0.220\r",
      "Progress: 91.1% ... Training loss: 0.063 ... Validation loss: 0.180\r",
      "Progress: 91.1% ... Training loss: 0.064 ... Validation loss: 0.228\r",
      "Progress: 91.1% ... Training loss: 0.056 ... Validation loss: 0.190\r",
      "Progress: 91.1% ... Training loss: 0.063 ... Validation loss: 0.218\r",
      "Progress: 91.1% ... Training loss: 0.057 ... Validation loss: 0.180\r",
      "Progress: 91.2% ... Training loss: 0.056 ... Validation loss: 0.185\r",
      "Progress: 91.2% ... Training loss: 0.055 ... Validation loss: 0.177\r",
      "Progress: 91.2% ... Training loss: 0.059 ... Validation loss: 0.221\r",
      "Progress: 91.2% ... Training loss: 0.063 ... Validation loss: 0.166\r",
      "Progress: 91.2% ... Training loss: 0.056 ... Validation loss: 0.191\r",
      "Progress: 91.2% ... Training loss: 0.063 ... Validation loss: 0.218\r",
      "Progress: 91.2% ... Training loss: 0.060 ... Validation loss: 0.171\r",
      "Progress: 91.2% ... Training loss: 0.063 ... Validation loss: 0.222\r",
      "Progress: 91.2% ... Training loss: 0.058 ... Validation loss: 0.173\r",
      "Progress: 91.3% ... Training loss: 0.055 ... Validation loss: 0.197\r",
      "Progress: 91.3% ... Training loss: 0.057 ... Validation loss: 0.189\r",
      "Progress: 91.3% ... Training loss: 0.055 ... Validation loss: 0.191\r",
      "Progress: 91.3% ... Training loss: 0.055 ... Validation loss: 0.192\r",
      "Progress: 91.3% ... Training loss: 0.055 ... Validation loss: 0.180\r",
      "Progress: 91.3% ... Training loss: 0.055 ... Validation loss: 0.174\r",
      "Progress: 91.3% ... Training loss: 0.056 ... Validation loss: 0.180\r",
      "Progress: 91.3% ... Training loss: 0.057 ... Validation loss: 0.191\r",
      "Progress: 91.4% ... Training loss: 0.056 ... Validation loss: 0.187\r",
      "Progress: 91.4% ... Training loss: 0.058 ... Validation loss: 0.211\r",
      "Progress: 91.4% ... Training loss: 0.055 ... Validation loss: 0.183\r",
      "Progress: 91.4% ... Training loss: 0.055 ... Validation loss: 0.183\r",
      "Progress: 91.4% ... Training loss: 0.056 ... Validation loss: 0.199\r",
      "Progress: 91.4% ... Training loss: 0.055 ... Validation loss: 0.179\r",
      "Progress: 91.4% ... Training loss: 0.063 ... Validation loss: 0.214\r",
      "Progress: 91.5% ... Training loss: 0.058 ... Validation loss: 0.176\r",
      "Progress: 91.5% ... Training loss: 0.057 ... Validation loss: 0.202\r",
      "Progress: 91.5% ... Training loss: 0.055 ... Validation loss: 0.183\r",
      "Progress: 91.5% ... Training loss: 0.056 ... Validation loss: 0.188\r",
      "Progress: 91.5% ... Training loss: 0.059 ... Validation loss: 0.173\r",
      "Progress: 91.5% ... Training loss: 0.055 ... Validation loss: 0.183\r",
      "Progress: 91.5% ... Training loss: 0.055 ... Validation loss: 0.185\r",
      "Progress: 91.5% ... Training loss: 0.055 ... Validation loss: 0.192\r",
      "Progress: 91.5% ... Training loss: 0.055 ... Validation loss: 0.185\r",
      "Progress: 91.6% ... Training loss: 0.057 ... Validation loss: 0.207\r",
      "Progress: 91.6% ... Training loss: 0.057 ... Validation loss: 0.193\r",
      "Progress: 91.6% ... Training loss: 0.055 ... Validation loss: 0.204\r",
      "Progress: 91.6% ... Training loss: 0.055 ... Validation loss: 0.191\r",
      "Progress: 91.6% ... Training loss: 0.056 ... Validation loss: 0.183\r",
      "Progress: 91.6% ... Training loss: 0.056 ... Validation loss: 0.198\r",
      "Progress: 91.6% ... Training loss: 0.055 ... Validation loss: 0.179\r",
      "Progress: 91.7% ... Training loss: 0.057 ... Validation loss: 0.180\r",
      "Progress: 91.7% ... Training loss: 0.055 ... Validation loss: 0.190\r",
      "Progress: 91.7% ... Training loss: 0.059 ... Validation loss: 0.199\r",
      "Progress: 91.7% ... Training loss: 0.065 ... Validation loss: 0.165\r",
      "Progress: 91.7% ... Training loss: 0.064 ... Validation loss: 0.205\r",
      "Progress: 91.7% ... Training loss: 0.067 ... Validation loss: 0.164\r",
      "Progress: 91.7% ... Training loss: 0.057 ... Validation loss: 0.201\r",
      "Progress: 91.7% ... Training loss: 0.061 ... Validation loss: 0.176\r",
      "Progress: 91.8% ... Training loss: 0.059 ... Validation loss: 0.215\r",
      "Progress: 91.8% ... Training loss: 0.055 ... Validation loss: 0.189\r",
      "Progress: 91.8% ... Training loss: 0.055 ... Validation loss: 0.202\r",
      "Progress: 91.8% ... Training loss: 0.055 ... Validation loss: 0.198\r",
      "Progress: 91.8% ... Training loss: 0.055 ... Validation loss: 0.184\r",
      "Progress: 91.8% ... Training loss: 0.055 ... Validation loss: 0.206\r",
      "Progress: 91.8% ... Training loss: 0.055 ... Validation loss: 0.203\r",
      "Progress: 91.8% ... Training loss: 0.058 ... Validation loss: 0.180\r",
      "Progress: 91.8% ... Training loss: 0.055 ... Validation loss: 0.192\r",
      "Progress: 91.9% ... Training loss: 0.059 ... Validation loss: 0.187\r",
      "Progress: 91.9% ... Training loss: 0.068 ... Validation loss: 0.249\r",
      "Progress: 91.9% ... Training loss: 0.063 ... Validation loss: 0.175\r",
      "Progress: 91.9% ... Training loss: 0.070 ... Validation loss: 0.227\r",
      "Progress: 91.9% ... Training loss: 0.065 ... Validation loss: 0.164\r",
      "Progress: 91.9% ... Training loss: 0.064 ... Validation loss: 0.222\r",
      "Progress: 91.9% ... Training loss: 0.068 ... Validation loss: 0.166\r",
      "Progress: 92.0% ... Training loss: 0.055 ... Validation loss: 0.188\r",
      "Progress: 92.0% ... Training loss: 0.057 ... Validation loss: 0.184\r",
      "Progress: 92.0% ... Training loss: 0.057 ... Validation loss: 0.181\r",
      "Progress: 92.0% ... Training loss: 0.057 ... Validation loss: 0.211\r",
      "Progress: 92.0% ... Training loss: 0.061 ... Validation loss: 0.177\r",
      "Progress: 92.0% ... Training loss: 0.066 ... Validation loss: 0.233\r",
      "Progress: 92.0% ... Training loss: 0.058 ... Validation loss: 0.170\r",
      "Progress: 92.0% ... Training loss: 0.057 ... Validation loss: 0.194\r",
      "Progress: 92.0% ... Training loss: 0.058 ... Validation loss: 0.187\r",
      "Progress: 92.1% ... Training loss: 0.057 ... Validation loss: 0.202\r",
      "Progress: 92.1% ... Training loss: 0.056 ... Validation loss: 0.207\r",
      "Progress: 92.1% ... Training loss: 0.058 ... Validation loss: 0.193\r",
      "Progress: 92.1% ... Training loss: 0.062 ... Validation loss: 0.229\r",
      "Progress: 92.1% ... Training loss: 0.055 ... Validation loss: 0.184\r",
      "Progress: 92.1% ... Training loss: 0.055 ... Validation loss: 0.195\r",
      "Progress: 92.1% ... Training loss: 0.056 ... Validation loss: 0.209\r",
      "Progress: 92.2% ... Training loss: 0.067 ... Validation loss: 0.167\r",
      "Progress: 92.2% ... Training loss: 0.067 ... Validation loss: 0.255\r",
      "Progress: 92.2% ... Training loss: 0.059 ... Validation loss: 0.190\r",
      "Progress: 92.2% ... Training loss: 0.068 ... Validation loss: 0.249\r",
      "Progress: 92.2% ... Training loss: 0.060 ... Validation loss: 0.196\r",
      "Progress: 92.2% ... Training loss: 0.058 ... Validation loss: 0.235\r",
      "Progress: 92.2% ... Training loss: 0.063 ... Validation loss: 0.178\r",
      "Progress: 92.2% ... Training loss: 0.057 ... Validation loss: 0.198\r",
      "Progress: 92.2% ... Training loss: 0.055 ... Validation loss: 0.187\r",
      "Progress: 92.3% ... Training loss: 0.055 ... Validation loss: 0.197\r",
      "Progress: 92.3% ... Training loss: 0.059 ... Validation loss: 0.181\r",
      "Progress: 92.3% ... Training loss: 0.058 ... Validation loss: 0.213\r",
      "Progress: 92.3% ... Training loss: 0.062 ... Validation loss: 0.222\r",
      "Progress: 92.3% ... Training loss: 0.056 ... Validation loss: 0.193\r",
      "Progress: 92.3% ... Training loss: 0.056 ... Validation loss: 0.222\r",
      "Progress: 92.3% ... Training loss: 0.055 ... Validation loss: 0.209\r",
      "Progress: 92.3% ... Training loss: 0.060 ... Validation loss: 0.190\r",
      "Progress: 92.4% ... Training loss: 0.057 ... Validation loss: 0.220\r",
      "Progress: 92.4% ... Training loss: 0.062 ... Validation loss: 0.181\r",
      "Progress: 92.4% ... Training loss: 0.059 ... Validation loss: 0.217\r",
      "Progress: 92.4% ... Training loss: 0.060 ... Validation loss: 0.172\r",
      "Progress: 92.4% ... Training loss: 0.059 ... Validation loss: 0.205\r",
      "Progress: 92.4% ... Training loss: 0.075 ... Validation loss: 0.167\r",
      "Progress: 92.4% ... Training loss: 0.088 ... Validation loss: 0.270\r",
      "Progress: 92.5% ... Training loss: 0.077 ... Validation loss: 0.161\r",
      "Progress: 92.5% ... Training loss: 0.076 ... Validation loss: 0.245\r",
      "Progress: 92.5% ... Training loss: 0.071 ... Validation loss: 0.171\r",
      "Progress: 92.5% ... Training loss: 0.078 ... Validation loss: 0.261\r",
      "Progress: 92.5% ... Training loss: 0.063 ... Validation loss: 0.182\r",
      "Progress: 92.5% ... Training loss: 0.057 ... Validation loss: 0.225\r",
      "Progress: 92.5% ... Training loss: 0.057 ... Validation loss: 0.221\r",
      "Progress: 92.5% ... Training loss: 0.058 ... Validation loss: 0.193\r",
      "Progress: 92.5% ... Training loss: 0.057 ... Validation loss: 0.222\r",
      "Progress: 92.6% ... Training loss: 0.057 ... Validation loss: 0.217\r",
      "Progress: 92.6% ... Training loss: 0.062 ... Validation loss: 0.183\r",
      "Progress: 92.6% ... Training loss: 0.055 ... Validation loss: 0.200\r",
      "Progress: 92.6% ... Training loss: 0.057 ... Validation loss: 0.196\r",
      "Progress: 92.6% ... Training loss: 0.055 ... Validation loss: 0.181\r",
      "Progress: 92.6% ... Training loss: 0.056 ... Validation loss: 0.186\r",
      "Progress: 92.6% ... Training loss: 0.057 ... Validation loss: 0.185\r",
      "Progress: 92.7% ... Training loss: 0.055 ... Validation loss: 0.200\r",
      "Progress: 92.7% ... Training loss: 0.056 ... Validation loss: 0.206\r",
      "Progress: 92.7% ... Training loss: 0.058 ... Validation loss: 0.188\r",
      "Progress: 92.7% ... Training loss: 0.055 ... Validation loss: 0.205\r",
      "Progress: 92.7% ... Training loss: 0.055 ... Validation loss: 0.210\r",
      "Progress: 92.7% ... Training loss: 0.054 ... Validation loss: 0.207\r",
      "Progress: 92.7% ... Training loss: 0.058 ... Validation loss: 0.179\r",
      "Progress: 92.7% ... Training loss: 0.061 ... Validation loss: 0.210\r",
      "Progress: 92.8% ... Training loss: 0.059 ... Validation loss: 0.177\r",
      "Progress: 92.8% ... Training loss: 0.056 ... Validation loss: 0.200\r",
      "Progress: 92.8% ... Training loss: 0.065 ... Validation loss: 0.177\r",
      "Progress: 92.8% ... Training loss: 0.069 ... Validation loss: 0.244\r",
      "Progress: 92.8% ... Training loss: 0.058 ... Validation loss: 0.186\r",
      "Progress: 92.8% ... Training loss: 0.055 ... Validation loss: 0.177\r",
      "Progress: 92.8% ... Training loss: 0.055 ... Validation loss: 0.190\r",
      "Progress: 92.8% ... Training loss: 0.055 ... Validation loss: 0.185\r",
      "Progress: 92.8% ... Training loss: 0.058 ... Validation loss: 0.173\r",
      "Progress: 92.9% ... Training loss: 0.056 ... Validation loss: 0.191\r",
      "Progress: 92.9% ... Training loss: 0.056 ... Validation loss: 0.181\r",
      "Progress: 92.9% ... Training loss: 0.055 ... Validation loss: 0.188\r",
      "Progress: 92.9% ... Training loss: 0.057 ... Validation loss: 0.183\r",
      "Progress: 92.9% ... Training loss: 0.056 ... Validation loss: 0.198\r",
      "Progress: 92.9% ... Training loss: 0.057 ... Validation loss: 0.199\r",
      "Progress: 92.9% ... Training loss: 0.056 ... Validation loss: 0.188\r",
      "Progress: 93.0% ... Training loss: 0.056 ... Validation loss: 0.172\r",
      "Progress: 93.0% ... Training loss: 0.057 ... Validation loss: 0.187\r",
      "Progress: 93.0% ... Training loss: 0.056 ... Validation loss: 0.168\r",
      "Progress: 93.0% ... Training loss: 0.055 ... Validation loss: 0.182\r",
      "Progress: 93.0% ... Training loss: 0.055 ... Validation loss: 0.178\r",
      "Progress: 93.0% ... Training loss: 0.055 ... Validation loss: 0.187\r",
      "Progress: 93.0% ... Training loss: 0.055 ... Validation loss: 0.190\r",
      "Progress: 93.0% ... Training loss: 0.056 ... Validation loss: 0.176\r",
      "Progress: 93.0% ... Training loss: 0.054 ... Validation loss: 0.189\r",
      "Progress: 93.1% ... Training loss: 0.054 ... Validation loss: 0.188\r",
      "Progress: 93.1% ... Training loss: 0.060 ... Validation loss: 0.221\r",
      "Progress: 93.1% ... Training loss: 0.056 ... Validation loss: 0.182\r",
      "Progress: 93.1% ... Training loss: 0.055 ... Validation loss: 0.211\r",
      "Progress: 93.1% ... Training loss: 0.062 ... Validation loss: 0.187\r",
      "Progress: 93.1% ... Training loss: 0.056 ... Validation loss: 0.216\r",
      "Progress: 93.1% ... Training loss: 0.063 ... Validation loss: 0.177\r",
      "Progress: 93.2% ... Training loss: 0.058 ... Validation loss: 0.220\r",
      "Progress: 93.2% ... Training loss: 0.060 ... Validation loss: 0.185\r",
      "Progress: 93.2% ... Training loss: 0.055 ... Validation loss: 0.196\r",
      "Progress: 93.2% ... Training loss: 0.056 ... Validation loss: 0.181\r",
      "Progress: 93.2% ... Training loss: 0.056 ... Validation loss: 0.188\r",
      "Progress: 93.2% ... Training loss: 0.057 ... Validation loss: 0.207\r",
      "Progress: 93.2% ... Training loss: 0.055 ... Validation loss: 0.197\r",
      "Progress: 93.2% ... Training loss: 0.057 ... Validation loss: 0.209\r",
      "Progress: 93.2% ... Training loss: 0.059 ... Validation loss: 0.176\r",
      "Progress: 93.3% ... Training loss: 0.056 ... Validation loss: 0.215\r",
      "Progress: 93.3% ... Training loss: 0.055 ... Validation loss: 0.204\r",
      "Progress: 93.3% ... Training loss: 0.056 ... Validation loss: 0.237\r",
      "Progress: 93.3% ... Training loss: 0.058 ... Validation loss: 0.214\r",
      "Progress: 93.3% ... Training loss: 0.058 ... Validation loss: 0.234\r",
      "Progress: 93.3% ... Training loss: 0.054 ... Validation loss: 0.207\r",
      "Progress: 93.3% ... Training loss: 0.055 ... Validation loss: 0.184\r",
      "Progress: 93.3% ... Training loss: 0.055 ... Validation loss: 0.197\r",
      "Progress: 93.4% ... Training loss: 0.054 ... Validation loss: 0.185\r",
      "Progress: 93.4% ... Training loss: 0.056 ... Validation loss: 0.209\r",
      "Progress: 93.4% ... Training loss: 0.056 ... Validation loss: 0.184\r",
      "Progress: 93.4% ... Training loss: 0.056 ... Validation loss: 0.199\r",
      "Progress: 93.4% ... Training loss: 0.055 ... Validation loss: 0.180\r",
      "Progress: 93.4% ... Training loss: 0.055 ... Validation loss: 0.178\r",
      "Progress: 93.4% ... Training loss: 0.056 ... Validation loss: 0.198\r",
      "Progress: 93.5% ... Training loss: 0.057 ... Validation loss: 0.211\r",
      "Progress: 93.5% ... Training loss: 0.054 ... Validation loss: 0.188\r",
      "Progress: 93.5% ... Training loss: 0.055 ... Validation loss: 0.194\r",
      "Progress: 93.5% ... Training loss: 0.056 ... Validation loss: 0.208\r",
      "Progress: 93.5% ... Training loss: 0.055 ... Validation loss: 0.204\r",
      "Progress: 93.5% ... Training loss: 0.056 ... Validation loss: 0.200\r",
      "Progress: 93.5% ... Training loss: 0.056 ... Validation loss: 0.213\r",
      "Progress: 93.5% ... Training loss: 0.058 ... Validation loss: 0.189\r",
      "Progress: 93.5% ... Training loss: 0.058 ... Validation loss: 0.222\r",
      "Progress: 93.6% ... Training loss: 0.058 ... Validation loss: 0.189\r",
      "Progress: 93.6% ... Training loss: 0.057 ... Validation loss: 0.211\r",
      "Progress: 93.6% ... Training loss: 0.056 ... Validation loss: 0.190\r",
      "Progress: 93.6% ... Training loss: 0.057 ... Validation loss: 0.211\r",
      "Progress: 93.6% ... Training loss: 0.055 ... Validation loss: 0.197\r",
      "Progress: 93.6% ... Training loss: 0.055 ... Validation loss: 0.202\r",
      "Progress: 93.6% ... Training loss: 0.055 ... Validation loss: 0.203\r",
      "Progress: 93.7% ... Training loss: 0.056 ... Validation loss: 0.210\r",
      "Progress: 93.7% ... Training loss: 0.056 ... Validation loss: 0.183\r",
      "Progress: 93.7% ... Training loss: 0.057 ... Validation loss: 0.223\r",
      "Progress: 93.7% ... Training loss: 0.056 ... Validation loss: 0.187\r",
      "Progress: 93.7% ... Training loss: 0.055 ... Validation loss: 0.195\r",
      "Progress: 93.7% ... Training loss: 0.064 ... Validation loss: 0.172\r",
      "Progress: 93.7% ... Training loss: 0.063 ... Validation loss: 0.220\r",
      "Progress: 93.7% ... Training loss: 0.055 ... Validation loss: 0.182\r",
      "Progress: 93.8% ... Training loss: 0.060 ... Validation loss: 0.225\r",
      "Progress: 93.8% ... Training loss: 0.054 ... Validation loss: 0.190\r",
      "Progress: 93.8% ... Training loss: 0.055 ... Validation loss: 0.202\r",
      "Progress: 93.8% ... Training loss: 0.056 ... Validation loss: 0.210\r",
      "Progress: 93.8% ... Training loss: 0.060 ... Validation loss: 0.185\r",
      "Progress: 93.8% ... Training loss: 0.074 ... Validation loss: 0.251\r",
      "Progress: 93.8% ... Training loss: 0.060 ... Validation loss: 0.181\r",
      "Progress: 93.8% ... Training loss: 0.060 ... Validation loss: 0.225\r",
      "Progress: 93.8% ... Training loss: 0.062 ... Validation loss: 0.180\r",
      "Progress: 93.9% ... Training loss: 0.055 ... Validation loss: 0.223\r",
      "Progress: 93.9% ... Training loss: 0.055 ... Validation loss: 0.219\r",
      "Progress: 93.9% ... Training loss: 0.057 ... Validation loss: 0.201\r",
      "Progress: 93.9% ... Training loss: 0.055 ... Validation loss: 0.220\r",
      "Progress: 93.9% ... Training loss: 0.055 ... Validation loss: 0.199\r",
      "Progress: 93.9% ... Training loss: 0.055 ... Validation loss: 0.200\r",
      "Progress: 93.9% ... Training loss: 0.059 ... Validation loss: 0.231\r",
      "Progress: 94.0% ... Training loss: 0.056 ... Validation loss: 0.190\r",
      "Progress: 94.0% ... Training loss: 0.056 ... Validation loss: 0.214\r",
      "Progress: 94.0% ... Training loss: 0.059 ... Validation loss: 0.174\r",
      "Progress: 94.0% ... Training loss: 0.055 ... Validation loss: 0.190\r",
      "Progress: 94.0% ... Training loss: 0.056 ... Validation loss: 0.184\r",
      "Progress: 94.0% ... Training loss: 0.056 ... Validation loss: 0.208\r",
      "Progress: 94.0% ... Training loss: 0.055 ... Validation loss: 0.197\r",
      "Progress: 94.0% ... Training loss: 0.058 ... Validation loss: 0.167\r",
      "Progress: 94.0% ... Training loss: 0.069 ... Validation loss: 0.194\r",
      "Progress: 94.1% ... Training loss: 0.060 ... Validation loss: 0.159\r",
      "Progress: 94.1% ... Training loss: 0.056 ... Validation loss: 0.200\r",
      "Progress: 94.1% ... Training loss: 0.056 ... Validation loss: 0.181\r",
      "Progress: 94.1% ... Training loss: 0.055 ... Validation loss: 0.186\r",
      "Progress: 94.1% ... Training loss: 0.055 ... Validation loss: 0.179\r",
      "Progress: 94.1% ... Training loss: 0.055 ... Validation loss: 0.173\r",
      "Progress: 94.1% ... Training loss: 0.056 ... Validation loss: 0.199\r",
      "Progress: 94.2% ... Training loss: 0.063 ... Validation loss: 0.173\r",
      "Progress: 94.2% ... Training loss: 0.055 ... Validation loss: 0.195\r",
      "Progress: 94.2% ... Training loss: 0.060 ... Validation loss: 0.159\r",
      "Progress: 94.2% ... Training loss: 0.061 ... Validation loss: 0.180\r",
      "Progress: 94.2% ... Training loss: 0.059 ... Validation loss: 0.155\r",
      "Progress: 94.2% ... Training loss: 0.056 ... Validation loss: 0.185\r",
      "Progress: 94.2% ... Training loss: 0.060 ... Validation loss: 0.165\r",
      "Progress: 94.2% ... Training loss: 0.059 ... Validation loss: 0.201\r",
      "Progress: 94.2% ... Training loss: 0.058 ... Validation loss: 0.174\r",
      "Progress: 94.3% ... Training loss: 0.067 ... Validation loss: 0.208\r",
      "Progress: 94.3% ... Training loss: 0.060 ... Validation loss: 0.166\r",
      "Progress: 94.3% ... Training loss: 0.068 ... Validation loss: 0.207\r",
      "Progress: 94.3% ... Training loss: 0.057 ... Validation loss: 0.168\r",
      "Progress: 94.3% ... Training loss: 0.057 ... Validation loss: 0.194\r",
      "Progress: 94.3% ... Training loss: 0.055 ... Validation loss: 0.182\r",
      "Progress: 94.3% ... Training loss: 0.055 ... Validation loss: 0.190\r",
      "Progress: 94.3% ... Training loss: 0.055 ... Validation loss: 0.187\r",
      "Progress: 94.4% ... Training loss: 0.056 ... Validation loss: 0.186\r",
      "Progress: 94.4% ... Training loss: 0.065 ... Validation loss: 0.162\r",
      "Progress: 94.4% ... Training loss: 0.059 ... Validation loss: 0.208\r",
      "Progress: 94.4% ... Training loss: 0.056 ... Validation loss: 0.199\r",
      "Progress: 94.4% ... Training loss: 0.055 ... Validation loss: 0.178\r",
      "Progress: 94.4% ... Training loss: 0.057 ... Validation loss: 0.177\r",
      "Progress: 94.4% ... Training loss: 0.057 ... Validation loss: 0.200\r",
      "Progress: 94.5% ... Training loss: 0.055 ... Validation loss: 0.201\r",
      "Progress: 94.5% ... Training loss: 0.055 ... Validation loss: 0.186\r",
      "Progress: 94.5% ... Training loss: 0.055 ... Validation loss: 0.191\r",
      "Progress: 94.5% ... Training loss: 0.058 ... Validation loss: 0.218\r",
      "Progress: 94.5% ... Training loss: 0.058 ... Validation loss: 0.189\r",
      "Progress: 94.5% ... Training loss: 0.058 ... Validation loss: 0.207\r",
      "Progress: 94.5% ... Training loss: 0.056 ... Validation loss: 0.199\r",
      "Progress: 94.5% ... Training loss: 0.060 ... Validation loss: 0.220\r",
      "Progress: 94.5% ... Training loss: 0.063 ... Validation loss: 0.171\r",
      "Progress: 94.6% ... Training loss: 0.056 ... Validation loss: 0.211\r",
      "Progress: 94.6% ... Training loss: 0.057 ... Validation loss: 0.185\r",
      "Progress: 94.6% ... Training loss: 0.058 ... Validation loss: 0.193\r",
      "Progress: 94.6% ... Training loss: 0.057 ... Validation loss: 0.172\r",
      "Progress: 94.6% ... Training loss: 0.056 ... Validation loss: 0.191\r",
      "Progress: 94.6% ... Training loss: 0.063 ... Validation loss: 0.163\r",
      "Progress: 94.6% ... Training loss: 0.076 ... Validation loss: 0.236\r",
      "Progress: 94.7% ... Training loss: 0.070 ... Validation loss: 0.160\r",
      "Progress: 94.7% ... Training loss: 0.073 ... Validation loss: 0.231\r",
      "Progress: 94.7% ... Training loss: 0.058 ... Validation loss: 0.181\r",
      "Progress: 94.7% ... Training loss: 0.061 ... Validation loss: 0.205\r",
      "Progress: 94.7% ... Training loss: 0.056 ... Validation loss: 0.176\r",
      "Progress: 94.7% ... Training loss: 0.055 ... Validation loss: 0.184\r",
      "Progress: 94.7% ... Training loss: 0.055 ... Validation loss: 0.188\r",
      "Progress: 94.7% ... Training loss: 0.057 ... Validation loss: 0.191\r",
      "Progress: 94.8% ... Training loss: 0.056 ... Validation loss: 0.192\r",
      "Progress: 94.8% ... Training loss: 0.057 ... Validation loss: 0.172\r",
      "Progress: 94.8% ... Training loss: 0.056 ... Validation loss: 0.181\r",
      "Progress: 94.8% ... Training loss: 0.058 ... Validation loss: 0.170\r",
      "Progress: 94.8% ... Training loss: 0.057 ... Validation loss: 0.210\r",
      "Progress: 94.8% ... Training loss: 0.054 ... Validation loss: 0.197\r",
      "Progress: 94.8% ... Training loss: 0.057 ... Validation loss: 0.209\r",
      "Progress: 94.8% ... Training loss: 0.055 ... Validation loss: 0.186\r",
      "Progress: 94.8% ... Training loss: 0.055 ... Validation loss: 0.196\r",
      "Progress: 94.9% ... Training loss: 0.055 ... Validation loss: 0.209\r",
      "Progress: 94.9% ... Training loss: 0.056 ... Validation loss: 0.209\r",
      "Progress: 94.9% ... Training loss: 0.056 ... Validation loss: 0.198\r",
      "Progress: 94.9% ... Training loss: 0.055 ... Validation loss: 0.208\r",
      "Progress: 94.9% ... Training loss: 0.058 ... Validation loss: 0.181\r",
      "Progress: 94.9% ... Training loss: 0.056 ... Validation loss: 0.212\r",
      "Progress: 94.9% ... Training loss: 0.055 ... Validation loss: 0.202\r",
      "Progress: 95.0% ... Training loss: 0.056 ... Validation loss: 0.188\r",
      "Progress: 95.0% ... Training loss: 0.057 ... Validation loss: 0.206\r",
      "Progress: 95.0% ... Training loss: 0.060 ... Validation loss: 0.180\r",
      "Progress: 95.0% ... Training loss: 0.066 ... Validation loss: 0.230\r",
      "Progress: 95.0% ... Training loss: 0.072 ... Validation loss: 0.169\r",
      "Progress: 95.0% ... Training loss: 0.066 ... Validation loss: 0.237\r",
      "Progress: 95.0% ... Training loss: 0.062 ... Validation loss: 0.173\r",
      "Progress: 95.0% ... Training loss: 0.062 ... Validation loss: 0.234\r",
      "Progress: 95.0% ... Training loss: 0.059 ... Validation loss: 0.179\r",
      "Progress: 95.1% ... Training loss: 0.056 ... Validation loss: 0.211\r",
      "Progress: 95.1% ... Training loss: 0.055 ... Validation loss: 0.205\r",
      "Progress: 95.1% ... Training loss: 0.055 ... Validation loss: 0.196\r",
      "Progress: 95.1% ... Training loss: 0.062 ... Validation loss: 0.176\r",
      "Progress: 95.1% ... Training loss: 0.070 ... Validation loss: 0.243\r",
      "Progress: 95.1% ... Training loss: 0.071 ... Validation loss: 0.160\r",
      "Progress: 95.1% ... Training loss: 0.071 ... Validation loss: 0.235\r",
      "Progress: 95.2% ... Training loss: 0.069 ... Validation loss: 0.171\r",
      "Progress: 95.2% ... Training loss: 0.060 ... Validation loss: 0.232\r",
      "Progress: 95.2% ... Training loss: 0.054 ... Validation loss: 0.190\r",
      "Progress: 95.2% ... Training loss: 0.056 ... Validation loss: 0.200\r",
      "Progress: 95.2% ... Training loss: 0.055 ... Validation loss: 0.203\r",
      "Progress: 95.2% ... Training loss: 0.059 ... Validation loss: 0.169\r",
      "Progress: 95.2% ... Training loss: 0.057 ... Validation loss: 0.192\r",
      "Progress: 95.2% ... Training loss: 0.061 ... Validation loss: 0.174\r",
      "Progress: 95.2% ... Training loss: 0.062 ... Validation loss: 0.206\r",
      "Progress: 95.3% ... Training loss: 0.057 ... Validation loss: 0.172\r",
      "Progress: 95.3% ... Training loss: 0.055 ... Validation loss: 0.187\r",
      "Progress: 95.3% ... Training loss: 0.057 ... Validation loss: 0.182\r",
      "Progress: 95.3% ... Training loss: 0.059 ... Validation loss: 0.167\r",
      "Progress: 95.3% ... Training loss: 0.057 ... Validation loss: 0.191\r",
      "Progress: 95.3% ... Training loss: 0.059 ... Validation loss: 0.166\r",
      "Progress: 95.3% ... Training loss: 0.055 ... Validation loss: 0.183\r",
      "Progress: 95.3% ... Training loss: 0.055 ... Validation loss: 0.183\r",
      "Progress: 95.4% ... Training loss: 0.054 ... Validation loss: 0.184\r",
      "Progress: 95.4% ... Training loss: 0.054 ... Validation loss: 0.195\r",
      "Progress: 95.4% ... Training loss: 0.054 ... Validation loss: 0.187\r",
      "Progress: 95.4% ... Training loss: 0.054 ... Validation loss: 0.189\r",
      "Progress: 95.4% ... Training loss: 0.056 ... Validation loss: 0.201\r",
      "Progress: 95.4% ... Training loss: 0.054 ... Validation loss: 0.190\r",
      "Progress: 95.4% ... Training loss: 0.054 ... Validation loss: 0.191\r",
      "Progress: 95.5% ... Training loss: 0.054 ... Validation loss: 0.196\r",
      "Progress: 95.5% ... Training loss: 0.057 ... Validation loss: 0.215\r",
      "Progress: 95.5% ... Training loss: 0.054 ... Validation loss: 0.186\r",
      "Progress: 95.5% ... Training loss: 0.055 ... Validation loss: 0.175\r",
      "Progress: 95.5% ... Training loss: 0.055 ... Validation loss: 0.191\r",
      "Progress: 95.5% ... Training loss: 0.058 ... Validation loss: 0.167\r",
      "Progress: 95.5% ... Training loss: 0.055 ... Validation loss: 0.187\r",
      "Progress: 95.5% ... Training loss: 0.054 ... Validation loss: 0.184\r",
      "Progress: 95.5% ... Training loss: 0.055 ... Validation loss: 0.199\r",
      "Progress: 95.6% ... Training loss: 0.058 ... Validation loss: 0.170\r",
      "Progress: 95.6% ... Training loss: 0.054 ... Validation loss: 0.185\r",
      "Progress: 95.6% ... Training loss: 0.054 ... Validation loss: 0.180\r",
      "Progress: 95.6% ... Training loss: 0.057 ... Validation loss: 0.198\r",
      "Progress: 95.6% ... Training loss: 0.055 ... Validation loss: 0.170\r",
      "Progress: 95.6% ... Training loss: 0.055 ... Validation loss: 0.186\r",
      "Progress: 95.6% ... Training loss: 0.056 ... Validation loss: 0.174\r",
      "Progress: 95.7% ... Training loss: 0.063 ... Validation loss: 0.198\r",
      "Progress: 95.7% ... Training loss: 0.058 ... Validation loss: 0.166\r",
      "Progress: 95.7% ... Training loss: 0.064 ... Validation loss: 0.195\r",
      "Progress: 95.7% ... Training loss: 0.057 ... Validation loss: 0.161\r",
      "Progress: 95.7% ... Training loss: 0.055 ... Validation loss: 0.180\r",
      "Progress: 95.7% ... Training loss: 0.059 ... Validation loss: 0.194\r",
      "Progress: 95.7% ... Training loss: 0.059 ... Validation loss: 0.165\r",
      "Progress: 95.7% ... Training loss: 0.056 ... Validation loss: 0.177\r",
      "Progress: 95.8% ... Training loss: 0.055 ... Validation loss: 0.162\r",
      "Progress: 95.8% ... Training loss: 0.058 ... Validation loss: 0.202\r",
      "Progress: 95.8% ... Training loss: 0.055 ... Validation loss: 0.177\r",
      "Progress: 95.8% ... Training loss: 0.056 ... Validation loss: 0.165\r",
      "Progress: 95.8% ... Training loss: 0.055 ... Validation loss: 0.179\r",
      "Progress: 95.8% ... Training loss: 0.056 ... Validation loss: 0.214\r",
      "Progress: 95.8% ... Training loss: 0.062 ... Validation loss: 0.180\r",
      "Progress: 95.8% ... Training loss: 0.061 ... Validation loss: 0.229\r",
      "Progress: 95.8% ... Training loss: 0.055 ... Validation loss: 0.186\r",
      "Progress: 95.9% ... Training loss: 0.055 ... Validation loss: 0.189\r",
      "Progress: 95.9% ... Training loss: 0.055 ... Validation loss: 0.202\r",
      "Progress: 95.9% ... Training loss: 0.062 ... Validation loss: 0.178\r",
      "Progress: 95.9% ... Training loss: 0.062 ... Validation loss: 0.218\r",
      "Progress: 95.9% ... Training loss: 0.059 ... Validation loss: 0.163\r",
      "Progress: 95.9% ... Training loss: 0.059 ... Validation loss: 0.202\r",
      "Progress: 95.9% ... Training loss: 0.055 ... Validation loss: 0.171\r",
      "Progress: 96.0% ... Training loss: 0.060 ... Validation loss: 0.196\r",
      "Progress: 96.0% ... Training loss: 0.063 ... Validation loss: 0.160\r",
      "Progress: 96.0% ... Training loss: 0.058 ... Validation loss: 0.192\r",
      "Progress: 96.0% ... Training loss: 0.065 ... Validation loss: 0.175\r",
      "Progress: 96.0% ... Training loss: 0.059 ... Validation loss: 0.222\r",
      "Progress: 96.0% ... Training loss: 0.058 ... Validation loss: 0.179\r",
      "Progress: 96.0% ... Training loss: 0.057 ... Validation loss: 0.206\r",
      "Progress: 96.0% ... Training loss: 0.057 ... Validation loss: 0.196\r",
      "Progress: 96.0% ... Training loss: 0.055 ... Validation loss: 0.184\r",
      "Progress: 96.1% ... Training loss: 0.055 ... Validation loss: 0.195\r",
      "Progress: 96.1% ... Training loss: 0.055 ... Validation loss: 0.210\r",
      "Progress: 96.1% ... Training loss: 0.059 ... Validation loss: 0.228\r",
      "Progress: 96.1% ... Training loss: 0.055 ... Validation loss: 0.200\r",
      "Progress: 96.1% ... Training loss: 0.054 ... Validation loss: 0.200\r",
      "Progress: 96.1% ... Training loss: 0.055 ... Validation loss: 0.210\r",
      "Progress: 96.1% ... Training loss: 0.054 ... Validation loss: 0.185\r",
      "Progress: 96.2% ... Training loss: 0.054 ... Validation loss: 0.187\r",
      "Progress: 96.2% ... Training loss: 0.054 ... Validation loss: 0.200\r",
      "Progress: 96.2% ... Training loss: 0.055 ... Validation loss: 0.186\r",
      "Progress: 96.2% ... Training loss: 0.056 ... Validation loss: 0.174\r",
      "Progress: 96.2% ... Training loss: 0.057 ... Validation loss: 0.201\r",
      "Progress: 96.2% ... Training loss: 0.055 ... Validation loss: 0.191\r",
      "Progress: 96.2% ... Training loss: 0.054 ... Validation loss: 0.183\r",
      "Progress: 96.2% ... Training loss: 0.055 ... Validation loss: 0.192\r",
      "Progress: 96.2% ... Training loss: 0.055 ... Validation loss: 0.183\r",
      "Progress: 96.3% ... Training loss: 0.056 ... Validation loss: 0.205\r",
      "Progress: 96.3% ... Training loss: 0.056 ... Validation loss: 0.181\r",
      "Progress: 96.3% ... Training loss: 0.054 ... Validation loss: 0.200\r",
      "Progress: 96.3% ... Training loss: 0.055 ... Validation loss: 0.198\r",
      "Progress: 96.3% ... Training loss: 0.056 ... Validation loss: 0.171\r",
      "Progress: 96.3% ... Training loss: 0.055 ... Validation loss: 0.194\r",
      "Progress: 96.3% ... Training loss: 0.055 ... Validation loss: 0.198\r",
      "Progress: 96.3% ... Training loss: 0.057 ... Validation loss: 0.180\r",
      "Progress: 96.4% ... Training loss: 0.057 ... Validation loss: 0.204\r",
      "Progress: 96.4% ... Training loss: 0.056 ... Validation loss: 0.185\r",
      "Progress: 96.4% ... Training loss: 0.058 ... Validation loss: 0.204\r",
      "Progress: 96.4% ... Training loss: 0.054 ... Validation loss: 0.199\r",
      "Progress: 96.4% ... Training loss: 0.054 ... Validation loss: 0.199\r",
      "Progress: 96.4% ... Training loss: 0.054 ... Validation loss: 0.204\r",
      "Progress: 96.4% ... Training loss: 0.055 ... Validation loss: 0.201\r",
      "Progress: 96.5% ... Training loss: 0.054 ... Validation loss: 0.205\r",
      "Progress: 96.5% ... Training loss: 0.057 ... Validation loss: 0.177\r",
      "Progress: 96.5% ... Training loss: 0.067 ... Validation loss: 0.219\r",
      "Progress: 96.5% ... Training loss: 0.058 ... Validation loss: 0.171\r",
      "Progress: 96.5% ... Training loss: 0.059 ... Validation loss: 0.216\r",
      "Progress: 96.5% ... Training loss: 0.055 ... Validation loss: 0.181\r",
      "Progress: 96.5% ... Training loss: 0.057 ... Validation loss: 0.204\r",
      "Progress: 96.5% ... Training loss: 0.061 ... Validation loss: 0.165\r",
      "Progress: 96.5% ... Training loss: 0.060 ... Validation loss: 0.210\r",
      "Progress: 96.6% ... Training loss: 0.056 ... Validation loss: 0.178\r",
      "Progress: 96.6% ... Training loss: 0.055 ... Validation loss: 0.187\r",
      "Progress: 96.6% ... Training loss: 0.056 ... Validation loss: 0.191\r",
      "Progress: 96.6% ... Training loss: 0.055 ... Validation loss: 0.178\r",
      "Progress: 96.6% ... Training loss: 0.055 ... Validation loss: 0.173\r",
      "Progress: 96.6% ... Training loss: 0.055 ... Validation loss: 0.181\r",
      "Progress: 96.6% ... Training loss: 0.058 ... Validation loss: 0.197\r",
      "Progress: 96.7% ... Training loss: 0.055 ... Validation loss: 0.177\r",
      "Progress: 96.7% ... Training loss: 0.055 ... Validation loss: 0.178\r",
      "Progress: 96.7% ... Training loss: 0.056 ... Validation loss: 0.200\r",
      "Progress: 96.7% ... Training loss: 0.057 ... Validation loss: 0.174\r",
      "Progress: 96.7% ... Training loss: 0.070 ... Validation loss: 0.217\r",
      "Progress: 96.7% ... Training loss: 0.057 ... Validation loss: 0.174\r",
      "Progress: 96.7% ... Training loss: 0.057 ... Validation loss: 0.199\r",
      "Progress: 96.7% ... Training loss: 0.058 ... Validation loss: 0.174\r",
      "Progress: 96.8% ... Training loss: 0.067 ... Validation loss: 0.233\r",
      "Progress: 96.8% ... Training loss: 0.058 ... Validation loss: 0.177\r",
      "Progress: 96.8% ... Training loss: 0.058 ... Validation loss: 0.205\r",
      "Progress: 96.8% ... Training loss: 0.056 ... Validation loss: 0.176\r",
      "Progress: 96.8% ... Training loss: 0.054 ... Validation loss: 0.205\r",
      "Progress: 96.8% ... Training loss: 0.058 ... Validation loss: 0.173\r",
      "Progress: 96.8% ... Training loss: 0.056 ... Validation loss: 0.196\r",
      "Progress: 96.8% ... Training loss: 0.058 ... Validation loss: 0.180\r",
      "Progress: 96.8% ... Training loss: 0.064 ... Validation loss: 0.230\r",
      "Progress: 96.9% ... Training loss: 0.070 ... Validation loss: 0.173\r",
      "Progress: 96.9% ... Training loss: 0.059 ... Validation loss: 0.205\r",
      "Progress: 96.9% ... Training loss: 0.061 ... Validation loss: 0.170\r",
      "Progress: 96.9% ... Training loss: 0.069 ... Validation loss: 0.228\r",
      "Progress: 96.9% ... Training loss: 0.068 ... Validation loss: 0.162\r",
      "Progress: 96.9% ... Training loss: 0.067 ... Validation loss: 0.224\r",
      "Progress: 96.9% ... Training loss: 0.061 ... Validation loss: 0.170\r",
      "Progress: 97.0% ... Training loss: 0.056 ... Validation loss: 0.209\r",
      "Progress: 97.0% ... Training loss: 0.058 ... Validation loss: 0.178\r",
      "Progress: 97.0% ... Training loss: 0.056 ... Validation loss: 0.185\r",
      "Progress: 97.0% ... Training loss: 0.065 ... Validation loss: 0.169\r",
      "Progress: 97.0% ... Training loss: 0.060 ... Validation loss: 0.189\r",
      "Progress: 97.0% ... Training loss: 0.059 ... Validation loss: 0.159\r",
      "Progress: 97.0% ... Training loss: 0.064 ... Validation loss: 0.197\r",
      "Progress: 97.0% ... Training loss: 0.057 ... Validation loss: 0.162\r",
      "Progress: 97.0% ... Training loss: 0.061 ... Validation loss: 0.201\r",
      "Progress: 97.1% ... Training loss: 0.062 ... Validation loss: 0.155\r",
      "Progress: 97.1% ... Training loss: 0.061 ... Validation loss: 0.196\r",
      "Progress: 97.1% ... Training loss: 0.054 ... Validation loss: 0.183\r",
      "Progress: 97.1% ... Training loss: 0.055 ... Validation loss: 0.190\r",
      "Progress: 97.1% ... Training loss: 0.055 ... Validation loss: 0.181\r",
      "Progress: 97.1% ... Training loss: 0.055 ... Validation loss: 0.172\r",
      "Progress: 97.1% ... Training loss: 0.060 ... Validation loss: 0.162\r",
      "Progress: 97.2% ... Training loss: 0.060 ... Validation loss: 0.196\r",
      "Progress: 97.2% ... Training loss: 0.054 ... Validation loss: 0.184\r",
      "Progress: 97.2% ... Training loss: 0.061 ... Validation loss: 0.204\r",
      "Progress: 97.2% ... Training loss: 0.065 ... Validation loss: 0.154\r",
      "Progress: 97.2% ... Training loss: 0.057 ... Validation loss: 0.195\r",
      "Progress: 97.2% ... Training loss: 0.064 ... Validation loss: 0.163\r",
      "Progress: 97.2% ... Training loss: 0.058 ... Validation loss: 0.204\r",
      "Progress: 97.2% ... Training loss: 0.054 ... Validation loss: 0.182\r",
      "Progress: 97.2% ... Training loss: 0.057 ... Validation loss: 0.166\r",
      "Progress: 97.3% ... Training loss: 0.081 ... Validation loss: 0.216\r",
      "Progress: 97.3% ... Training loss: 0.084 ... Validation loss: 0.159\r",
      "Progress: 97.3% ... Training loss: 0.062 ... Validation loss: 0.220\r",
      "Progress: 97.3% ... Training loss: 0.067 ... Validation loss: 0.172\r",
      "Progress: 97.3% ... Training loss: 0.057 ... Validation loss: 0.189\r",
      "Progress: 97.3% ... Training loss: 0.056 ... Validation loss: 0.179\r",
      "Progress: 97.3% ... Training loss: 0.056 ... Validation loss: 0.190\r",
      "Progress: 97.3% ... Training loss: 0.056 ... Validation loss: 0.173\r",
      "Progress: 97.4% ... Training loss: 0.057 ... Validation loss: 0.193\r",
      "Progress: 97.4% ... Training loss: 0.062 ... Validation loss: 0.168\r",
      "Progress: 97.4% ... Training loss: 0.071 ... Validation loss: 0.225\r",
      "Progress: 97.4% ... Training loss: 0.063 ... Validation loss: 0.162\r",
      "Progress: 97.4% ... Training loss: 0.060 ... Validation loss: 0.197\r",
      "Progress: 97.4% ... Training loss: 0.063 ... Validation loss: 0.163\r",
      "Progress: 97.4% ... Training loss: 0.054 ... Validation loss: 0.181\r",
      "Progress: 97.5% ... Training loss: 0.055 ... Validation loss: 0.179\r",
      "Progress: 97.5% ... Training loss: 0.055 ... Validation loss: 0.184\r",
      "Progress: 97.5% ... Training loss: 0.059 ... Validation loss: 0.196\r",
      "Progress: 97.5% ... Training loss: 0.057 ... Validation loss: 0.172\r",
      "Progress: 97.5% ... Training loss: 0.061 ... Validation loss: 0.218\r",
      "Progress: 97.5% ... Training loss: 0.064 ... Validation loss: 0.167\r",
      "Progress: 97.5% ... Training loss: 0.066 ... Validation loss: 0.228\r",
      "Progress: 97.5% ... Training loss: 0.068 ... Validation loss: 0.173\r",
      "Progress: 97.5% ... Training loss: 0.056 ... Validation loss: 0.202\r",
      "Progress: 97.6% ... Training loss: 0.059 ... Validation loss: 0.179\r",
      "Progress: 97.6% ... Training loss: 0.068 ... Validation loss: 0.232\r",
      "Progress: 97.6% ... Training loss: 0.057 ... Validation loss: 0.177\r",
      "Progress: 97.6% ... Training loss: 0.054 ... Validation loss: 0.194\r",
      "Progress: 97.6% ... Training loss: 0.056 ... Validation loss: 0.180\r",
      "Progress: 97.6% ... Training loss: 0.054 ... Validation loss: 0.194\r",
      "Progress: 97.6% ... Training loss: 0.056 ... Validation loss: 0.192\r",
      "Progress: 97.7% ... Training loss: 0.054 ... Validation loss: 0.201\r",
      "Progress: 97.7% ... Training loss: 0.055 ... Validation loss: 0.198\r",
      "Progress: 97.7% ... Training loss: 0.070 ... Validation loss: 0.163\r",
      "Progress: 97.7% ... Training loss: 0.068 ... Validation loss: 0.232\r",
      "Progress: 97.7% ... Training loss: 0.076 ... Validation loss: 0.170\r",
      "Progress: 97.7% ... Training loss: 0.065 ... Validation loss: 0.233\r",
      "Progress: 97.7% ... Training loss: 0.061 ... Validation loss: 0.180\r",
      "Progress: 97.7% ... Training loss: 0.059 ... Validation loss: 0.234\r",
      "Progress: 97.8% ... Training loss: 0.060 ... Validation loss: 0.180\r",
      "Progress: 97.8% ... Training loss: 0.065 ... Validation loss: 0.221\r",
      "Progress: 97.8% ... Training loss: 0.062 ... Validation loss: 0.165\r",
      "Progress: 97.8% ... Training loss: 0.060 ... Validation loss: 0.225\r",
      "Progress: 97.8% ... Training loss: 0.056 ... Validation loss: 0.178\r",
      "Progress: 97.8% ... Training loss: 0.054 ... Validation loss: 0.189\r",
      "Progress: 97.8% ... Training loss: 0.055 ... Validation loss: 0.185\r",
      "Progress: 97.8% ... Training loss: 0.062 ... Validation loss: 0.210\r",
      "Progress: 97.8% ... Training loss: 0.073 ... Validation loss: 0.163\r",
      "Progress: 97.9% ... Training loss: 0.065 ... Validation loss: 0.222\r",
      "Progress: 97.9% ... Training loss: 0.085 ... Validation loss: 0.161\r",
      "Progress: 97.9% ... Training loss: 0.087 ... Validation loss: 0.259\r",
      "Progress: 97.9% ... Training loss: 0.070 ... Validation loss: 0.160\r",
      "Progress: 97.9% ... Training loss: 0.062 ... Validation loss: 0.221\r",
      "Progress: 97.9% ... Training loss: 0.054 ... Validation loss: 0.187\r",
      "Progress: 97.9% ... Training loss: 0.055 ... Validation loss: 0.206\r",
      "Progress: 98.0% ... Training loss: 0.058 ... Validation loss: 0.198\r",
      "Progress: 98.0% ... Training loss: 0.055 ... Validation loss: 0.202\r",
      "Progress: 98.0% ... Training loss: 0.054 ... Validation loss: 0.184\r",
      "Progress: 98.0% ... Training loss: 0.055 ... Validation loss: 0.177\r",
      "Progress: 98.0% ... Training loss: 0.055 ... Validation loss: 0.199\r",
      "Progress: 98.0% ... Training loss: 0.054 ... Validation loss: 0.200\r",
      "Progress: 98.0% ... Training loss: 0.055 ... Validation loss: 0.191\r",
      "Progress: 98.0% ... Training loss: 0.056 ... Validation loss: 0.209\r",
      "Progress: 98.0% ... Training loss: 0.057 ... Validation loss: 0.178\r",
      "Progress: 98.1% ... Training loss: 0.061 ... Validation loss: 0.241\r",
      "Progress: 98.1% ... Training loss: 0.061 ... Validation loss: 0.177\r",
      "Progress: 98.1% ... Training loss: 0.061 ... Validation loss: 0.235\r",
      "Progress: 98.1% ... Training loss: 0.058 ... Validation loss: 0.182\r",
      "Progress: 98.1% ... Training loss: 0.054 ... Validation loss: 0.197\r",
      "Progress: 98.1% ... Training loss: 0.058 ... Validation loss: 0.213\r",
      "Progress: 98.1% ... Training loss: 0.055 ... Validation loss: 0.171\r",
      "Progress: 98.2% ... Training loss: 0.054 ... Validation loss: 0.182\r",
      "Progress: 98.2% ... Training loss: 0.055 ... Validation loss: 0.176\r",
      "Progress: 98.2% ... Training loss: 0.054 ... Validation loss: 0.187\r",
      "Progress: 98.2% ... Training loss: 0.056 ... Validation loss: 0.181\r",
      "Progress: 98.2% ... Training loss: 0.054 ... Validation loss: 0.192\r",
      "Progress: 98.2% ... Training loss: 0.055 ... Validation loss: 0.169\r",
      "Progress: 98.2% ... Training loss: 0.054 ... Validation loss: 0.197\r",
      "Progress: 98.2% ... Training loss: 0.055 ... Validation loss: 0.183\r",
      "Progress: 98.2% ... Training loss: 0.056 ... Validation loss: 0.218\r",
      "Progress: 98.3% ... Training loss: 0.060 ... Validation loss: 0.209\r",
      "Progress: 98.3% ... Training loss: 0.056 ... Validation loss: 0.181\r",
      "Progress: 98.3% ... Training loss: 0.055 ... Validation loss: 0.197\r",
      "Progress: 98.3% ... Training loss: 0.055 ... Validation loss: 0.167\r",
      "Progress: 98.3% ... Training loss: 0.057 ... Validation loss: 0.171\r",
      "Progress: 98.3% ... Training loss: 0.056 ... Validation loss: 0.196\r",
      "Progress: 98.3% ... Training loss: 0.057 ... Validation loss: 0.168\r",
      "Progress: 98.3% ... Training loss: 0.054 ... Validation loss: 0.171\r",
      "Progress: 98.4% ... Training loss: 0.055 ... Validation loss: 0.182\r",
      "Progress: 98.4% ... Training loss: 0.055 ... Validation loss: 0.190\r",
      "Progress: 98.4% ... Training loss: 0.057 ... Validation loss: 0.172\r",
      "Progress: 98.4% ... Training loss: 0.056 ... Validation loss: 0.192\r",
      "Progress: 98.4% ... Training loss: 0.055 ... Validation loss: 0.170\r",
      "Progress: 98.4% ... Training loss: 0.065 ... Validation loss: 0.211\r",
      "Progress: 98.4% ... Training loss: 0.070 ... Validation loss: 0.156\r",
      "Progress: 98.5% ... Training loss: 0.057 ... Validation loss: 0.191\r",
      "Progress: 98.5% ... Training loss: 0.056 ... Validation loss: 0.158\r",
      "Progress: 98.5% ... Training loss: 0.054 ... Validation loss: 0.188\r",
      "Progress: 98.5% ... Training loss: 0.055 ... Validation loss: 0.178\r",
      "Progress: 98.5% ... Training loss: 0.054 ... Validation loss: 0.174\r",
      "Progress: 98.5% ... Training loss: 0.057 ... Validation loss: 0.168\r",
      "Progress: 98.5% ... Training loss: 0.055 ... Validation loss: 0.190\r",
      "Progress: 98.5% ... Training loss: 0.054 ... Validation loss: 0.180\r",
      "Progress: 98.5% ... Training loss: 0.055 ... Validation loss: 0.176\r",
      "Progress: 98.6% ... Training loss: 0.054 ... Validation loss: 0.196\r",
      "Progress: 98.6% ... Training loss: 0.054 ... Validation loss: 0.182\r",
      "Progress: 98.6% ... Training loss: 0.054 ... Validation loss: 0.189\r",
      "Progress: 98.6% ... Training loss: 0.055 ... Validation loss: 0.213\r",
      "Progress: 98.6% ... Training loss: 0.054 ... Validation loss: 0.189\r",
      "Progress: 98.6% ... Training loss: 0.056 ... Validation loss: 0.186\r",
      "Progress: 98.6% ... Training loss: 0.059 ... Validation loss: 0.211\r",
      "Progress: 98.7% ... Training loss: 0.063 ... Validation loss: 0.180\r",
      "Progress: 98.7% ... Training loss: 0.070 ... Validation loss: 0.246\r",
      "Progress: 98.7% ... Training loss: 0.063 ... Validation loss: 0.166\r",
      "Progress: 98.7% ... Training loss: 0.057 ... Validation loss: 0.187\r",
      "Progress: 98.7% ... Training loss: 0.055 ... Validation loss: 0.183\r",
      "Progress: 98.7% ... Training loss: 0.061 ... Validation loss: 0.169\r",
      "Progress: 98.7% ... Training loss: 0.057 ... Validation loss: 0.211\r",
      "Progress: 98.7% ... Training loss: 0.061 ... Validation loss: 0.171\r",
      "Progress: 98.8% ... Training loss: 0.054 ... Validation loss: 0.184\r",
      "Progress: 98.8% ... Training loss: 0.054 ... Validation loss: 0.191\r",
      "Progress: 98.8% ... Training loss: 0.057 ... Validation loss: 0.191\r",
      "Progress: 98.8% ... Training loss: 0.055 ... Validation loss: 0.178\r",
      "Progress: 98.8% ... Training loss: 0.054 ... Validation loss: 0.184\r",
      "Progress: 98.8% ... Training loss: 0.056 ... Validation loss: 0.196\r",
      "Progress: 98.8% ... Training loss: 0.056 ... Validation loss: 0.174\r",
      "Progress: 98.8% ... Training loss: 0.055 ... Validation loss: 0.187\r",
      "Progress: 98.8% ... Training loss: 0.055 ... Validation loss: 0.170\r",
      "Progress: 98.9% ... Training loss: 0.054 ... Validation loss: 0.186\r",
      "Progress: 98.9% ... Training loss: 0.057 ... Validation loss: 0.200\r",
      "Progress: 98.9% ... Training loss: 0.058 ... Validation loss: 0.169\r",
      "Progress: 98.9% ... Training loss: 0.055 ... Validation loss: 0.183\r",
      "Progress: 98.9% ... Training loss: 0.056 ... Validation loss: 0.168\r",
      "Progress: 98.9% ... Training loss: 0.056 ... Validation loss: 0.190\r",
      "Progress: 98.9% ... Training loss: 0.056 ... Validation loss: 0.165\r",
      "Progress: 99.0% ... Training loss: 0.059 ... Validation loss: 0.195\r",
      "Progress: 99.0% ... Training loss: 0.057 ... Validation loss: 0.174\r",
      "Progress: 99.0% ... Training loss: 0.055 ... Validation loss: 0.166\r",
      "Progress: 99.0% ... Training loss: 0.057 ... Validation loss: 0.194\r",
      "Progress: 99.0% ... Training loss: 0.054 ... Validation loss: 0.175\r",
      "Progress: 99.0% ... Training loss: 0.054 ... Validation loss: 0.191\r",
      "Progress: 99.0% ... Training loss: 0.054 ... Validation loss: 0.187\r",
      "Progress: 99.0% ... Training loss: 0.056 ... Validation loss: 0.203\r",
      "Progress: 99.0% ... Training loss: 0.063 ... Validation loss: 0.185\r",
      "Progress: 99.1% ... Training loss: 0.057 ... Validation loss: 0.214\r",
      "Progress: 99.1% ... Training loss: 0.057 ... Validation loss: 0.179\r",
      "Progress: 99.1% ... Training loss: 0.054 ... Validation loss: 0.192\r",
      "Progress: 99.1% ... Training loss: 0.055 ... Validation loss: 0.200\r",
      "Progress: 99.1% ... Training loss: 0.061 ... Validation loss: 0.171\r",
      "Progress: 99.1% ... Training loss: 0.056 ... Validation loss: 0.185\r",
      "Progress: 99.1% ... Training loss: 0.061 ... Validation loss: 0.214\r",
      "Progress: 99.2% ... Training loss: 0.054 ... Validation loss: 0.184\r",
      "Progress: 99.2% ... Training loss: 0.065 ... Validation loss: 0.212\r",
      "Progress: 99.2% ... Training loss: 0.060 ... Validation loss: 0.168\r",
      "Progress: 99.2% ... Training loss: 0.063 ... Validation loss: 0.224\r",
      "Progress: 99.2% ... Training loss: 0.061 ... Validation loss: 0.172\r",
      "Progress: 99.2% ... Training loss: 0.055 ... Validation loss: 0.191\r",
      "Progress: 99.2% ... Training loss: 0.055 ... Validation loss: 0.199\r",
      "Progress: 99.2% ... Training loss: 0.054 ... Validation loss: 0.187\r",
      "Progress: 99.2% ... Training loss: 0.054 ... Validation loss: 0.185\r",
      "Progress: 99.3% ... Training loss: 0.056 ... Validation loss: 0.196\r",
      "Progress: 99.3% ... Training loss: 0.055 ... Validation loss: 0.191\r",
      "Progress: 99.3% ... Training loss: 0.055 ... Validation loss: 0.204\r",
      "Progress: 99.3% ... Training loss: 0.057 ... Validation loss: 0.190\r",
      "Progress: 99.3% ... Training loss: 0.054 ... Validation loss: 0.207\r",
      "Progress: 99.3% ... Training loss: 0.054 ... Validation loss: 0.185\r",
      "Progress: 99.3% ... Training loss: 0.055 ... Validation loss: 0.205\r",
      "Progress: 99.3% ... Training loss: 0.054 ... Validation loss: 0.199\r",
      "Progress: 99.4% ... Training loss: 0.054 ... Validation loss: 0.204\r",
      "Progress: 99.4% ... Training loss: 0.055 ... Validation loss: 0.210\r",
      "Progress: 99.4% ... Training loss: 0.055 ... Validation loss: 0.198\r",
      "Progress: 99.4% ... Training loss: 0.058 ... Validation loss: 0.218\r",
      "Progress: 99.4% ... Training loss: 0.061 ... Validation loss: 0.173\r",
      "Progress: 99.4% ... Training loss: 0.058 ... Validation loss: 0.213\r",
      "Progress: 99.4% ... Training loss: 0.054 ... Validation loss: 0.194\r",
      "Progress: 99.5% ... Training loss: 0.054 ... Validation loss: 0.188\r",
      "Progress: 99.5% ... Training loss: 0.079 ... Validation loss: 0.253\r",
      "Progress: 99.5% ... Training loss: 0.074 ... Validation loss: 0.160\r",
      "Progress: 99.5% ... Training loss: 0.069 ... Validation loss: 0.214\r",
      "Progress: 99.5% ... Training loss: 0.058 ... Validation loss: 0.168\r",
      "Progress: 99.5% ... Training loss: 0.056 ... Validation loss: 0.178\r",
      "Progress: 99.5% ... Training loss: 0.055 ... Validation loss: 0.202\r",
      "Progress: 99.5% ... Training loss: 0.058 ... Validation loss: 0.172\r",
      "Progress: 99.5% ... Training loss: 0.055 ... Validation loss: 0.208\r",
      "Progress: 99.6% ... Training loss: 0.055 ... Validation loss: 0.185\r",
      "Progress: 99.6% ... Training loss: 0.054 ... Validation loss: 0.199\r",
      "Progress: 99.6% ... Training loss: 0.057 ... Validation loss: 0.199\r",
      "Progress: 99.6% ... Training loss: 0.056 ... Validation loss: 0.232\r",
      "Progress: 99.6% ... Training loss: 0.058 ... Validation loss: 0.185\r",
      "Progress: 99.6% ... Training loss: 0.055 ... Validation loss: 0.208\r",
      "Progress: 99.6% ... Training loss: 0.058 ... Validation loss: 0.180\r",
      "Progress: 99.7% ... Training loss: 0.056 ... Validation loss: 0.189\r",
      "Progress: 99.7% ... Training loss: 0.055 ... Validation loss: 0.175\r",
      "Progress: 99.7% ... Training loss: 0.055 ... Validation loss: 0.184\r",
      "Progress: 99.7% ... Training loss: 0.055 ... Validation loss: 0.178\r",
      "Progress: 99.7% ... Training loss: 0.057 ... Validation loss: 0.213\r",
      "Progress: 99.7% ... Training loss: 0.060 ... Validation loss: 0.185\r",
      "Progress: 99.7% ... Training loss: 0.059 ... Validation loss: 0.228\r",
      "Progress: 99.7% ... Training loss: 0.057 ... Validation loss: 0.198\r",
      "Progress: 99.8% ... Training loss: 0.056 ... Validation loss: 0.226\r",
      "Progress: 99.8% ... Training loss: 0.054 ... Validation loss: 0.198\r",
      "Progress: 99.8% ... Training loss: 0.054 ... Validation loss: 0.202\r",
      "Progress: 99.8% ... Training loss: 0.055 ... Validation loss: 0.181\r",
      "Progress: 99.8% ... Training loss: 0.055 ... Validation loss: 0.190\r",
      "Progress: 99.8% ... Training loss: 0.054 ... Validation loss: 0.197\r",
      "Progress: 99.8% ... Training loss: 0.056 ... Validation loss: 0.221\r",
      "Progress: 99.8% ... Training loss: 0.064 ... Validation loss: 0.176\r",
      "Progress: 99.8% ... Training loss: 0.063 ... Validation loss: 0.229\r",
      "Progress: 99.9% ... Training loss: 0.056 ... Validation loss: 0.177\r",
      "Progress: 99.9% ... Training loss: 0.065 ... Validation loss: 0.236\r",
      "Progress: 99.9% ... Training loss: 0.056 ... Validation loss: 0.184\r",
      "Progress: 99.9% ... Training loss: 0.054 ... Validation loss: 0.201\r",
      "Progress: 99.9% ... Training loss: 0.058 ... Validation loss: 0.204\r",
      "Progress: 99.9% ... Training loss: 0.055 ... Validation loss: 0.195\r",
      "Progress: 99.9% ... Training loss: 0.056 ... Validation loss: 0.220\r",
      "Progress: 100.0% ... Training loss: 0.060 ... Validation loss: 0.183\r",
      "Progress: 100.0% ... Training loss: 0.054 ... Validation loss: 0.210\r",
      "Progress: 100.0% ... Training loss: 0.055 ... Validation loss: 0.185\r",
      "Progress: 100.0% ... Training loss: 0.055 ... Validation loss: 0.184"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "### TODO:Set the hyperparameters here, you need to change the defalut to get a better solution ###\n",
    "iterations = 8000\n",
    "learning_rate = 0.6\n",
    "hidden_nodes = 13\n",
    "output_nodes = 1\n",
    "\n",
    "N_i = train_features.shape[1]\n",
    "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "losses = {'train':[], 'validation':[]}\n",
    "for ii in range(iterations):\n",
    "    # Go through a random batch of 128 records from the training data set\n",
    "    batch = np.random.choice(train_features.index, size=128)\n",
    "    X, y = train_features.ix[batch].values, train_targets.ix[batch]['cnt']\n",
    "                             \n",
    "    network.train(X, y)\n",
    "    \n",
    "    # Printing out the training progress\n",
    "    train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)\n",
    "    val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii/float(iterations)) \\\n",
    "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
    "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    losses['train'].append(train_loss)\n",
    "    losses['validation'].append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAusAAAH0CAYAAACEkWPuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQRAEQRAEQQhRRFkXBEEQBEEQhBDl/wFISsdCFgFTVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116574630>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 373
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.plot(losses['validation'], label='Validation loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查预测结果\n",
    "\n",
    "使用测试数据看看网络对数据建模的效果如何。如果完全错了，请确保网络中的每步都正确实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+0AAAIgCAYAAADwRojNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsvXmcHFW5x/071T1LEiYJuUlIWCSo\nrIJhE5A1RlC83kv0goLCRVT01avAveB9RcVLEFwuL2tQXJBLXCKg7ChBAQkQ1iwSlCWsAQJknUyS\nmcnMdFed94/unqk6dWqmTtep7pru3/fzySfT1VXVp7fq8zvP8/weIaUEIYQQQgghhBBCsodT7wEQ\nQgghhBBCCCFED0U7IYQQQgghhBCSUSjaCSGEEEIIIYSQjELRTgghhBBCCCGEZBSKdkIIIYQQQggh\nJKNQtBNCCCGEEEIIIRmFop0QQgghhBBCCMkoFO2EEEIIIYQQQkhGoWgnhBBCCCGEEEIyCkU7IYQQ\nQgghhBCSUSjaCSGEEEIIIYSQjELRTgghhBBCCCGEZBSKdkIIIYQQQgghJKNQtBNCCCGEEEIIIRmF\nop0QQgghhBBCCMko+XoPICsIIV4DMB7AqjoPhRBCCCGEEEKIfWYA2CKl3K3eAzGBon2I8WPGjJm0\n9957T6r3QAghhBBCCCGE2OX555/Htm3b6j0MYyjah1i19957T1q2bFm9x0EIIYQQQgghxDIHHXQQ\nli9fvqre4zCFNe2EEEIIIYQQQkhGoWgnhBBCCCGEEEIyCkU7IYQQQgghhBCSUSjaCSGEEEIIIYSQ\njELRTgghhBBCCCGEZBSKdkIIIYQQQgghJKNQtBNCCCGEEEIIIRmFfdoJIYQQQgghATzPQ2dnJ7Zu\n3Yr+/n5IKes9JEICCCHQ1taGjo4OTJo0CY7TuPFoinZCCCGEEELIIJ7n4c0330Rvb2+9h0JIJFJK\n9PX1oa+vDz09Pdhll10aVrhTtBNCCCGEEEIG6ezsRG9vL/L5PKZNm4Zx48Y1rBgioxfP89DT04M1\na9agt7cXnZ2dmDx5cr2HlQr89hFCCCGEEEIG2bp1KwBg2rRp6OjooGAnmcRxHHR0dGDatGkAhj63\njQi/gYQQQgghhJBB+vv7AQDjxo2r80gIGZnK57TyuW1EKNoJIYQQQgghg1RM5xhhJ6MBIQQANLRZ\nIr+JhBBCCCGEEEJGJRXR3shQtBNCCCGEEEIIIRmFop0QQgghhBBCCMkoFO3NSm8n8LcFwObV9R4J\nIYQQQgghhJAIKNqbldu/Atz5H8D8fwE8r96jIYQQQgghhBiwatUqCCFwxhlnBLafccYZEEJg1apV\nqTzuokWLIITA3LlzUzk/CUPR3qysfqr0/6bXgN6N9R0LIYQQQgghGUQIEfiXy+UwefJkzJ49GwsW\nLKj38FIhajGA1I98vQdA6oT09H8TQgghhBBCAlx44YUAgEKhgJUrV+KOO+7Agw8+iGXLluGKK66o\n8+iC/PCHP8T555+PnXbaKZXzH3LIIXj++ecxefLkVM5PwlC0NyseRTshhBBCCCFxUFPBH3jgARx3\n3HG46qqrcPbZZ2PGjBl1GZeO6dOnY/r06amdf+zYsdhrr71SOz8Jw/T4ZoWRdkIIIYQQQqriwx/+\nMPbaay9IKbFkyRIAwbTyF198ESeffDKmTp0Kx3GwaNGiwWM7OzvxrW99C3vvvTfGjBmDCRMm4MMf\n/jD+8pe/aB9r69atOPfcc7Hzzjujvb0de+21F6644gp4Eb5Uw9W0P/XUUzj55JOx0047oa2tDdOn\nT8dHPvIR/P73vwdQWpzYbbfdAAC/+tWvAqUB8+fPBzB8TftLL72E008/HTvttBNaW1ux44474vTT\nT8dLL70U2nfu3LkQQmDRokW45ZZbcMghh2Ds2LGYNGkSTjnlFLz11ltRL3/TwUh7s0LRTgghhBBC\nSNVIKQGU6t79vPLKKzj00EOxxx574NRTT8W2bdswfvx4AMDrr7+OWbNmYdWqVTjqqKNw/PHHo6en\nB3/84x9x/PHH4+c//zm+9KUvDZ6rv78fH/7wh7FkyRLMnDkTp556Krq6unDxxRfjoYceMhrvdddd\nh69+9avI5XI44YQTsPvuu2PdunVYunQprr32Wnz605/GrFmz0NXVhauvvhozZ87EJz7xicHj999/\n/2HPv2TJEhx77LHYunUrTjjhBOyzzz544YUXsGDBAtx555144IEHcPDBB4eOu/baa3HXXXfhhBNO\nwDHHHIMnn3wSN998M1asWIGnn34abW1tRs+zEaFob1ak6/ubop0QQtKiu7+I7dr4c0sIaRxmnP+n\neg8hNqt+9PFUznv//fdj5cqVEELgAx/4QOC+xYsX41vf+hZ+8IMfhI773Oc+h9dffx033ngjTjnl\nlMHtXV1dmDVrFs4++2yccMIJ2GGHHQAAl19+OZYsWYJ/+7d/wx/+8Ac4TilR+vzzz8dBBx0Ue7zP\nPfcc/uM//gPjx4/HI488gve9732B+1evLrWBnjVrFmbMmIGrr74a+++/f2yHeCklTj/9dGzZsgW/\n/e1vceqppw7ed/PNN+OUU07Baaedhueee27wOVS49957sWTJEuy3336D2z772c/ixhtvxJ133olP\nf/rTsZ9no8L0+GaFkXZCCEmdeQ+8hP3m/hln3/i3eg+FEEJIAubOnYu5c+fiO9/5Dk466SQcf/zx\nkFLiP//zP7HrrrsG9t1hhx0Gjev8rFixAg899BBOPPHEgGAHgIkTJ+Kiiy5CX18fbr311sHtN9xw\nAxzHwaWXXhoQu7vtthvOPvvs2OP/6U9/imKxiO9+97shwQ4AO++8c+xz6Xjsscfwwgsv4IMf/GBA\nsAPAySefjCOPPBIrV67E4sWLQ8eeffbZAcEOYDDb4Kmnnko0rkYh8dK/EOIMADeMsJsnpcwpxx0O\n4AIAhwFoB/AygP8DcI2U/jBw4Jh/AfANAAcAyAF4FsC1UspfJXkOTYnHSDshhKTNFfe9CAC4a8Xb\nOPe4PTBj8rg6j4gQQkg1XHTRRQBKqfATJ07EUUcdhS9+8Ys47bTTQvvOnDlTm9L9+OOPAwA2b96s\njWCvX78eAPD8888DKNWyv/zyy9hll13wnve8J7T/rFmzBsc1Ek888QQA4GMf+1is/U1Zvnw5AGD2\n7Nna+2fPno3Fixfjb3/7G44++ujAfbqU+V122QUAsGnTJssjHZ3YyNd7GkDUp+UoALMBLPRvFELM\nAXArgD4ANwPoBPCvAK4EcASAT6knEkJ8HcA1ADYC+C2AAQAnAZgvhNhPSvkNC8+lOZASgPTdpmgn\nhJC0WbOlj6KdENIQpJVynmUq9etxmDZtmnb7xo0bAQD33Xcf7rvvvsjju7u7AZTEPYDBVPm4j6Oj\nq6sLAFJrA1cZa5RrfWV7ZRx+Jk6cGNqWz5dkqutqY7lNR2LRLqV8GiXhHkII8Xj5z1/4to0HcB0A\nF8AsKeXS8vbvAvgrgJOEEKdIKW/yHTMDwGUoifuDpZSrytu/B2AJgPOEELdKKSuPR4ZDvehQtBNC\nSOps2Vao9xAIIYTUANWYrsKECRMAAFdffXWs1PbK/mvXrtXev2bNmthjqgjjt956K5V2bZWxRo3p\nnXfeCexHzEitpl0IsS9Kqe9vAfC7VZwEYAqAmyqCHQCklH0opcsDwFeV030BQBuAH1cEe/mYTQAq\nDg9fsTn+hkatPqBoT8SWvgIeeH4t+gpcCSSkltyybDWuvv8lbB4lYni0jJMQQkg6HHbYYQCARx55\nJNb+HR0deO9734u33noLr7zySuh+fxu5uI+9cOHCEfYEcrlSVbNJlPuAAw4YdkyV7QceeGDsc5Ih\n0jSi+3/K/1+v1KhXCh3u1RzzMIBeAIcLIfyFIMMds1DZh4yEKtIp2qtGSonP/OIJfPFXS/G1Bcvr\nPRxCmoYVb3bhG39YgSvvfxGX3vtCvYcTC4p2Qghpbg4++GAcddRRuO222/B///d/2n3+/ve/Y926\ndYO3P//5z8PzPHzzm98M9GV/7bXXMG/evNiP/dWvfhX5fB4XX3wxnnvuudD9Ffd4ANh+++0hhMAb\nb7wR+/xHHHEE9txzTyxevBi33HJL4L5bbrkFDz/8MPbYYw8ceeSRsc9JhkilB40QYgyA0wB4AH6p\n3L1n+f8X1eOklEUhxGsA3gfg3QCej3HMO0KIHgA7CyHGSil7Rxjbsoi77OeJZBWPkXZbbO0v4tm3\ntwAAHn91Y51HQ0jzcN0jrw7+veDJN/D9T+43zN7ZYEtfsd5DIIQQUmd+97vfYfbs2fjiF7+IefPm\n4dBDD8XEiROxevVqPPPMM/jHP/6Bxx9/HFOnTgUAnHfeebjjjjtw66234sADD8RHP/pRbN68GTff\nfDOOPvpo3HXXXbEed5999sG1116Lr3zlKzjggAMwZ84c7L777ti4cSOWLl2Kjo4OPPjggwCA7bbb\nDoceeigeeeQRnHrqqdhjjz0Ge7u///3v155fCIFf/epXOO6443DyySdjzpw52GuvvbBy5Urccccd\n6OjowK9//etQuzcSj7Qax34awEQAf5JSvqncVylk2BxxbGW735EgzjHjyvsNK9oJGGm3iOsO+QO4\nXnyDEkJIibVb+jC1oy2y/i+KSeNaUxpRerCmnRBCyM4774xly5bhmmuuwa233ooFCxbAdV1MmzYN\n++yzD84666xA+7O2tjbcf//9mDt3Lm6++WZcffXVmDFjBi644AJ88pOfjC3agVIbtX333ReXXXYZ\nFi1ahDvuuAOTJ0/G+9//fpx55pmBfX/zm9/gv/7rv3DvvffixhtvhJQSO++8c6RoB4BDDz0US5Ys\nwSWXXIL7778fd999NyZPnozPfOYz+O53v4s999wz8lgyPMLECTH2SYV4FMDhAE6QUt6t3PcigN0B\n7C6lfFlz7GMAPgjgg1LKJ8rbBgC0AGiRUoZCFUKItwFMBzBdShnfkSF4jmUHHnjggcuWRQXiG4i+\nLcCPdhm6/eVFwI4H1Gs0o5oN3f04+JL7AQAtOYGXvv/PdR4RIaOHK/6yEvP++jKOeO8/YcGZhxkd\ne+V9L+LqB14avJ1VJ+MZ5w9ZuvzbATvhipP3r+NoCCEkHpWWY3vvvXedR0JIPOJ+Zg866CAsX758\nuZTyoFqMyxbW8xOEEPugJNhXA7hHs0slWh5lHThe2c/kmC0xh9nc0IjOGv7oerWR9nVb+rCxu9/W\nkAgZNdy6/C0AwKMvb8TaLX1Gx24/tiVw20sx0+WZ1V148tWNRu1+gHB7INa0E0IIIaQa0igqiDKg\nq7Cy/P8e6h1CiDyA3QAUAbwa85jpKKXGrx6pnp2UCbV8Y1p3tfiFuifNengCwNNvduGI//0rDv3B\nA3juba45keaivzi0YDhQNFs8zOeCP19pCeLlb2zCCT9+FCf/4gn85Tl9y50o1IW8Tb0DNodGCCGE\nkCbBqmgXQrQD+HeUDOiuj9jtr+X/j9fcdzSAsQAek1L6Q4/DHfMxZR8yEqoRnXo7S/RtAfqirAzq\njzopNw32feu2v6PgShQ9iS//ZunIBxDSQHhSav82PRYANvakk63y1Gudg38v8f0dB1eqop2RdkII\nIYSYYzvS/ikA2wO4R2NAV+EWABsAnCKEOLiysSz4Lynf/KlyzA0A+gF8XQgxw3fM9gC+Xb75s6SD\nbxpGixHd+pVwL98L7mV7Qr6zot6j0aKKdtMU+be7tg3+vXrTtmH2JKTxUDNVqj0WANZvTSeKHSiB\nMV1YUC6taUbaeweKWLRyHfoKGV6EJYQQQkhV2BbtXy7//4uoHaSUWwB8CUAOwCIhxC+FEJcCeBol\nA7pbANysHPMagP8GMAnAUiHET4QQVwJ4BsB7AFwupXzc8nNpXEaJaH/jsd8jV+hBrrgNbzz6+3oP\nR0sxFGk3m9T/0yh0wCbEFl4CTwh1/w0p+UIUfR0iTCuJ1OvB5m2F1LpMnH79UzjjhiX40q+ZsUMI\nIYQ0GtZEuxBibwBHItqAbhAp5R0AjgHwMIATAZwFoADgXACnSE1hsJTyGgAnAHgWwOkoLRCsAXCG\nlPIbtp5HU6BYDfQXstk7+O0NQ2nx72zaWseRRJM00j4a21YRYgt/5NrUD6JWot0/RuOFBeU5SZlO\n27f+ooulr28CADz2ykbr5yeEEEJIfbHWp11K+TyA2I12pZSPAjDqj1VuH3f3iDuS4VEi69sGCmir\n01CGxbe4IDPaAz1c024Yad+Oop00L8UEqefq/hu700qPH7pemqfHh/ff1DuA7S0v1tnoYkEIIYSQ\n7JKGezzJOorxnMyoEZ30Ly5oGxHUH9eT2A69+JDzN7RhIFTDOhJqpJ0TbtJM+EWt6XdHFcSpRdr9\nl6GE2QBAOnXt6uOYjpMQQggh2cZapJ2MIpRIu2c6W64V/sWEjNbdu56Hm1ovwb7OKtznHghXftzo\neCGCySkbu/sxdXy7zSESklncBO7xrnJJSEu0O4UeXNFyLcagH08NXGB0rG4NblOP/fR4XZlOPhc7\n8Y0QQgghGYeivRlRJseem80otvQvJmRUtMv+LdjXWQUAONx5Fr2GkXI1WrhuK0U7aQ6klIFLUdJ6\n8fUppcfvufGvmJNbXHrMznsAzIp9rG4hIo1Iu2qI6UrJH3dCCCGkgWB6fDMiR0d6vJDZj7T7Fzwc\nSONooTrZXr81nWghIVkjqR9EKD0+pe/O2MKQsdt2RcM+7ZqFiK4UerWrrwWz4wkhhJDGgqK9CVEj\n615GRXuwpj2bs1DXHZqAO5DG0cJwpL3PyrgIyTpJ2yWGjOh6+lOp5ZYJynRqVdMeirTTG4MQQghp\nKCjamxDXC7Z4kxmNYgcmyBkdo/T1cHbgJU/xZaSdNAmqSDfVmeqCV1/BQ89ACguQCa5DujWEWhjR\nmbrcE0IIISTbULQ3IZ47OozoxGgwogtE2r0qzLTCNe2ENAM687QkxwMppcj7rkPCNNKuq2mvgRGd\nrtUcIYQQQkYvFO1NiOsGI+2qiM8KchRE2v2lBTlRRXo8I+2kSVHXCk2Fpk4Qp+Ign8Bbo2Yt3xJm\nLRBCCIlGCBH419bWhilTpuDAAw/EmWeeiYULF8K1ZOo8f/58CCEwf/58K+cjjQMNZpsQtaZdZrQH\nut+IzjTCVSs8dQHE0B+g6DLSTpqTpEJTJ/I3pOAgn2TxUJd5s6WvqNkzGUmzFgghhIzMhRdeCABw\nXRddXV149tln8Zvf/AbXX389Dj74YCxYsAB77LFHnUdJGhWK9ibEVUJcMqPp8aOjpj0o0l3DrAV1\nUk8jOtIslISlxPvE63hR7mxch63bf8s2+6nn/pQA08VDnWgvppDZpC7+mZbpEEIIGZm5c+eGtq1d\nuxZnnXUW/vCHP+DYY4/F0qVLMXXq1NoPjjQ8TI9vQtT0+Ky2fBsNot1TTP1M06PUiNj6rek4YBOS\nNVxP4qL8fPyp7du4vfV/jL01dNq3kMYCpKxetOsi3mlEwcOmfryGEEJILdhhhx1w0003YdasWXjz\nzTfxgx/8IHD/smXLcM4552DmzJmYNGkS2tvbsfvuu+O8887Dpk2bAvvOmjULn//85wEAn//85wMp\n+atWrQIAvP322/je976HI444AtOmTUNrayt23HFHfPazn8Xzzz9fk+dM6gMj7U2ImsKdVff4gBEd\nsjlGVWiYLoAoATL0FTxs7S9ifHtL0qERkmlcKXFcbhkAYF9nFRb3rAGwQ+zjdenxqaSFy+qvQ7o1\nhDQWFtjyjRBC6ofjOLjggguwaNEi3HjjjbjyyishhAAAXHfddbj99ttxzDHH4Nhjj4Xruli+fDmu\nuOIKLFy4EE8++SQ6OjoAAGeccQYmTpyIO++8E3PmzMH+++8/+BgTJ04EADz88MP40Y9+hA996EM4\n8cQTsd122+Gll17CLbfcgrvuuguPPvooZs6cWfsXgaQORXsT4hWT1WHXjAQRrlqRtOe9WqoAAOu2\n9FO0k4bH8yTafCJYema13qpQBcJp4jYQASO65Cn8bgpjDLvHW38IQggJMndCvUcQn7mbU3+II488\nEvl8HuvWrcOqVauw2267AQC+9a1v4Sc/+QlyuVxg/+uvvx5nnnkmrr32Wnzzm98EUBLtAHDnnXfi\nE5/4xOBtP7Nnz8batWsHhX6FFStW4IgjjsD555+PhQsX2n+CpO4wPb4J8eToqGkX/qhWRtM91fR4\nc9GuM6pKoS6XkIzhehKO7ztu2sVCWy+exrXMd07H0LRTN8ZCClHwkGjP6PWSEEIalba2NvzTP/0T\nAGD9+vWD23fdddeQYAeAL3zhCxg/fjz+/Oc/Gz3O1KlTQ4IdAGbOnInZs2fjwQcfRKHAeWQjQtHe\nhITc4zMq2kdDTbtqRKfeHgndS8/UVmKDLX0FvLq+u97DiMSVErkEkXbd90QXfU9MEvf4GqXwq4sV\npqZ+hBBCklPxJKqkxgNAoVDAj3/8Yxx55JGYNGkScrkchBBwHAdbtmzBW2+9Zfw4f/rTn/Cv//qv\nmD59OlpaWgbr3u+++2709/djw4YN1p4TyQ5Mj29CQjXtGU2PHxUt35TXTpfuPhy6yXUaKb6kuejq\nHcBRlz6IrX1FXPapmTjpoJ3rPaQQnhJpN/XWqNV3J3AdgmF6vDaF3/61LGnPe0IIMaYGKeejib6+\nPnR2dgIApkyZMrj95JNPxu233453v/vdmDNnDqZNm4a2tjYAwFVXXYX+frNWv/PmzcM555yD7bff\nHscddxze9a53YezYsRBC4I477sCKFSuMz0lGBxTtTYjqcK6my2cGf017Ro3o1AUPNYthJGrlLk2a\niyde7cTWcj/w+55bk0nRXvQkHJ8Ilq5ZpF0nTLMWadcuLNQg0s5LCCGE1JbFixejWCxihx12wIwZ\nMwAAS5cuxe23345jjz0W99xzD1pahvyKPM/DpZdeavQYxWIRF154IaZNm4bly5dj+vTpgfsff/zx\nxM+DZBemxzchoch6RtPjA9H1jKZ7hksNkov2VOpySVPh/wxldRHI9ZT0eFOTN+2Cl/3vjkhgiKl7\nSmmIdrWGPavvOSGENCKe5+H73/8+AOCzn/3s4PaXX34ZAHDCCScEBDsAPPXUU9i2bVvoXJX6d10L\n4Q0bNqCrqwuHH354SLB3d3dj+fLlyZ4IyTQU7U1IqE1ZlZH2tPuJB12bsylkQ5F2C6KdE26SFP9n\nKKufJ08q6fGmkfaalZZUn/FTq/R49XnTiI4QQmrDunXrcMopp2DRokV417vehW9/+9uD91Ui7osW\nLQod87WvfU17voqZ3RtvvBG6b+rUqRg7diyWLVuG7u4hz5pCoYBzzjmHtewNDtPjmxBPmRybGtH1\nDhRxxv8twbqtffjZvx+EvaaNtzk838CGJp6jJT3eNRTtegdsTrhJMgKiPaMfp5B7vGnqeY3S45N4\na+jS4z1Zrud3hOaI6lBfi6wu1BBCyGhm7ty5AErBr66uLjz77LNYvHgxBgYGcMghh2DBggWYPHny\n4P4f+MAHcMQRR+C2227D4YcfjiOPPBJr167FwoULseeee2LHHXcMPcYHP/hBjB07FldddRU6Ozux\nww47AADOOussTJgwAWeffTZ+9KMfYb/99sOcOXMwMDCABx98EJ2dnfjQhz6EBx98sCavBak9FO1N\nSKju2lBo/vrx1/HUqpLZxunXP4WnvnOsraEFcOCfLGdzEqo6Xpu6x+sjcdl8rmT04P9cpZ0RUy2e\n4h5veh3SfU3SiGInSY+PMoQrehKtNkW7ZKSdEELS5qKLLgIAtLa2oqOjA7vuuitOP/10nHjiifjI\nRz4CxwkmMOdyOdx111244IILcM8992DevHnYaaedcOaZZ+KCCy7APvvsE3qM7bffHrfeeisuuugi\n3HDDDejp6QEAnHbaaZgwYQIuvvhiTJkyBb/85S/x85//HBMmTMBxxx2HSy65BBdeeGH6LwKpGxTt\nTYiaDi8New+veLNr8O91W9NzqBSjoeWbagDFmnaSAfyiLatR12LRQ074FhcMP/f1MKIzzfiJGo7t\n94R92gkhJD2SLH5PmjQJ1157rfa+VatWabcff/zxOP7447X35fN5nHvuuTj33HND982fPx/z58+v\ndqgk47CmvQlJ2qd9akebzeFEIkaBe3y4fV7y9NmsiiwyeiiOgpp2V108zKgfRDA9PrlZHmB/YS6c\nHm/19IQQQgipMxTtTUhIaBpGsaeOb1eOT0cU+IV6VtPj1ZReO+7xaUQLJfDmEqDzVfvnJpnDC6TH\n13Egw6CWkphm/OgWvAoplJY4geujaaQ9QrRbHqd6zWCknRBCCGksKNqbEDUabBodHtuaC9zevK2Q\neEw6RILJcq0Iu8cnT/FNJTL6zO+B648F5h1I4d4EBI3osingVNNGG+nxabR886fHOxbM8gD7C3Pq\naxFVS08IIYSQ0QlFexMSigYnnIi+3dWXdEhagpH26ibjA8WUxb60Hy1Mw0wLrz1U/kMCqxbbPz/J\nFKMhPT4UaTc2otNE2lN4rk6Clm+RkXbLiwvqIkBWF2oIIYQQUh0U7U1I4jpsZYK4Zsu2xGPSEaxp\nN5+EXvPAS3jfhffiu3f8w+awAniKwLbiHp+GyPK/54biiIw+/GIxs+7xal92G5H2NDovyOrLdGqV\nHh82orN6ekIIIYTUGYr2JkSNaJnWtKtRnLQi7U7CSPvl972Igivxmydex9td6SwsqJF2G+7xqURG\n/eM0zAaosOz1TTjpp4/hQ5ctwt9Xb7Y0MJIG/rWkrEZd3YQ17brFrTQ6L/ivQ/42lHGISpqxvTAX\nEu1U7YQQQpqIrAYobELR3oQkTY9XJ4RrNqck2i26x7+yvjvpcLSE+rQnzFoA0m9bVY0z2dX3v4QT\nf/oYlr6+Ca9t6MHvnnrd4uCIbfy13Vl1EldFunFpSY2+O0kyfqLEs+3a+7B7fONPXggh6SKEAGDu\n1UNIPaiI9srnthGhaG9CQhNJ4+hw8Pbbm1OKYluoaa/wRmdv0sHoCRnRmb2Wurl1GhPuzu6h92hb\n/4DRsd39RVz1wIuBbVv6ihF7kyzgBtZosing1NaTwjQ9vkbtEkUK6fG2Xe7VbIqsZlcQQkYPbW2l\n9r49PT11HgkhI1P5nFY+t40EfPMJAAAgAElEQVQIRXszokaHE6bHpxVpzyWsafeTlmhPWmqgS+ct\npBAafWvT0PN/aU2X0bHdfcVQcD4VszxiDf93NKtRVzVLxTSao3teaXx3RAIjuijxbPs9Uc+X1YUa\nQsjooaOjAwCwZs0abN26FZ7n8dpCMoWUEp7nYevWrVizZg2Aoc9tI5Kv9wBI7fFUYWmYlqpG6t9J\nSbT7hbpppF39YXljY0qiXU3xNTSi0+mUNESW9Jl+FYtmYyy4Hv499xecmHsEPynOwX3ewZkVgqSE\n/zua1airKtJN3eNrlaUSrGlPbpYH2E/jV43tuKZGCEnKpEmT0NPTg97eXqxevbrewyFkRMaOHYtJ\nkybVexipwUh7ExKKDhtOIFUR8M7mbamsvgZbLRmOUXlOr21IKb0rofDQtnxLRRD7zmlaO9zXjQvy\nC7C/8wrOz98IwH56L7GL/zOUUc0eco8XFmra0/hcigTeGpF92i2raqbHE0Js4zgOdtllF0yZMgXt\n7e0NXStMRi9CCLS3t2PKlCnYZZdd4DiNK20ZaW9CbBvR9RU8dPUWsP241qRDC5CkP7I6aX19Yy88\nT8JxLP/oJHXir5F7vEjQ8s3r24I2UQAATBEl13hG2rONNxrS4xO6x9fquxPsYmFa067fbt89Xlk8\npGgnhFjAcRxMnjwZkydPrvdQCGl6Gnc5gkSj5mRbEJq2U+SllIEIl2M6WVae0raCi7VbU0jjV9Pj\nDQRxZOpsKlHsoRfE1OG+WByKiObKLa/SaK1F7OH/jmZVtIfLdJKbvNkWw54n4fiyVHKm6fE169Ou\n3jY/v5SSYp8QQgjJKBTtTUhy8zSdaLfrIO9JICfsGkClkiIfSo+PP84ogWG7HRSgpB6bRjTdIbf5\nfEW0Mz0+07iB9PhsvlfSTaFdouW086Ink2X8RNa022755im3zd7zNZv7cNyVD2P25Q9h9aaUOm0Q\nQgghpGoo2puQkEg3TY/XiICtlluAFT0vaERnWtOuEZWpiHZVABu8lpHtoFLp0+4TcaZ194Wh97Zl\nMNKeTSFISgQi7VkV7aEyHVMjuvTT411PBqLrxtehGkXa1e+j6Vv+w4XP4+V13XhtQw/OuelpiyMj\nhBBCiA0o2puQ0GTZuE97+q2WPA/KZDl5pH1VCqJdXQAx6dMeJTB0Cw5JcZJE2ouFofOIUuSR6fHZ\nJtjyrY4DGYawe7yFlm+2I9gymB7vGHex0G9PI43fj+lCzaMvbxz8e9nrm6yMiRBCCCH2oGhvQtKI\ntFtvYeR5QdFuOAnVTejTiLSLkBO/gWiPisKlHGk3NqJT0phbUGR6fMbxL/xkNT3e8+y7x9te8HJd\nGWr5ZvJ6Ri7MWb9eqi3fzM4/YQw9aQkhhJAsY1W0CyGOEkLcKoR4RwjRX/7/L0KIf9bse7gQ4h4h\nRKcQolcI8YwQ4j+FELlhzv8vQohFQojNQohuIcSTQojP2XwOTYESjbJRp2nfDVkGUlGdhC3fAGB9\n94Bmz4QkSI+PEhjp17Qbvt/F4OuWh8v0+IwTiLRnVLRLJQXA1FtD9xG0fh2SimgXMtIRXnt8jWra\n1YXUqNKbKCaMabE5HEIIIYRYxppoF0JcAOBhAEcDuBfA5QDuBrA9gFnKvnN8+94O4CcAWgFcCeCm\niPN/vXy+fQH8FsB1AHYEMF8IcZmt59EMqK2VzNNSw9us9x0O1ZImT49PQwyrAtjktaxlpD3w+iWM\ntOfhZtaRnJTwp0tn9r1SF7wslOmknvEDafR61so9Xj1fVGeKKCjaCSGEkGxjJSdOCPEpABcDuB/A\nv0kptyr3t/j+Ho+S4HYBzJJSLi1v/y6AvwI4SQhxipTyJt8xMwBcBqATwMFSylXl7d8DsATAeUKI\nW6WUj9t4Pg1PwpZv2vR46y2MJPIJDKB0k9Y0UrqTpMfXsuWbv32eseGXWwjcboHLmvaM4xevGQ20\nh/0fLLSeTGPx0B9pz8EzimJHivYUau+Dt82Op2gnhBBCsk3iSLsQwgHwvwB6AXxWFewAIKX0z/pP\nAjAFwE0VwV7epw/ABeWbX1VO8QUAbQB+XBHs5WM2AfhB+eZXkj2TJiKU0m3qzO7hM7kHcHbuNmyH\nUnugdNJS/enx2UvhB3T+ANmsaQ+IdtOIZlGNtLOmPesEjeiy+V6FDTGTZ9OkUabjKGU6JqI9ag0h\njXH6MY20d7QHRbvtxQ9CCCGEJMNGpP1wALsBuAXAJiHEx1FKYe8D8JQm+j27/P+9mnM9jJL4P1wI\n0Sal7I9xzEJln2ERQiyLuGuvOMc3AqEUbsMI13t7luHslusBABNEDy4u/rv9/siu2h9ZQkoJIUS8\n43UmVamI4aDwMJksR5tUpVDT7l/0MM2sUCLteeGiP6NCkJQIpMdnNtSerOVbrbw1ciJYppPJ9HhP\n4j3iLXzUWYJ7vEPhyWQ/Z13bCpi8XZul0RFCCCEkKTZq2j9Q/n8tgOUA/gjgRwCuAvCYEOIhIcQU\n3/57lv9/UT2RlLII4DWUFhPeHfOYdwD0ANhZCDE2wfNoGtTosGm9+Ec6Fwz+/cV8ac3Edm9xTwZr\n2ksRLrPjVVJJ6VZfS5NIe6RJVRot3/xO/Ibp8UXVPd5lJC7juIH0+GyK9iTp8dGlJemmx5teh2r1\nHfdcD79ouQL/b8vv8ZOWecYLNep4NvWkYNpJCCGEkKqxIdqnlv//CoAxAI4F0IFStP3PKJnN/cG3\n/4Ty/5sjzlfZPrGKYyZE3D+IlPIg3T8AL4x0bKMQrsM2m+j2ifbQNuuR9tBk2SzCVYt2UEBYABsZ\n0dWoHVSJoXEZu3R7SqSd7vGZx/8Zymp6fDjSXp2JY84Zyr7xpHlq+LCPo6TH5+AZnd+/eOgbpv1s\nGncA73HeAQDsJd4wfg3U/Tsp2gkhhJBMYUO0V1q0CQAnSSkfkFJ2SymfBfBJAKsBHCOE+GDM81Wm\nNiazjmqOaVpCkXZDEacT7baFQfJa0trUtKuvnerMPxy1Sp0FlEi7qXt8KNLOmvas4xe1nsxmtF2t\naa82SyUnREC42ywHKHrhjB+T8/vFcGt+6Oe2YPv74+t5nxPSWLSr10aKdkIIISRb2BDtm8r/vyql\nXOG/Q0q5DaVoOwAcUv5/pKj4eGU/k2O2jDhaopkcm4r2MaFttiehrieRF8Ga9iyK9mTp8frtaaTx\niwTt89jybfThuh7Oz/8O17Vcjl3FGqOU7lqhLh6aZIAEItgOkPeJdpsLSq4nA50rjK9Dvn3b8rmh\n7bbT4z3VW8NwYU55Tp29FO2EEEJIlrAh2leW/++KuL8i6itKr7L/HuqOQog8SqZ2RQCvah5Dd8x0\nAOMArJZS9sYfdvOiRt1MI+3boEmPt93CSFG0OdP0eG1buhTEcIL0+KjXLA1B7CTo0y69sGhny7ds\n856+Z/GV/B9xXG4Zvpz7k5HQrBmhSHt1pSU5IYKi3eJn01Ui7aX0eJPjh/72R9qtX4uU76hJ60mA\nNe2EEEJI1rEh2h9GSWTvLoRo1dy/b/n/VeX//1r+/3jNvkcDGAvgMZ9z/EjHfEzZh4xAaHJsKtp1\nNe220+OV6K4Dz8yIrmbu8Wq00CR11nceX71rOmn8vnMaGtFJTZ9227XDxC4TihsG/54qujKZGRES\nlkZGdEN/O45APucXxBYj7SFDTM8oPd5/PWj1j9Hy+yHdZJF2tcZ+I0U7IYQQkikSi3Yp5QYAN6OU\nuv4//vuEEMcB+ChK6e2Vdm23ANgA4BQhxMG+fdsBXFK++VPlYW4A0A/g60KIGb5jtgfw7fLNnyV9\nLk1DgggXAAwguDaTS8FNXBXtAmZ1mrpJcTpiWO01XV2fdv+EPu1Iu+n7LdX0eFG6TTO6DOP7XJYW\nvLL3XoXS4U1MHBUjumCkPeX0+CoNMdvyKYr2pD3vGWknhBBCMo2NPu0AcC6AQwF8RwhxNICnAOyK\nkhGdC+BLUsouAJBSbhFCfAkl8b5ICHETgE4AJ6DU2u0WlBYBBpFSviaE+G8A8wAsFULcDGAAwEkA\ndgZwuaYfPIkiYaRd3X87bLNuTKamxzuQGKjSAGrwnDWItBs5YCsmVf3F0rGpLC4k6NMuvXCkHSil\nIbdaSdYh1pHB6HAm11fSMqKz+GSLrq71ZHVlOsH0eMtviFTS4xP2vGeknRBCCMkWVkS7lHKdEOJQ\nABegJNQPA7AVwJ8A/FBK+YSy/x1CiGMAfAfAiQDaAbyMkvifJzU5xlLKa4QQqwB8A8DpKGUJPAfg\nAinlr2w8j6YhNKEznEAqoq9D9Nrv0x5KjzeMcGn7tEtIKSH8uegJUU3dqjXTasvnsBXlCHYKtfeO\n9AZ7LJj2aYeSepsbFO1ZVIIECHYIMPWDqBXhuutqjegEWnJ+Z3Z73x9PhmvaTV5LLyLSbrvlm/pa\nqtkxIxGKtNOIjhBCCMkUtiLtkFJ2oiS6z425/6MA/tnwMe4GcLf56EiAhC3fHCWqMx691iehak2m\nabQwSlC6nkQ+Z0+0qwsgJu3U/NG2NFNnAQTa5yVOj6+IdrZ9yy5qenwGRXuS61DNIu2eDCzMOYbt\n1Py7Blq+pZwebxppDxvRFSL2JIQQQkg9YG5rM5LQPV7dvwPbrLd8U0W7SNAf2Y9tQexYalsVjMKl\nIdqHXk9j0e6pfdqH0uNJRpHVp3TXjARGdAHR7ojAQpzNz6WnuMcDgGcyzoj0eNfy9TK0WOgmTY/v\nj9iTEEIIIfWAor0JSdqnXZ1sd4he6yndnuJYbpweP0yk3SY2a9orpFPT7o+0G6bHa1q+AeksLhA7\nBNLjhZnjec1QF+ZMjOh8n71Qn3bLkfacCJ7PLcZPPfdfs/xmkwXr6fF2a9r7Ch62DRheJwghhBCS\nGhTtzUjS9Hgooh299lu+FVUjOsNa0giRYj3Sri54mJhpRUXhbPsDhKKFpos0SqS94h7P9PjsEkqP\nr+NYIghnpVQXwc47DvJOSi3fNLXhJoI46B6f0263Qsg9PploB4BO1rUTQgghmYGivQkJR4fNJpBq\npLYUabddoxmcLOeEWYpvlDi3nREQilqbOEtH1LTbNNICSq+Fv6ZdTekfkYhIO43ososYDenxqh+E\ngRiuRLAFPLSjX0mPt/dcPU2auWdwrYuqabd9vRTKd9RYtGs+H53dFO2EkJHp7i+ikx0nCEkdivYm\nRHU8V2+PhKOKdmyzXt+sM7bzqkyfjbO9GqQM9nAubbRQ72o70i5lsj7tUaI9BZd7YgnfdzSr7vFq\n+F8YtlIbh214oPUbuLX7c9i/uGLoPovXIlcn2g0EsRfV8s12erzaxcJCpL2738yBnhDSfLyxsReH\nfv9+HPaDB7B0VWe9h0NIQ0PR3owkTI8PGdEJ++nxass3INy7fdjja5Ae72pMqkzc46PqXW2/lmqk\nPWlNewsj7ZlHhPq0Z/C9CmWpmC3KfTn/J7zbWYOx2IbvdX178D6bppjqghUAeJptUfjF8G4DK/G5\n3J/RgV77iyjK4oI0XBTQjSeTnxlCSKZ44IW16BlwMeB6WPiPNfUeDiENjbWWb2T0oIpukwhXaf/g\nBHE87KfH66LqbpXt1ILnsGtSFa5pry7Snma9q+tKtPgj7YaZFepCRB6sac86IlDTLo3aJVbwPAnH\nsdgeUUW9DsFkwQvYVegniDa/P7pIu6xi8XAXsRZffvl8tLT040DnJdzmXmRtjKUHUhYSZLI+7VHb\nCCHEz4DPf8h2aR8hJAgj7U1ISLSbpserRnSi1/rFWmsAVWVaqh+r9a4yHGk3MtPyjaVFSY+XFqNc\nRc8L9po2jrSrop0t3zKP7zteTXr8vf9YgwMvuQ9f/vXS9Hq8q+7xhgte3XKM9j6b1yJdTbvJ4mHl\nOnR5y8/QIktt1ObkHrPfxUJd8DBMj9ddFxlpJ4SMhD/4wIU+QtKFor0JCYl0C33aradKa2tJTQSx\nfrvNOuxiwvR4/w9c3hGpta1S0/hDdfgjoJpctbDlW+YRqnu8oQD72u+Wo6u3gL88txZ3P/O27eGV\nSJge3w29aLf5udTVr5ssHrqexCRswSHOysB26xEp5dpo2vJNtzBD0U4IGQm/oS6vGYSkC0V7M6JM\njkMp3iOQ07jHW0/p1hnRaYR85PFSApCYId4JLFJYjbR7GiM6A0Ec6DUtBHI+0W41xVcGe02bexgo\nRnSi9D7YrB0mdknqHu///D384gZr4woQug6ZtXSMirTb/Y7rRLtBerwHfC7/l8C2V7zp9iPtajq8\nYRaM7jVjpishZCT8kXaWzBGSLhTtTUg4Pd4w8qrp0247ciQ1NZmeSSTO9fDD/C+xqO08zG+5FBUx\nbbumPZQebzBGv5DKOUgt0l4sBt8v4/R4ZX9G2rOPGmlP8l69vL7bxpBChL01zCLY29AW2FZZfLQ5\ncdS2fDOJtEuJOc6jgW05eCiMgj7t/H4TQkbCf53QtY4khNiDor0JUSPryd3jt9k3otMsAhilpUrg\nM/kHAQDH5J7BjLJple1Ie04kER5Df+ccJdJu8fVUzbTMjejUlm+l2wXWtGcWodS0J3mrXlmXjmhP\nkh5fSucOfkfGoweAXa8FvXu8WU37RBF8/fLCtdqWDkDotaN7PCGkFvivHan5nxBCAFC0NycJI+2h\n9Hik0fJNYwBl4tqsjOfd4p3SOSxOlvXu8Sbp8T5h5Qi05NLp41xUTP1MyyHUhYiKEZ3NhQViF79Z\nZDU17R1tQ41FuvuLVo0RK6iLfyafS1dK5JX9K+LYbqQ92eKhp7lGOPCsL3KqXhqmbR1pREcIqYaA\nER0vGYSkCkV7E5I40q4cv53og+cWEo/Ljy493izSHvz12FWsBWB3Qq/r025qplUhl2JNu1Qj7Ybv\nt5pO30L3+MwTqGkX0jhtcdqE9sDtDd0DVsYVQP38GC14SeSUMp0J5Uh72kZ0RoaYmsWFPDzri5wh\nkW5qRKd57ZkeTwgZiYARHa8ZhKQKRXsTovZlN02XViPtANDu9SYak4quF7KZe3yEaLfsyq6aZxml\nx/sOdRT3eJs1r0VFtBtH2kPp8RXRzh/orOL/HJbS483eK3Xvl9ZutTCqIOp3RW0lORyeNtJeSY+3\nGWnXpMcbZPy4HkILe0k9BkLj0UTzTRY4AX1XDUbaCSEjETCi40I+IalC0d6EqCLdND1eJ/raXbt1\nr7pJp5EBlCJUK+nxVkW7DE+WTaLYFSH1XrEaU/rfQC6XTk27KjwcYw8DJT1elM7HSFw28ZQMkFJ6\nvOE5FMH2YgqiPVSmY5SlAuSEGmkvp8dbnDhqFwo1de5RSBnOCMjDtWrcqTPENP2O6z4fdI8nhIxE\nwIiO1wxCUiU/8i6k0XCkBwzpQ/N0aU1EbKy0G2l3E/ZHVlPC3+3Yr2nXpsebtHyTEoc5z+Gm1kuA\nfwDPjvlfvIldANgVHuoChmmkPSo9ni3fsom6mFRNZFcNsr6Yghld2D3eRLR7gxkfFSZUIu0WP5fa\nxUPDLhZ5xawyZznSrisVMI6061psMtJOCBmBgBEdrxmEpAoj7U2IGmk3EXGlyFF4/7Fej12zKt1k\n2WAZVyo19jtiI9rRb3VCX3TDr4Vp26prW64avD2379LAfbYIRdoTivbcYMs3LqtnEXUxKVeFEZ26\nfyrp8YrQNMn40aWdT0wh0i4132eTjB/d8Tl4Vhe8XN012bSmXZdQwEwaQsgI+H8rmH1HSLpQtDcZ\nUpfSbTBZ9qRe9HUIuw7yukiRbgIcebxiZOcIid3EGrtiWJseb2amNcnXDmqS3DT4t9U0/lCk3WxC\nr+7PSHu2cT010i4Ti/ZX1vdYGZsftS2ZUaRdyuhIe4omjgD0CjcC1dUdKC16Wc34cZO1ngT0Cx3s\nuUwIGQl/IISRdkLShaK9ydCZpxm1WvLCk2UAGIe+1F2bk6THA6W6dqsGb9r0eLPX0pNDdQr+c9lt\nW6WKdkMPg6iWbxlYVf/TM+/gawuWY/kbm0beuUkoRV6H3ptq0uNVDdc7EL+OOy66VmhxUev2gSEj\nOrt+EMki7TrxnLPsHp800i6l1Na0M9JOCBmJgBEdF/IJSRXWtDcZRU+GIusm0WFddBkA2kQBBddD\ne0su8RhLD6SJ/BhMIqXG9fnd4m3rNe3qa2liAOVJiSIctGoWQazWtCsiw9SkypHFgAdCfjDSXt/0\n+K7eAXztd8sBAI++sgFP/89H6jqerKBGXm2kx6eRVaEKWpMuFjo/iYoRnc2FOV12j5ohMOzxGoGf\nF57Wrb1aip5nrfVknO2EEFIhYETHSDshqcJIe5Ohm+yapMfro8tAGwp2o8MJI+2649/jvJ2pPu1F\nT8KFfpHDbk179UZ0unKKloxE2l9cO1Ra0NVbGGbP5iJsRCdNMroBhEW760m7nhVAyO3OZDFpuPR4\nmwtz2qi6QRTbkfoMBRPhPxKeF/5O69Lyo4iK+lOzE0JGImBEx4sGIalC0d5kFDU9fc3T48P7t6KI\ngk1jMl2EyyRCrGnLtLPYkIJrs7oAYpbiW4wQ7Xb7yVcv2nXlEJWWb/Xu076tEBwXJwwldC3fTCMg\nupfSdrRdLbsw8tbQpcdXjOhsjjOpIWaEeJaevUWmosZJ36SmPSoLg/WphJCRCBjR8ZpBSKpQtDcZ\nnka0m06WdaKvFQWrgljbaklnChV5fFi0t2HAshiWcBQDKKNooSfhRnwF04y06zIloihq2klVIu31\nrl/r6h0I3O5Ooe56NKJmw+TgGS9o6KLqNks2AF0XC4POC1Iir/ZpT8GITpvxYyCIo8z1hDR/T6JQ\njQcBswXOqNer3pk0hJDsEzCi4zWDkFShaG8yikmN6DRpqYD99HidkZxnIho0k+0WuFZrSfUGUGav\nZVSk3Wa9uCo8HMRPdS64XiizIp+Rlm/rt/YHbnf3UbQDUe7x5udQKRTtTshCNe0m3hqaxaSJ6AEg\nrX7H9V0s4p8/Kj0+D9daZpI248eol3yEaGfUjBAyAv5Ie72z7whpdCjamwyde7yVSHvZiM4W2omx\nwURUZ0TXgqLlSLuX+LWsTaQ93Kc97ulLvej17vE2Db+qYWOPEmnvp2gHSpOoQHq8kMYLLNr0eNuL\nNGpNe8IynTZRQLvlbBpd7bm2DVwUEenx1Tj6R6F7LUzb5+mgZieEjIT/esvsHELShaK9ySh6Xiil\n2yR9VhtdRqmmPe30eKOWb5oIl+0xuhoDKLVOdziKnkQhooFDmim+OQPBUPDCkfYWlF7bev9Ab1Ai\n7Vv7aEYH6NOlTdqUAfp6ZtvlEOEyHROhidBiEgBMQE+mrkNRteV5i23fVOPB4R5XezzT4wkhVRIw\nouNKHyGpQtHeZJSchtUIl4ydCul6wXZSFVpRsGtUpZl0mqTHC000rEXYj7QnceL3pIQra1/T7ggJ\nN2ZWxLCR9jq3fNvQrYp2RtoBfbq0zuNhOHSXA9vvt7rA5cCLXbbhRRhiThTdqV+HTELQUenxObjW\nFkFK31GKdkJI7QkY0fGaQUiqULQ3GbqevgIy9sXW8/RGZm0oWDWq0qalmvRA19a0242069rfCYN6\ncbdG7vG619KNKeKKrs49Phst39T0eIr2ErpsGBMTR0AfMbG/SBNePIz7kSo9x6hIu8XrkOY5ewaC\nOKo9XA6etetlaSFVeRwLfdoZNSNJsd4mkmQO/+JjvecEhDQ6FO1NhutJjWuzF3uCFjVZbrUcxdam\npRoID6GJcLWgaNeITpOGbJJ67noYpqbdohGdpr7fLcYTuAXPC2VWDLrHZyw9njXtJbRu4jbS462W\nbIQXFhyDxUNdK0KgHGnP0HXIifge54VnLdKuW6Qx6WJB93iSBj958GUcfMn9uO7hV+s9FJIibPlG\nSO2gaG8y9O7xZpNlfU27Xfd4nZGSZ2JEp5lst1o3otOZ+sU3eXM9TyPaSwfbTPHVlRXEjbxqI+3l\nmnabCyCmSCmxQTWiY6QdQFR6vKERnWb3gaK991uXpZIzWTyMuA6NF72RbuhVobnmmJi8IaKNnW0j\nupxalmMhPZ6anVSL60lc89eXsLFnAFc/8FK9h0NSxD+nqnNDGUIaHor2JkM32XUgY19soybLpZZv\n6bZaMvlFEJr0b9vp8ZGvZeysBYQdsFEYPLctdKUCRU30XUfBDZdT5DMQad/aXwyJSBrRldD5TmQu\n0i7DGT/C5Lvjhfu0A/Y7ROh6spt4awzb8s3S9dLVtL+zUdPO9HhSLUXPQ1+h9Pnu7i8yTb6B8ZsY\nMzuHkHShaG8yoqLDRkZ0EaLdaguwFCLtOSHhuvaEXdL0eM+Tg07sFcaglPKdeqlBTBFX1KQhD6bH\nW3YTN2Fj90Bo21amxwMIt3wDLIl2y6UlanTY6LsT0cUiD9eqt4auZZtOyOuQUkamqduPtIdLnkyO\nN9lOyEionx1+lBqXQMs3Ls4QkioU7U1GUSM0TaLDUZPlVlG0awClM6IzER4RE2thU7RrXgth6MTf\nIlTRXhKjcd3d46BLhXdjRtqLw0Taaz2pv3vF2/j//vwCNnT3h5zjARrRVdC5iZuL9vC2AZuiXVeH\nbVRaoq9pb4FrueVb9enxUQucQCnDxlYJjO6aTvd4Uk/Uz47VhTSSKVxG2gmpGfom0aRh0UXa88Kg\nT7smFRMoRdo3p91qKWF6PAB4xbDYqxbX9eAI1R/ArOd9i/JajhH9gEw/0u7FNaLT1LS3CBeArGnL\nt1fXd+OsG/8GAHh9Yy8+vt/00D6saS+h69tt0qc9KpXVZmaFF2WIafDd0V2HchbTzgEkaj3pSn0K\nP2CWVTASOlM/k7r7qEVGpjSTaglF2qnZGxa2fCOkdjDS3mToUrpL2+NPRGtjRKdL6Tb45Y8SKRYj\n7TohlDM09Qunx5ci7XadujWR9pivZammXfc87YmOONy2/K3Bv//4zDshEzqA7vEVdG0ZTb47UW+r\nzWiZzojOKOMnok+77Uh7svR4fXtMoNyn3dLrWdQspJq4x0ddC5jqSqpF/Q7ys9S4BI3o+D4TkiYU\n7U1GpGiP6SbuuR5yIkJcr04AACAASURBVHxhLjmzp5seb9IfWUSK9rDYqxZd2rkjzPwBVNHeVkmP\nTzvSHrdPu+dpxVGpdrh2P9Dbj2sN3FbbvQE0oqugXViL+X4D0Z+9gWK6Ld9yRt8daBeTbH8uZQL3\n+OHS40t92tNzjxcRrvU6ohZb69gcgoxyQqKdYq5hCRjRcXGGkFShaG8yip4XSo8HSmI8DlG10G0Y\nSD3SbtRqqRY17ZrXQhg48XtSE2kXZSM6i69lkl7ThaIbKgEAKi7dtZvVd7QHK3lWb9oW2odGdCVc\nz0uUHh8V7bb5frtSQoSEptl3R2tEJ4p2DRK1Jo4mWUlR6fGuvT7tmm4BOiPPyOMj3m9GzUi1qAtS\nFO2NS8CIju8zIaliRbQLIVYJIWTEvzURxxwuhLhHCNEphOgVQjwjhPhPIURumMf5FyHEIiHEZiFE\ntxDiSSHE52w8h2ah1CpJE2mPGcWOitC2CsuRdp17vMn5o56PxUi71qHexInf9cI17YOR9nSzFuIa\n0blF/SJH3qLoiINap/z3t7pC+7CmvYSbMD3e//HtQC8Oc55DHnbFsC4KbdqnPcqIzmoGiOY6FNfU\nT5dNUCEvPGvXS93igFFNe5QRHaNmpEoYaW8e2PKNkNph04huM4CrNNu71Q1CiDkAbgXQB+BmAJ0A\n/hXAlQCOAPApzTFfB3ANgI0AfgtgAMBJAOYLIfaTUn7DztNobKLaNulSvXVERWhbUbDmhgzoJ51x\na0kBRNa0Cy/l9HjI2BEqzwtHsdNo+aaLsnoxRbsXIdqti6MR6C8EPw8vrg1dVugeX0ZbAlNFpN2B\nhwWt38f7nddwr/sBbHFvsDZGz0MoOuxAxm4b6UpNdBnltHObed2a1y2uIPaGqWl3rKbHhzs82Gj5\nxj7tpFrCLd/4WWpU/It7XOgjJF1sivYuKeXckXYSQowHcB0AF8AsKeXS8vbvAvgrgJOEEKdIKW/y\nHTMDwGUoifuDpZSrytu/B2AJgPOEELdKKR+3+Hwakqg02bhR7KhIexsKqRtAweAHIco9Xto0otOm\nx8c3aHM0Cwjtwr4RndYfIG6f9giX+VrXtPcVRx7vtoKLoushn2vuqh9dFNuk5Vtlgn2wWIn3O68B\nAI7PLcHvbEbaNenxDrzYX/GoKHYLilavQ6rDPRB/8TAqGwCwm6lSdCVakrjHR4n2FL7fRdfDn59d\niwljWnDk7pOtn59kA1W8MQLbuPjfWylLXSeEEHUcESGNSz1mtycBmALgpopgBwApZR+AC8o3v6oc\n8wUAbQB+XBHs5WM2AfhB+eZX0hpwIxGVFh030u4Xqv2iffDvVhQtt1pK1qddN9kG7Na068SwSXq8\n8MJjGYy0W61pD7/nsY0Ho9LjRdFuRHME1Eh7hba8g7GtQxU1dJDXR6FNslQqH+sjcs8GtlutaY+I\nDpuYOOoEcR4uClZLS3Q17TEzaTSt9yo48KyVwHgy7KSfM2ifF7X4lkb1y23L38LXfrccp13/JJ5+\nM1ziQhoDpsc3D3yvCakdNkV7mxDiNCHEt4UQ5wghPhRRnz67/P+9mvseBtAL4HAhRFvMYxYq+5Bh\niDKcc+PWafoingNiyNG7Rbix66TjoHM/NqnLjYq064RyteiyDhzI2P2NHc3x7WnUtOsWQGKKdjdi\nkcN6a60RiIq07zmtAxPHtAzeZop8KUJqIz3+MOe5wPaBok3RHk4dd0zaJUaYvOXhwk25TMfEPT46\n0m4vPV7fPi++P0CUuE8j0v43n1BfQdHesFDINQ8h00GmyBOSGjbT46cB+I2y7TUhxOellA/5tu1Z\n/v9F9QRSyqIQ4jUA7wPwbgDPxzjmHSFED4CdhRBjpZS9ww1SCLEs4q69hjuuUYgU1jGjpn6hKoWD\ngmhDiyy33yqG23BVjU60GqR86tznAX1KerXoonAl4RHveN1Y0ujTnqTl27BGdHWsaa+w97TxeLrQ\nBWzuA8BIO6AXcWZ92iXa0Y/9xcuB7TYX5XR196X0+IR92oVtIzrd4mHMBc6IunugUntvZ5y6RZpK\nxk+cH3j/6yXEUBVSGnXI/sVICrnGhX3amwd1cY/fa5IFlq7qxDf+sAJ77NCBn552EHJOY5Rs2Iq0\n3wDgwygJ93EA9gPwcwAzACwUQsz07Tuh/P/miHNVtk+s4pgJEfeTMlGT97ju8fBN3D3k4DpDUU7P\nomjXG9ElF+120+N1oj1+TTs0YxkjatOnPbbwiBBqLXBrmx4fEWnfe3oHtvO1g2OkXS/iTL47ngQO\ndV5Amwi+ll7RoomjlKHWkw5k7Ml9aWFC173BtmjXnSvuAme0EZ3NcRY1Ef1Seny84/3ivMXnB5HG\n5JvtoZqDkBEd3+uGhf4FJIvc8NgqrNrYi788txZLVnXWezjWsBJpl1JepGz6B4CvCCG6AZwHYC6A\nT8Y8XWU5xOSbH/sYKeVB2hOUIvAHGjzmqCSqljl2TbsvQuuJHDzRisp8URYsRtq1Ne0WRLvF9Hhd\nirkDiULc9HipS4+37x6vm73H9zCIirQXsxFpnz4eD65cP3i7u9/e+ztacTU9zKVBlFxKiSOdv4fP\nW7D32rqeRKu2Djve8bo6bsD+YpL2OmLQpz0qPd6my31U1kLcSLk/4t+WcwbLINKJtA+ds5bXD1Jb\nmDLdPIQXaOo0EEJ8bNk2NF9ppGBO2kZ0Pyv/f7Rv20hR8fHKfibHbDEaXRMSFTmN6x7vj9B6IgfX\nGaprh9uXaGzBB9K5x5vUtKefHq9zYDeJtDtaI7pyerxF4aEzIYvrHh9pRFfjPu1RNe17TR+PDkba\nA+jS47Xfpwg8Ccx0Xgltj/I3qAZXY9LmQBr1aY+qabdaWqK75hi4x0cZ0eWstnzT97w3MfWr0JKv\nXaSdbcAaF/W9reVvBaktLIUgWcQNZHU1zkpS2qJ9Xfn/cb5tK8v/76HuLITIA9gNQBHAqzGPmV4+\n/+qR6tlJtOFcXBHnj9hJOPD8ot1i+qy+1ZKNmnabRnQRoj3mj1ZOE2kfI2oTaY+bHh/VIs967fAI\nREXaJ4xpoWhXKKXHB98bkywVV8rBLgaBc9gU7ZoxmkSHXVcir6kXt50B4mhbT8Zc4IzIBgDsRtqL\nXrh2Pifiu8e7gfR44dtuZXjBx/KdlEKucVHfWy7QNCZSSqiXGabHkywQLMWq40Ask7Zo/2D5f78A\n/2v5/+M1+x8NYCyAx6SU/lnjcMd8TNmHDEOUa3hcN3F/pF6NtAubol0b4Yr/zXMi0lJtinZdKzUH\nMvZkWej6tKNGNe2x3eOH69NeuyvhcH3aO9qHfBVoRKePvBpF2iNM3qKyLqohKqU79uc+4vlY72qQ\n4DoU5XAPADmLi166nvUmr6V/HK2+SHtcU0ATig0a/SBBVJFOIdeY6N5XvtckCwRLsRrntyaxaBdC\nvE8IMUmzfVcAPy7f/K3vrlsAbABwihDiYN/+7QAuKd/8qXK6GwD0A/i6EGKG75jtAXy7fPNnICMS\nFVH34grigHt8Dl7O15nPYnq8VrRbqGnPSZs17eHxmLStymnG2J6Ge7zmtYzrHh9VC51H0WprrZHQ\nRdqP2n0yAGC7Nn+knTXtnqam3WTBS0poxaZVIzpdSreQsct0EPH5zZVFuy3Bqcv4iS3adYsnZfIm\nCxQjoCuHyMELRcCi8C8ypm1EF3CPZ/S1YVF/vxhpb0x03+Gsfq/7iy4ef2VjpKktaSwatRTLhhHd\npwCcL4R4EMBrALYCeA+AjwNoB3APgMsqO0sptwghvoSSeF8khLgJQCeAE1Bq7XYLgJv9DyClfE0I\n8d8A5gFYKoS4GcAAgJMA7Azgcinl4xaeS8MTFd2Ib0Q3dLwUDmTOF2l37U3qtUZ0GUuP10baRfz0\neG1Nezk93uqEWRtpj+mAPZx7fC3T45Ue4WNacvjenH0BBEV7N9PjUdS5xxv2adcZqFlNj9fUtFce\nOxYRor1FlMZd9GQg1btqEhjRDece71ivaQ+O08iIzh9pr6F7PI3oGhc124ylEI2J7hqR1U4BX5i/\nBI++vBFH7zEFv/7CIfUeDkkZL5DVVceBWMaGaH8QJbF9AErp8OMAdAFYjFLf9t9IJewhpbxDCHEM\ngO8AOBElcf8ygHMBzFP3Lx9zjRBiFYBvADgdpSyB5wBcIKX8lYXn0RREifPYNc6KEV1qNe0JI+2O\n9IZ6CgS2WxTtmjGW0uPjHa+L+leM6AopG9FJTT29jmj3+BqnxxeGnsPNXz4M++40AePKYn38mKH0\n+K5tjLRrI7xWRLvlPu1CM+GL+xgRor0y7qIr0ZKrenilh4g0kosp2nUZD2VsGjnqugWYZPz4Jzf+\n9Pi03eNrmalDagvd45uD0ZIe73oSj768EQDw8IvrIaWEEI3Rt5voadRSrMSiXUr5EICHqjjuUQD/\nbHjM3QDuNn0sMkSUOI/tHu8TcVLkIPND6fGOZyc9Xpc6W74n9jkcWRwU7RIComx6lbMaaU9qRKcT\n7fYj7boFkGqMB/3UM9I+bUL7oGAHgCkdQ5/B9Vstth0cpXiuFxbEBj9ankTI2Ayw36dd9x2P+7kc\nUbR7HoBkqt3V9JIHYFbTLoZp+WZpIqHzB8hVGWn3p8en8fVmpL05YBuw5kA3T8ni91q91rqeRN5G\nJhbJLIFSrAa6/qRtREcyRnRNu3mkXQoH0rGfHl/0pN493qim3ZfG3zJm8G+dY3vVRIj2+EZ0mj7t\nIoWadp17fMyrWJRoz8GFlLVbVfdH2tvyQTE21Sfa126x2HZwlOLqBK1BaUlUpB0WF7yKrj6KLWNG\nX3XfHaC0mATY+VxG1aRHtZNUKRn6RYl2e4teunE6Bj3v/ZObgHt8KjXtjVlnSIKwDVhzoE2Pz+B7\nzc9j89GokXaK9iYjSrTHFXHwt3wTeSBvX7SXonDVR7gAwPEvQuT9oj3tSHv8XtN5zQJCGu7xurrc\n+OUQERFN4Y9opo8/0t7eErxs7TC+ffDvdYy06z0MjPq0p1/T7kXUtGsXHLQniBbDAFCwkHrtenIw\nQ0d58NjHa69jAHKQKBTtRdqT9Wkf+rvVtyCWxuSbkfbmQP1dyGqdM0mG1ogug++1+nuQxTESuwT7\ntDfO+03R3mRE1bTHjrRLJdKeGxJMjsVIu25CH2UupyJVQdCSjmjXR9pN3OOja9rT79OerHa4xVc7\nXAuGi7RvP7ZlMEK4ta+IbQPN7Q6r/Y6b1LR7SN+ILsKkLfbiQsR+wfT4ZOhc2QHEzhv3VBd+X6eN\nHFxrvhVFz4twj48r2kvHdqAXE9Dj256yezxr2hsW9bPHBZrGZDTVtPvh57HxCYj2Bnq7KdqbjKgU\ncy9uunQgPT4P6Yu0O5q+49XguhKOpqbWrNWSP9I+tLCgi25Xiy5rwWSyrBPtY0U/AImizSKcBJH2\nKGOwPErba/HjJ6UMRNrb8sHLlhACUzv80fbmTpHXLsgYpsdr08JtGtFF1IvLmF0sotPjS9sLxfTS\n402M6AKLH3m/aPcwYOk7rlsAMSnTcaXEe8VqPNH2NVz+5sl4n1hVPm8KkXbXP5FqoJkUCaAu5mZR\nyJHkjJb0eGZ+NB/BSDvT48koJUqsxY5wBdLjHQh/9Mizk5qsc0MuPaCBAVQg0j528E+bNe26yL+A\njG16oevTDgBtKFhOj0/gDzCMER1Qm8mYX7C35hw4TthAxm9G1+wp8tpIu0F6vJTpR9qjnNljfy59\not3zXYMqZRsDMcX/cESlx8fN+AkZxPnaY+aEiwELCwulx/FChncm6fFFT+Kylp9hnOhHq+zHz1qu\nBFD6HNgm7/bhi7l7cFLuIbiN5A5EAqjCLYtCjiRntBjRMdLefARr2us4EMvYaPlGRhFeRIRKxryI\nBcS9yAWiR7Z6oBc9Tx+FiynaQ/2RW/yR9gy5x0M/ljHot/ujohXtcdOQR2qtlf7VMBBlb9GvM9KM\nbgjdApy2hWIENTGii4hiR5XvqPiFs8y1A25poaaymNRvoV48MtJusHiY9x9fw0h7ThgY0bkS7xVv\nD97exVlf2p6C0Ppo/59xVstvAQDzevZCqVMsaTRCLd8okhqS0dKnnZkfzQcj7aQhiIpkxRVx/rRU\nzwmKdluRds+Dvj+y0WR56PmI1qFIex5FSEuTUd1rmYuZliqlREuEIB6DgRpE2pO1fBuqHa5BpH2Y\nevYKATO6Lc0dadelmJt0XnA9D3lNeYqwKNpLkXbdwpx5yzfpK3+xakQXmfETd4FzmEg7PAwU7Xgv\nuJ4Xul46JjXtUsLVTAXSmHzv6r0x+Pe0vtesn59kA/WzQ5HUmOiuMVl8r0Pu8RkcI7GLP6DUSJF2\nivYmI6ofe+y+3V4w0i4Coj3dSLuIOwlVauKFLz2+FUV7F+zI9PiRz+96crD+VqVdDNh1Zde9t1WI\nIz8tolzTXgOHj+Gc4ysEIu2saddsNEiPjzIpjGteGAM3wj0+dgaI31vD71lRFu0DNiLtrr71pBM7\nPR6RNe15uFYWFoCSyU5eY0QX9zrnehJFTU/7NCLtgYwPi58nki0YaW8OdIv2WfSq0PVpJ42N/y1m\npJ2MXqJq2mOLdl//c6FG2i21fIt0lq4yLdXnHt+CorXosG48DmSsHy1XysH6W5Ux6LfqrKxLja5G\nHPmx6dI9Ev1Ff6Rdf8nyR9rXN3ukXfOexa3DBqKzK2DVPV4fxXarSY/3ifYWi6K95MpefetJ15PI\nieEi7bbS48Pu8SZlOlGiPY065MDn0KK/CMkW7IvdHOjT4+swkBHgIlLz4Z+bNtL1h6K9yYiMqMc2\ngPJN3J0cnEC9uK2Wb/pIu5l7fLRot3XBFlHu8TEj7a1RkXYMoJByn/bYLcCi+rTXMD2+r+CPtOvT\n46eMZ6S9gnZBxqCmPcpwLsqxvRo8KfUp+DHHKQLp8cEINgAr7dSiesnHTY8PufAH0vjtifaiGx5n\nDl7sMiDXi0qPtzK8AH7Rrrt+ksZA/Y3NYp0zSY7eiC57ql3NCKQRXeMTrGmv40AsQ9HeZETVjEal\nzWt2HPo7lB5vqeVbZJ/2alu+DYn2VmEv0q7t0y5krFSc4dLjW4XFFH5AKzLi17TrBVwt+7THirR3\nsKa9gnZhzmAiFdV2zaZoj3IOr8aIzi+GW4QLQFoxoit6+taTupR5HSXRnn7LN51xYM6gi4XrSbhS\nkx6fwsTWfw23+Xki2UKNbDVSpIsMMVpavrGmvfkIusc3jmqnaG82oozo4ka4fCmN0snB8Ue5LDmz\n22/5lk6kHTqHbcRbAPHUelcfrShYiRRW0KZGx0yX9oq+9xtDrdYqfdpr8ePnj7RHGdFNHc+Wb4Mk\nTY+P8KawakQXkYIf2xDTn1qdawXE0E+ZLUEclcJvsniYjzSic+1F2jWLnI5xTXt4KpCG0PL7AZh8\nJsnoQi3vokhqTPRGdHUYyAgwPb658DwZiFVl8TNZLRTtTUa0e3y8T7VqROe0+qJcttLjNemepQeP\nmZY6Qnq8rTZlUemdUYLEjyuj0+NbUbQq2pP0aQ+0CAy8juU05BrXtLe3OKUVD+WzMGlsK/Ll/u2b\ntxXQV2hiQZAwPT6qdt2myIoq04nrHh/47jl5wGkZvJmHi4IFQVzqYqEzxDSJtA+THm+t5Vt4ccEx\nSI8vehJFTffXNFKaA1kKrGlvWEKRdoqkhkSXaZfF95qR9uYifP1pHNVO0d5sRKXHx52Qy+BkOY1I\ne3QtqZ2adlvp8VGT9zjpYUXPi06PRwH9xfiT7pHQpvPGjbT7BZzGpbvWkfYDBpYDV88EfnIo0LNh\ncLvjCEzxOcivb+Joe3L3+Ij0eEvfbyBatMcVio6/paOTB3JDor0FRTuR9qjrUMz0+FL/dH96vC/S\nLjxrC3Ou6yEngq9bzsSILqrlWyqR9qHn7LCmvWGhSGoOdNeYLKbHq3X2Way7J/ZoZCNMivZmI0Jo\nxo28CjXS3uKPtNtq+abv4Rw3wlUcrqbdYnp8ZElBjMmo5w21TVNpRRFS2uk1DUD/nseoHfY8Gaxv\n9i1+VJzvrWYERFCJtH/ceQJnv/NNYPMbwIaVwIqbAvtN9TnIr93SvGZ0evf4+J+lqPR4x2YNcmQX\ni3iPEbgO5fKAM1Q2kbeUeu5GXIecaiPtOaWm3VJ6vG4BxCg93tWL9nRq2pke3wyEjOgaaNJMhtAb\n0WXvvVYzAvh5bGzC5RB1GkgKULQ3GZE1o9W0AHOcYHo8bLV8U1olDd0T7/gatXwTUT3vYwhiVzWP\nat1u6E9REk3+tPAkaFu+xRAe/UUvMEYRaK1V25r2KejCFS0/Dd6x+c3ATX+v9maua9ctwJkIJBGR\nHm9TtHsRv6JxvTUcX2q1UNLjW+Bac4/XemvEvQ6FFg+DNe3WFrw070vJPT7e4a6MavmWdGDK+ZTa\ne4r2xkX9XciikCPJ0bd8y957Hfo81sBAl9SPsKdG46h2ivZmIzLSHjc93m9El0c+hUh7lHu8SXp8\nwPW5ZezQn8K19gUWUUZ0MSajntryzS/ay9ttOGBLKQPmTxXiTJh7B4rI+XvJ+1/HWrrHF1zs77yM\nNqF8vra8Fbg5YcyQcOvua+J62aTp8RGLTg6K1ko2Ir8j1bjH54Lp8Tm4Vr470V0s4qedB9raKZF2\nG2MEoH1vS10skrV8s70gpxqEOqxpb1jY8q050BvRZe+9phFdc6GWPzDSTkYtUZGsatLjhcjBaRma\niEbVaJtSqiXVpcfHn4QGotgtwQixtVX/COER57UMtXxrHTf4Zxsqkfb0UnzjLNJsK7jBjIVWv2gv\njb0mfdqLHrbDtvAdW94pZX50vgpIiXGtQ9HCnoHmFQT677hJerz+tWuBa+39lhFmjV7sSLtS0+43\nohN20uO9hGU6amQZgX7y9tLjdQsdJjXtRU+ioDOis5xCqi6C6BYTSWPQyOmpZAitEV0GU8/VQE0W\nx0js0chGdOFfatLYRLo2x/xQ+/fL5ZFv9dWLW3KP9zwka/k2jBFdK4rWosNRk/eoSKWfoira2zSR\ndgsO6FFmWnEi7X0FN5jeq5QZALW5GPYXPIwTmhr1LW8B8z8OvPE4cNh/YGzb5wbv6h1oXkGg62pg\nZPoVUdOeL6d0t+SSr/VGGdFFdWRQCaXH54Z+ymylx0e1nozKsFHxpNLWMZdOerzOcT8HL3Z0U7ef\nAw+eFJq9q6eoXJcFI+0Ni7rgQ5HUmIzWSDvLNRqbsBFdnQaSAoy0NxtRQrOKPu1QIu2tKFi5YLtS\nQugiXHFdm6VSS9oSjBDbi7RHucfH6NMuJVr8qeetHUN/1iDSHsfDYNuAp3gDDL2OrWUTPWtmecPQ\nX3TRoYu0b32nJNgB4Ilrg5H2/iYWBNr31qTlm/61K4l2S5H2KD+I2KLdF2nP5Utt38rYNaJLkB6v\nLh7mg+nxnoSd9pMRRnRxL3NFzwstTrRhwH56vBv0KmGkvXEJR9obJ9JFhtDNpbIo2lmu0VyogblG\nuv5QtDcbkUZ0MT/U/vR4Jxeo02wVdvqLu5pJJIDYkfaSAZTvSxuItBfsfYGj0uPjGNENF2kX9kR7\nMcrUL8ZruS1WpL02RnTbid7/n703D7YlucsDv6yqc+5979639q7uHiQMogUIJCMTM2BjDwwMOxM2\n2AwzmCUwASPFjAWCsVnGzISFISwGJAEWCI+kYQmxmUUjJAJJCAmhrbW0drVaUi9Pvb79vrucqsrM\n+aNOVf1yq8o6lXXuuefWF/Hi3eUseWvJk7/8vt/3tT5uO6n/puPMtNvucV9JN9Akj8/DFJlwt2f4\n5LRLTT1iGtGFiXxzRk92MMR0F+3F3xlinDZ1Qhd5vBCaIgDAJlIIiWAeBoC5OTAW7esL0wjqkAYy\nYlBYjehWUFWhbzaPTPt6Y50jJ8ei/djBfvH6M+1qTjtdiG6EYtqd8nj/Hs1Yk6WK+aUeM4k8C8PC\nRo7i36fwaOppDyqP53bVgs/5Lnram4v2EEVHG2Y5t/e0azgvr1Zf7x3jnnbbZlInp25HQR2UaXew\n+T70MNfubxar8vgYYTLQi3lo8cg3fZyqEV3xulne/3j2lcfnQliK9mLjMORaR1cejEZ06wt9w2gV\nC7kR/XFU5PFGT/sKjnFEOKyzp8ZYtB839JbHU4YrVor2KbIg/eJc2AtNO+tlQuhGdFGCnNg3yDxM\n772rt9XfiM4R+RZSHu/qy/Uo4vbTXPMGsBjRLUEeXzDt7bnrZ8Xl6uvdY8y024q4Lkw7c/S0T1i4\nPmzh2vDyiJUz4hJZbDDtwVpLLCoV3zYdIbXNRxr5Nm+NmXm65Te/kUsev6BxJ4ANVsyRIYutfDSi\nOzZYZ6ZrRA2rEd0KnuvRPf54wfDUGOXxI44sXLJU78g3jWmn8njkQRahznxkX4ZLl7WyGDmri/Y8\nVNHu6mn3zGlX5fG0pz1c5JuzL9djEmti2ssxpoGy5JtgMO1bt1gfd4bXRfve2NOuoos8vqGnPZh7\nvKu1xMcPQlfiRGrkW6iedpc83ren3ZjHtMg3AGEc5B057d6Rb/pchJppD7m41Zn22NPQb8TRw1i0\nHw8cHaZ9vB6PE4ye9jU63WPRftzgWhR7sh4KOxLFymJ5wjhms/5Z7W4DKL8Fbi6kaqAWxeCsHqfM\nZ73HWLzQ4vJ4YcjjbT3t/Re1OrtVD7K9qN1PhdoPT43osDwjuoNMi3y7+Yusj9vORqYdgJN59X++\nq2gP41lRvMfi3hoG0x7FqhFdoMg3w0huDpsKyPV8ZZyWnvYg909Ppj3nJtO+iWGY9rGn/XhgLJKO\nB6xGdCvYCqEXcWNP+3rDnH9Gpn3EEYVLFi19JzHdiI4xzFDLPtNZu4y59S1cDFcHebwia40ShWkX\nAzPtPqqFXOju8TSnvWSxAxQe3OEe37ennS25p52Rov2WZ1oft5Vdqr7eP8ZFu+0e79bT7s5pDxZT\n1sOIziimI9U9PtQ4i3nI1qbjdyy5bgKZDMO0285t6U7vAyFNs8rNuTw+LNMuNHn8+iykRqgwI5fG\nImkdYfPN6OrMINnOAgAAIABJREFU/tmLu/jFN34C9z54uf3BC2J0jz9eyNfYw2As2o8ZnPJTz50o\ng2kHkBEWO03bXb7b4IwpW7RH02Da+xftwsHCAX6Oy2kukDhz2kO6xy/uxG/mtJs97UHkvS2Y6Uz7\nLfdYH3fi4Knq691jbUS3uIkjoPa0Z6zekEvAg3kYODe2POYhm2cFVfzE4EE2k/pGvskGeXwSUh5v\nKdojSO+FSq6rflCYigL+oSK+7zPK448HdCZzLJLWE7Y5puvU+2N/8EH8p7d+Gt/5incOFtU65rQf\nL5ibNIc0kAEwFu3HDK4F5yLu8dG8aOeo87HztH9BbDBU5Xt3iFqKNCaOFu0hmHYu7SZVxRu0L0bT\nXFQScwBaTns493jhyrz3YdpT7sxpn1Ty+ENg2m+2M+3Tg4vV13uzY1wQ9JbH18/PIlpohmPa3fL4\n9vOWCVEZuQGYM+3UiG5YeXwE4bUxZ8j4iRFdeT6CRL7ZinYmnWZ/OoxNEAwkj+eqm76vYmHE0YNp\nBDUWSeuIEPL4Dzxcp758/LHrvcdkgx5Vuk5y6REm9PlGZ96PMsai/bjBJT/1lM8qxd6c3eJEep5m\nAVjsngZQRk87i8GJfFby/j3trgU94CePn+VCdY8fjGlf3D1+L3XntE+XyLQbPe232HvaJ/sj0w7Y\nUw26yOOpg3vG6qJ9EjDyTTjl8e3XU8Z1eXysRL4l4EgDjNMtj/czeeNC2yyJ1Q0QIJQRnf1YCles\nnoZcT7IAsDEv2kPKmg0jurGnfW0xunUfD9iN6Baf03YOlsO0j9fjesNszzmkgQyAsWg/bnAa0fky\n7fWkWjLtghTtWRCmHdbFsi/Tbva8qvJ4EWxjwT4T+DBcKefunHYWzj0+793TfvhMe5al2GJko2X7\nduvj4t0nqq/3Uu7Fhq4jbJtGvvcOADBStOeUaWd5uB3rHn4QaS4ae9oT5IGYdrtCIYL0KmaFwbSb\nPe0h7h+nT8miefKoe9pDyppzIRGx+vViHN97dN2hXzdjT/t6wsq095jSdgaSx5vM63g9rjPMTZqR\naR9xROE0T/P8UKWLfxabRXueBmCxXdnikF7jLKKW1J52QYp28DASfqfk2INBSjPuNKKrmfb+TJS7\nL7f9tY2e9mQKgAEAYla8bghGsw0sq30SxHQbiOzTFtt9EtO4GB8XcikmeasIm8FXF9MvWrRn0Wb1\ndQIxuBGdz2ZSxoXZ0x6pKRYhogi5i2ln0ssiwPDmiElOe1AjOsec7sm023LaN5fBtIN7m+WNOFpY\nZyOoETWsRnQ95oydg/7pQzaMTPvxgukef0gDGQBj0X7cENCIjs3ZLcHqnvYs6z/pCmHvw+4iS000\nJk6QRb3kIcaIBnl8+7HMCdufMzPvHigM2PrCFfm2UE97NFHYwkkgRrMNk/xG/U3Z+/9drykY9+d+\nLzCZb3jwFLdO6vSCY9vXbnWP73CepINpRx4u4s91j/hseNmYdiKPjyGCjFM47h3feaiRaZ9v2AXZ\nWHK1GnioFoBigWPK4+dGdAHXtropZgKxVr2GI2ropzWkN8KI1YFt+upSEOtF//X9oZh2bRNpvB7X\nGuvMtCftDxmxVuiZ004X/2wuj5dEmpqHMHlzGkBJr0WkHi0EFkOQMYZg2p2u7PDsyyWKBMHUYjhk\nT3uhOlhcHm8YfsVTID+YjzNgbncDpny33l7cmBftX/I/AF/8HQBjwEPvAC5/BgBw92QHFw6KY7mb\n5ji3NbW84nrDGvnWSR5fPz/XjOhCuce75hufDa/UYNpjzYgulDzePQ95yeN12flg8nj7Qte3aJci\nV2TrADGiCxr5pm6CxOAj47WmGJn24wFbMdTlXGfa86/uhYnj1WEUcevU5DzCgMm0r8/5Hpn2YwYn\n4+bJxClM+9yIjkrP80D94u6i3Y9pn2jFJo/q4i1E5Bt3mOUB8HLAzrO6aOdsokhn6572/kxxzu0u\n95HHJo2R065Fay2DaZdSKkw726hd9sEKKTztcb9zcq36eu+wstp3L3aKWAuOgPJ4WrQvI6fdRxGQ\n5UJrf1GvyyRQ5JvLEDOC8JoujTaf2CzaQ2zMOY+Z8GOtmOVxQ+S0m5FvfoqFEUcPek00nuf1hG3z\nslPRrl0ol3cHKtq19xmZ9vWGEfm2Rqd7LNqPG0IW7XOmvcxrB4A87y89zx3yeOYtj6/HKMCAKIIk\nTBwL4B7fVx7PyeaGiBKFhdsIyLQ7e9o9mNf9lFtcuuvNhckSmPaUC5xELXlnm6fMB23fWn15R1RH\nxgyV+dqId7wM+I9fALz6Ww6tcO/LtNNiL49pT/vwkW9eRnRcGO0vqhEdRxbq3mHmOfRm2rlATJ8/\nUE+7S7UgfM+V5ZhvzuegoD3tWuRbMjLtawtDjjye57VE38g3PYrtyrKY9vF6XGvoSp91asMai/Zj\nBhfD6p3TThbLUWzK43kApr2vLFWSjQM5v8RD97S7TKoAP/f4XCnaNaa9LNqD9LQLuxO/J9M+aWLa\nWT642dt+ytW4tw1b0X5b9eWtUc207x8G0/5XPwtAFpL9h96x/PcHwCzXn2+2OACliFPd43kw113X\nfOPFtHNbTns9B03AMQtwXboMMX172mkxLVisbG6WxWvfTRBDgu94/yYwac6HQ8jjDaadidHFeU2h\nX9ajGnk9YTWi63BP6+uHoZh2fRNpnHfWGwbTvj41+1i0HzsENaIrCjjKYvNARnSuqCWfvQWaAV2a\n5KlMewAJf4N7vPSQpYq8ZvtlpPe0h5PHuzZAfI3oyrxmAECyqW0uDC+P3005thkt2k+bDzpVF+23\n4Iry3EPFjScP530txVoE4S0Roz3SnDDtE+QGM7IwnJuHfkZ0BtNONpNicKR5h00KBxrnIZ/NQ+Le\nLlmsbCyEYtpdGwuA3zwEAJHlcRssvBGdPhcl4EE3BUasDvQiaTzP64m+RnS6bP3K3nLc48frcb2h\nn++RaR9xhOGarLoX7VE8v3xCG9E5e0n9mHboi2UAMnRPu6MYBvzk8UbRHg9jRMddrQYeRftBxqvF\nO4BiY2HJ8vj9NMepNqZ965bqy3Oo5fF76SHI4ykCbA4tAtu9E0N6L6YiUZ9z3YguWMRfj8i3lJuR\njnrkG9CfTemb0y6lXrTXTHsSqmhvnIfaN0CklPaedgzR0y4MI7qR8VpPrPOieUSN3kZ0S2Pax5z2\n4wR9U2aMfBtxZOE0pPL8UFVz2ucLZRK3JAL0tAsXO+wbtWRh2llSF5s80MZCbDF409/f+XyiSCiK\n9gRgxe0Ys6IoCVW0u45lG/YzXrH+AOZFe10cTUNGgDmwO+PYokX7dNt80ORk9eUm2WTYPezIt7y/\nd8IisLU+FEy757kiz1eZdh6QaV/cWyOzMu20p724ZkOw2Nacdl95PK+Po860l8Vrb3l8I9Pefv0L\nCSOjHUClsAkZ1WUy7aMR3bpCXzSPNft6wmpE12HO0NcP1/azcJ8xBGNP+/HCOke+DVK0M8a+lzEm\n5/9+yPGYb2WMvZUxdo0xdoMx9m7G2Pe1vO73McbeM3/8tfnzv3WIv2F94bp4/S5qynBFlREdkccH\nYBeLgthuAOWziJSEKZTzQpgW7SKEw32jPN5jsawz7YDBtoeQnhc57ZZj2VIcSSmxn+nyeJNpH1oe\nv2fI4y1Me1IXlhtkk+E4Mu1SSquKokvRrrrHq0Z0wRiKXky7xT1eM6ID+hftrs3Dwh+g/flUni5Z\nBLCaaS/UALJ3730T0+7T054LgYSZjxuEadeM+UamfX1hLJpHt+61hG1+6MO0A8DV/fASeT3ibSza\n1xtj5FsHMMbuBvByADcaHvMCAK8D8KUAfgfAKwE8DcCrGWMvcTznJQBeDeCO+eN/B8CzAbxu/noj\nPOA0IPPuaadGdMVCmUVhmXbuWMh6F+1EHi9YMbaYFu0BGNC+RnSKRL9krxO1XzxUT7vVPb5lQT/L\ni8KkdLIvxqf1tLPh5fF7PvJ4WrSz+rgeWuRbiUMo2l1FXJd4LdrjzGMqjw+4SeM0oms/Z5mR055o\nOe1hTN4a23Q8jmWuqGkSIIoAMOV1srxv371dwg94tukIB9Ne9bSHW+zoCqSRaV9fmPLU8TyvI2zn\ntcucofe0A8CVASTyppv4eD2uMwwPgzU63UGLdsYYA/AqAJcAvMLxmKcDeAmAywCeJ6V8vpTyhQC+\nDMCnAfw4Y+y/0Z7zVQB+fP77L5NSvlBK+XwAXzF/nZfMX3dEA4TGukqygPSNhIoo014W7Um9YBYB\nnNldZlS+slShy1IBxJO6+BAD97T7MFzWol1j2oeMfHNmzM9ROK9LVR4fq/L4CfIgY2yCybRbjOio\niR9xwt5dNtOus8fZ3nLfH+62jWLDy+81aOEsSE/7lHHkPMxGiKug9Ck001xYoghVIzqgvyeEPl+W\nYN6Rb+T6m28eqg7yAmnP49loROfx2rkQakLEHJtDyOO18cRj5NvaYpQjHw/0NaKzpc8M0de+znLp\nESbMtID1Od+hmfb/FcDXAvgBALuOx/wggA0AvyqlfLD8oZTyCoCfn3/7I9pzyu9fPH9c+ZwHAfza\n/PV+oOfY1x46O0wd1fsw7ZTlkgGYdtdiM4LwG6YuS4VatMsg7vHuwteHaRd0DCV7TbPaWR4k8s21\nqG8zoivj3qqc6mhSMIVLNqLbS3Mt8s3S006Y9imR8+8tu6ddV3DMdszHBNjUakLTJo2vYy51jxfx\nBgT5mOA8zEaIi1H3MUhMrUy7GvlWPq4PmlQLPu7xyrGKTNPO0uW+D1zydgBem4cF095kRNdreOp7\naZulRbvF+iymRtTQN3vGon090deIzta/PkRWuymXDv4WI1YI+vldp4+ZYEU7Y+xZAH4BwEullG9r\neOjXzv9/o+V3b9Ae0+c5rnG+z/YPwD0+zz/KMBb0hPXxdo+HWbRHoZl2R7+rr2uzakQ3l8eToh2h\njOhcx8yjp53lWq84YGS1h5DHu3rv25QVZj/7pjHG5RTtHFvsoP6BVR5fn9uJrMe8dKadtxTtf/vL\nwH+4C/iLnxxuCE3yeN+edlLsySiGIIUmD7ApV7zw4j3tWa6pCTSmPRlcHi+8Fn30WMnyGDKVae9r\n5FhsHtpfw8dbIxeaqd8cQ/S0C51pZxJiXD2vJUx56li0ryNs01e3nnbzsZd3w29s6zL8RZj2hy7t\n4tNPObt+R6wQRqa9BYyxBMBvA3gYwE+1PPyL5v/fr/9CSvkYCob+LsbYyflrbwG4E8CN+e91fGr+\n/zMXGPqxgpTozbTHNnk8cW6WAZg4Z9HO/HraGTeZ9oQW7SHM8rT4Igofph3CUrRrWe1B5PENDthN\n2E915/h5sa65xy/DiK5LT3tCjuv+snva9Q2rmfYB/6afA/ID4D2/AexfwRAQooh30xEx6c20K7nd\nUVK1mABhlDTFCznG4pPTznkj014yzyGM6Kz3DvPraVfk4OUxDMy0N+e0e6RYSGln2ofoabcoqPKB\nlScjDgfrbAQ1ooaVae9wqjPL85fCtHe8HD984Rr+yUveiq/7pb/BOx64GHBkI4aArafdRx13FBCK\naf8/ADwXwPdLKfdbHntm/v81x++vaY/zffzZtkECgJTyK2z/AHzC5/lHGfoCTy7CtFN5/Pz5EWFf\nZYBFmCsyjcGv8KDPLwuOZEqKdhHALE/oGyD1YtyHaZdESs3K42cw7YEi32w9zh7yeMOEDlD67idL\niHwr5PGkN7yFaU8Upv1w5fF/+9HP4vfe/XDxjb44GahYaWKHF+lplyyp1CpAmNaS4oXsm3vM44M1\nM3LadSO6cJFvLvd4n2KW9rRXZp1aT3uIvvvY0pMO+Jn6cSGrXHuKYSLfzHMu8kNOeBgxCMwiaT0W\nzCNUWI3oujDtlvlvmJ529X26Mu3/5+s+Wu0z/9Br7g01rBEDwW6QeAgDGQC9i3bG2FeiYNd/SUr5\nzv5DqtzRuh7iNTklw8GUx/dk2ueyeCqPD1G0u8yoIk+JL41aKhmuyUbNxrIgTLu+AUIKG4+iPSKM\ncDSxMe0ZuJC9M0u5kGALMu3Uib2W8JPiiHGk3K+/d1HsznyM6OpzGxGJ+t5s2fJ49bqKs1381J98\nGH/2wc8Bqca6e1wjiyAXor88XmfaaaRjIKadueYbH6bdyGmPlZz28u/v29MuhKw9HQh8UyyUY2Up\n2hOI/hL+JkNMH3k8l1b3+M35hl1IhtTmVRKinWrE6mFk2o8H+ka+2Vzch3GPV9/H5lrfhCd26ha9\n/eyQU2lGtMJ2Xa2LRL5X0U5k8fcD+FnPp+lMuo5yVX7d8/FtTPyIOYRewJEFpPToJZUai1cx7UrR\nHkIeb38N36gl1YiuGON0Sop2EcCITmc0qRmfx7FkZLEaJRamnRV/Q18mzrWoby3aM246x2tjLBnN\nIdn2/ZRjC6SnfWozoqvHFIvVYdq3WaEQeNEf3oePPvio+tgAag8bXAaJkadKBQAiqRftlGkPVLQ7\nI9/8jOiamfYw8vhGbw2PY0nnMRaZ8vgIYlh5vI8RnbQz9RssA/NUFPjCpqAamfb1xFi0Hw8MkdN+\neQB5vF6kd53XzpyYtD9oxMrAttZZk5q9N9O+jaKX/FkADhhjsvwH4N/NH/PK+c9+Zf79J+f/Gz3o\njLE7AGwBuCCl3AMAKeUugM8B2J7/XscXzv83euRHqDAWeFTS7cMcaQVg2cseE/aVBShGXD3hEaTP\nMBWGqWwBmBJ5fBSg8DCK4bibaoGy/ZG1p70YY2/5rKOnva04OjDk8TazvLJoH2423J8dYGO+gSFY\nrByjemxURUGY9qUb0amLjXKzIeMSf/6eT2qPHU4e30fSDZhGdLRoF4Hk8c6NLS8jOp1pTzQjujDy\neFeKBfNsNVAK0tg0okvAg6gBnPJ4j3koF9Ia+QYAG8jCusdbrvmRaV9P6KqeLpLpEUcHViO6DgWx\n3Yhu+J72rjntpzfHov0oYZ2Z9qT9IY2YAfjPjt/9fRR97n+LolAvpfNvAfDVAL6R/KzEN5HHULwF\nwPfOn/Mqz+eM0GAYKxF2yqf/Mdf7o+cL0GhSF3JwsOSd4Oxp98tplwrTXlzitKc9mRuoTZPF96y4\nlJgy+waI8DiWdHOjKtpJv/gGSqa9Z46zk3ktZO2MMcuz5vJ4tMjjSXG0ZamlQyCf1Sy7jKaAbbwx\n3ZCZoeiUYdhbuhGduti4bSNDeQgffuwJ7bEDFe28wT3em2mnLPZEkceHYkYjZ+SbjxGdZgIZJZrs\nvHSP7+nM7hhL7LkBojLtpjw+YgHk8VJajQeL9/eJfLPL44GiaB+caQ8UIThidSCEubk+9rSvJ/pG\nvtnmvxsDtLWZPe3drsdJrK4Vr+1nI/u+wrB7LRzCQAZAr6J9bjr3Q7bfMcZ+DkXR/hop5W+RX70K\nwE8CeAFj7FVlVjtj7Bxq5/lXaC/3ChRF+08zxv60zGpnjD0dwPNRbB7oxfwIDYZJFWWHPRguYTD1\nxQKUMu0hipEmWWru8+FPn18ukjVZ937KexXtRqtBR6Y9Fmmlc4nLDYVENaID0Dur3eVyH8/Zwthe\nsxfyeEbd45vk8cPNhrO0LtpFPEVse1CZHz8vmjeQYYbp8nvaNXn8CbmHScyQcYlr164AZG9rKHm8\n24jOrw8bUHPapebMzgPEJRZwXTM+RnR6TnusyuNL93gHU+6NnvJ4wfPKoaWWx6s97b3l8Y5IRwBe\n/gBNTPsm0rAMqWXDZyza1w+2An2Ux68n7IZf/XLa+655bOjbrqFvJHzuyv5YtK8wbKz6ujDtwXLa\nfSGl/CyAnwBwHsC9jLFfY4z9MoAPAfh7sBjaSSn/DsD/Pf/9hxhjv8wY+zUA985f50Vl8T/CDa4x\n7TSqzado5zorM1/MJxPK2IfoaW+Sxy/W065Hle1l/cZpyOOVnvb2xXIsm5n2KUL1tMMpl2764NpL\nXe7xtO+++H1fiW8TOCna6XsbIBL5ctx7GV+qLFNqRXuU7+NLbt8CAGxDC9UYimlv8DDwPRRqT7ua\ngS5CRb65PkA92OE0F1pOuyqPj4P1tLvnIa8UC1KQlvGYRuRbACM6F1PucyyL59vnwk2WBmVI7Uz7\nKI9fN/Ttcx5xdND3XKcWNdTBAEZvhhFd16L9QCvar7aFZI04TFivyzVR+yy9aAcAKeXLAXw7gI8C\n+JcAfhjA4ygi417keM6PA/j++eN+eP68jwL4Ninlry5h2EceRU67q6fdg2nXpdbzgjgmDDELII9v\nNoDyeAFLT7vKEPPe0mlddcA6qBZyLpCQoj2eWJj2siAOYFRlc49vk/gaPe2xLac9THHUhNy7aK83\nPE5PinFJCRz0bC/ognRmfpD/gzuKMS+1aLdF/C0oj2eayVuoot0lg2ce0ZOp7njuMqLrKY+nRbtE\nLUnx2QCRUirzWMW0MzXyLQjTbjnfxSD8inbb9QIUTHtQ9/hRHn8sMBbtxwdDGNEN4c7el2nfOVA/\n9z53Zc/xyBGrgFEevwCklD8H4Ocafv86AK/r+JqvAfCaXgM7xtAXeKxj0W4a2ZVFe71gjiQvitJ4\n8f0gl0kVg/CSuNgi35SinRXy+D4wesWVol029ounXGjO7GVBbGPae24uOJjXmInG3eandmaVRB+A\nlWlfhns8Twl7bTOhq35XM+1nJgIX5kPfnXGcnA42zSk4ODiAPsLn3FZcf1vsQP3FUPJ4h1w6hkC6\nYE873ZBaBff4LLe4xxPVUBKMaa/vUREliOfnLIJsnYdyzSCOWZj2EPJ4o2VJ+aWfE3+jPH5gpj1E\n2siI1YLtc2Ws2dcTViO6LpFvlqL9IOON66dFoK9ROhftujx+ZNpXGutsRHcoTPuIwwHXncS1QrP1\n+YYkvOzTpM7N/SWffXtJFYapXCRrBmp9mXZ9A4Mp8tzmgjjNRVXwAqiL0QHc441zXo635Vg+eGkX\nG8y3aB9QHp/VxS5rLNoJ057U53YIqZ0L+wfmB/mzby6m2G1oRftATLuriCuMB/1eQ3ePxxBFOxlj\naRZZvLdfoWm4x2tzENC/aBeSjpFsTHoY0aW5sCd1RPVHbhQkpx3qsaDwOZa5cMrjN5AFZSdsBboc\naPNqxOHB1joyMu3rCasRXYeNPpsaSsjwRIA+zi7Xo5TS7Gkfi/aVxjoz7WPRfoygs646097WL24U\nBBWLTdmjvLeRiHT0xUeQRt6m9fl0ccgqt7fqR1PkvePAuNAW5eT126TIs1yoLHZsyWkP5h7vZl6b\n+nIfurSn9bSb8vhJoL77JvCsZtojT6b9VFKf277Hrwts8vi7TnKc2kiqzPYKIVIWLMgd5zuC9F5M\nxeT+Y1GibEiFinxTNgZoZKQP027ktMcKgx1sM4lb2mxQbHi1LSqNMTIzpz0BP3QjuiamPWZi+J72\nUB4JI1YGtg3rsWhfT9im2C4+MjamHQjf1tanp3035caG9+euHtgfPGIlMDLtI9YCurM0XYz7sNjm\nYrlkjyjLJXoXcUovKVOZKZ/Jlhbt1d9ocY/vA0MeT+PzIBsLhjQXmDCLPJ4y7SyUe7ydaY/hXpAf\nZByPXTvwMKIblmnPuVBy19nEj2nfTurx7KfLm6hTC9MeZTt49l1nqsz2CoGKXx0uI7qYLRj5FqtF\ne5B0CCmV4pxGyvmzw01GdKJ6XL9x0qKdGsi1H0vrGOn/KOaP3jntDfJ46bExlOqtBgRtG3udYVNQ\nddy8yrnAxx69PuZ+rzDW2QRqhAob096lIHatHUIr5PRrssv8oZvQAYV7/IjVBbcqONZjDhqL9mME\nY0GvscNtk60p+bSwR4z3ZzepkRxTGS7XziwFp0V7ZLrHh5DHG73isf+ifpaLKocdAGGx6/OxEcw9\n3i2Xdo3xoUt78zHQot0d+TaUEd2eFjvHPN3jt+P6OUOY2riQzmbmD2c38Iybt7Ct97Qfijx+kaJ9\ngigJK483NpLI/OFnRGfZPNTmoPJxfSAVQ0t1g7N1rjSy5E0jugQCGfdMxHCAa73zFD457cXmgrto\nH949vlvR/r3/+T345pe9HS/8gw+GGtaIwBgj344P7DLkLkW7/bGhY99Mpt3/9W/MzM+8izdmS229\nG9EN9jnoEAYyAMai/RhB6P2PlGlvMSYDgFnGkVCnYUuc2gR5f6bdwcL5LJYBR9SSZkS313PCNTPv\n6QZIs3x2lnO1p93GtFc97X03FwQiZmfaXcfywUu7xRhoT3tpkmeRxw/FtO+nXDXs8+xp34oPp6c9\nSy2SudkO7jizabrHDySPd2/SePpBQI18Y1GCSGHa+4+bS9UQsyvTbua0J9brsvdmEmXaSd99xCR4\nSwa8UQxbctrLY9Bnc8EwB6XwMaLL3fL4LjGBXrD1tHe4ni7vpnjnZy4BAP7sg4+OheCKwsZyAd2K\nuRFHA7ZT2mWj79CY9g6X4o6FaQeAR8e+9pWF7bNhlMePOHIQRqGpyeNb+jRTkm0uwGpTJU062nuX\nVJHHk0WuZ2wVlYVWrs3Emb2Qx/fPaVcX5VQe3zzO1OhpN1nsUEZ0NnYLmCsrHB+YD82LdivTnqjH\nERiuaN+d5fbjZANh2reiw2Has9TyIZ7ewG2nN7F1yDntXVhTncWOSBRhCNd7IeBUqbii4CiyLEdM\nN6JYZDei6820q0y5ILFveVvRbjDtpTyeMu39DfNc6RCAKu9vGmepTNARWh5vY/67FO1X99SWkieu\nj32lqwjXPNM1G3vE6sNWCHWZdl0eRSE/t6U0N6y7FHCuov3xcf5ZWVgTLNajZh+L9uMEM7JNZ7Gb\nr+o0qxfsgl46iglUAHm8dMhSWXOveDW23Ma003zxAPJ4KVUGW+upbetpVyPf5s9NLJFvPTdAhOOc\nNqkBHrTK48uedvU4AsMZ0e0ZTLtfTvsWkccvk2mnpnkVZju4/cymRR4/XE+73YjO3z1ez2lXvC9E\n7tWi0jhGPdGAMu0en6xCMYhLAMaUYngSyD1eGQuLIMmc18a0Z7lUVU0WI7pyLu7jlNxoROfZ0+6W\nx/OwbLaDXrklAAAgAElEQVRtPB0UJ1f21A2jkelaTdj6nIH16SkdUcN2qrucZzfTHm5NYTVG7DDn\n6s7xJYZqCxzRH3avhfU4X2PRfowg9AVerMYYtRrRKUV7vUimr5MgD7BYtshK0R5TVoIyOjZ5/DSI\nEV2TP0DzOFOuGdFV/eIWI7qeGyAuJituyLx/8OJcHm91j6c97cXYhsppL4p2i8u+DYRpPxHVz1lq\n0d5JHj8Q0+7sae8ij1ed3WnRnrD+kY5GoamcV4/oSXpNWyId4+q6DCmPjxRTTFdhUiLlHDFTFQsA\nlJ72OMDmAhdSaVmiyiSfDRCj1YAg9oi26wIb8+9jlldCZ9rH2KXVhOu2G9sZ1g+2NUSXTd3McU2E\n/Nzua4xoM6IDhlv3jOgPm4JjXTYNx6L9GMHMWacGUNI5gZbI8nrRJJidaY8DuMerTLvWK95RHm/t\naQ+R024U7f6997PMEfmW2CLf+rpLkwU9kfcW8vguRnSWnPaB3eP30lwxomsu2usNj5NUHt/zPHeB\nyC3s+WynkMczXR4/UE87l4iZeT66yeM1I0eNxQ4RU6ZETyqRb+3nS9DWAmuCRaCcdoVpj5X7h6p5\nbJj5mHZWTHu/nna6AULj83wK4lmbEd0KyeN1pn0s2lcTrs3g0UF+/WCNfOtwmjPHHB2yaO8bQXj9\nwL7B3ldxNmI42Ap0n7joo4CxaD9G4FJb0HftaSd94IIyOoHl8ZKrTF+JGALcRx7PLUx7VC+6EyZw\nkPaTJxv+AFqUU5MUJ+UCU7pQrnLabUZ0PT8YyLEQ2uZK+WEmhMSbP/4EPvXEDg4yjkevFYvhTdYs\njx/cPV5n2j1z2k+QQv9giRI2aZPHpzdwanOCbT3ybUCm3S2P7860syjRCuL+RpOGezzpaW8zopNS\nqkV7uXmoXJdhinZlA0Fn2tvk8VzfIC2LdjXCEuh3j+sbIHST0yfzvjCisxfOoY3orG72fZj2Q45d\neujSLh65vHeoY1hFuD76ukiSRxwN2IqjLgUxXSdtJPXcGPJz23bddfFXcMrjx6J9ZWHdqFmTTcOk\n/SEj1gVCQHUK1iTdbT0feV4vlqVLHs/6L+qVyCnN4M1rshU5SlIsLgsCxiCiCWJRLPzSWb+inRtm\nWtqxbFigGEZ0lcmbhWnvueMsdCf+ebGYsNqI7qVv/hRe+uZPYXMS4SXf9eVV7/PpiUD1J8amPH46\nsBGd0dPeZERHxkU3G5bJtMvc3tMOKbG1pJ52lzFZIY/3e42Y3H9RMjUK4t4MtraxoDLtLXOQkEjo\npkL53ChGcdMXXhMhMtD1Nh3a0+4yeCxhusebOe1BjOik1OZ0yrR7GtEtSR5vq+a6yOOvaEX7Yfa0\nv/PTl/A9v/UuRIzhtT/8X+MfPP38oY1l1TAy7ccHNra5y3lOyTrp1OYEsxvFZ2hYpt1mljfK49cZ\n9ijCQxjIABiZ9mME04hOZ4fb3ONJ0a7I4ykTF8A9XtqZdt/IN9jk8VBN7VKb03cHGHnYHeLzzMi3\n+XNJUbrBwjDtdFEsmLpHV0rT/uh9FwAU5i8v+L0PVL8/OyXvbZHHB1MDOLCX5va+ehsI075BivaD\nvqaIXcAdRXu2ZxbSA8nj8yb3eM+FSqxHlWmFZl8lTZM8vo1pz7hu4kg2chTFT39vDaXoZXGnnvaM\n6/J4s6c9CiGPb9g89GXaE0s7BQDELKw8HtJyzfeQxz969fDcm1/1js9CyuJa/sFXv/fQxrGKcG30\njJFv6wfbKe1ynmnRf2qznr/7khUU1p72DmN0uccPRVaM6A8baTYa0Y04cmgzomvr+cgI067K42mM\nUd57Uc/o4k5zuG+bbIWQymI7TgjzRBa0WWopsDrAMNPSNkCaFvVpLrRebTNObRpIei40B2zq+p/P\nZca67LTE3afI9FAZ0S0vp30v5diwHScbyLHbJIX+wRKZdmuM22wHmN0wfz6QPN5o25jDWx4v1Kgy\npmWgF0V72J52fR5qWvQV945jI0drpenLtCvy+Egt2tt62tNcqJsfzIzHLHva+4zTUFYoGyC+7vH2\ne6yLeaEXrPJ4//vTZkTn2/IRGg88Vd/TOwc5Lt3o93myTnCtI0amff1gNaLrMGdkjqI9ZOSbzQdp\nUXn82ZP1/DoW7asLK9O+JvPPWLQfI5jssL/jOaC6Y/PIvlgOsahXFnKxWgy3TZSF3JMWHXYZf96z\naDcYTe1YNkmnZnofqVV6XhQmvT+8yLGULFaKdj4vPJ5xy5bxtK98xnmc2yQ/sBrRBXLpdmBvlqvH\nyZdpR724Dxkd0wbmYtpnO+bPl5zTHvka0ZFCNZMxosjMQO9vjijBlHtU3ZjLWv0gXEy72nvf3z2e\nbh52l8fHSqydKY+P5gx3L/d4Y04nx8ODaTfc48l9lICHLYqtRbv/fXBlV33sjVmO6w4WbGgkEVO+\n//P7Hj2UcawiXPPM6B7fHTkX+LsHLjrN0A4brqnal23PFHl8PTeG/Ny29bR3UQPskKL9/Fa9Bhkj\n31YXtjloNKIbceRQLOgtC0m0L5YBQGS1pJxHrsVyf/d4heGick+PjYWUCycDTl8rs5mGdUCzaqE9\n8s0q+7Yw7X2Ldqkz7YQtzOdFu+0D8qe/+VlgOZGfVrF0qgs/sEQjOk+mnT4n5I59GyJbAXJwDUiX\nW7TbmPaYSS8TR9pawhEhYtA8K/r3tOf6PKSbTTbcOxmX7paJWDPE7LnwU+TlUVyz5Wg3oku5MNsM\nAMWIriyW+/guNEVP+jTxFUZ0tGinTLt/4oAXeka+6T3twOGY0UkpcUF73z9+/4Wlj2NV4bp/x6K9\nO37mTz+C7/mtd+PbX/63K9le4Nyg8Zw3FKZ9o/4cGLqnvRPTTjZMzp8ka8g1KQLXESPTPmItYEhn\nO+a0I62dcnlMaFgtp71/0U4XyxoL1zJRzjKNOaIyfrKg5X3l8YY/gNZq0BL5tuHJtO/N+hbtWtY0\nZdp5GStHjMdYUbB/+d1nAWqsVhbM5FzXRnTDTIa7hhGdH9M+kcsv2rmQSKSlzeDgKnBw3fz5UO7x\nDqYdAKQH80o3E3LEYIxZ0iH6tmzoKRZ0Y040Xk9pLtQoQgfTHiLWkTLDTOtpF2057blAYstp1xIc\nAGCvxzVqmPrRTQyP+LyZvrlA7qNiA2XhoZmwMO2skzzevGcOw4zu8m5qXFsf+dx1fOYpSxvMiuBt\n9z+FX3/rA/i9dz+MBy/uDvpeY9EeDm//1EUAwIOX9vDIldVKKpBSJScmca0+8T3X+TKYdsdYfDdB\naE/7OcK0j5Fvqwu7+eAhDGQAjO7xxwiGaZHizN7seA4AIiPyeFq00xxn1t+oSlnIJbqEv10eH9GC\ngGbRk+IgD8C0Jw3xeU0TemoY0VmY9nkv917WU/5JC7WSaZ+fZhvT/s5/+3W47fT83NKivYlpH2g2\n3F/QiG4i63GH3LFvwg1dyl9CCuC6RTo7FNOubyYRiBZ2uHiQyrTHETM25XrntEupForkmoohGu+d\njAvFaFCJAZyoZoRXHVE9vlDmoSgBGFmUtka+ORQ/ZBOxPE/76eLj5EKqm5R0A8THPb6BaQ/uHm/b\nNOrJtJfxlMuEzrKXuPehK/j8W7aXPJp2/PUnn8QPvKo2y4sjhjf/2D/G028226JCwFkkrQnTtUxQ\nJroLO7wM0OEwBiRRhGw+L/qea/r3bdOiPaCBrOu45UJiqrW52EB72lWmfU2qwDWEPeZvPc7XyLQf\nI5iLZd2ZveWiJvJ4oRTtWs9rX1mqIiv1Z7CBwnVUjVoiucik6ONZz8g3fdHewYk/zzJEbJ6RzuJ6\n08OS096XLVQKNRZrTHvxO1rYbiZEmaAU7Zae9oHl8bsptxv22UCKDcq0L6to3znIVFUAxTWLdHYo\nIzqHPL74nU/RTnrakRTyeKOnPax7fBdvjSIu0aG+mJysvjyBFLuzvFdPNtPk7ZIU3G2y7sLgTd0w\nK17HwrT3lceTzUPFid/DfNCIpiObX17qqw6wbiJ4Fu37qV3hcRjyeBfj+cFHri55JO3gQuLnX/9x\n42d/+8DFQd/T/vPB3nJtQY/lqvXk0vViErFig7f6nd9YU8WIbhh5fF/lxw0H056u2PkYUcN2/a3L\npuFYtB8jNDkNe8WpZfViRUxOWF8nDiCfVRZ3Wk972wdX6opaAsAmpOjjaa8FqSTSU8HUfte4xYk/\np4oFUhDRwnMjlDyeSmRZpBQeopTHk02WjQmZEmw97dT9mgkvc8BFsa/3tHsy7YnCtC9npbhzoKkC\nqJfCtYeNx/fdNHLBFfkGwK9AIpsJHFEhj9d6xfsnGmixcuT1GaTV7beE6QdB7mlyDWwiRS5kv7mI\nyuOjGPTj0kfxo/a0l/J4yrQXv+9dtCvyeJUpb/UA0WX8iap6CMu0W/5ODwk/YGfZgcJBftmgTPsz\nb6uZ9Q8+vHpF+x+//wI+9aQp2784oNu965pbF6ZrmVCZ9tU6fnQ4EWOgpLWv9FyRx2/QyLdwf6tr\nfeLTdy+ExA2ihDo3uscfCdg+t1Zt02tRjEX7MYLOyigsNhNWSQkFNSaTCtOu97z2lMeTxS7ruLFQ\nuDZThismX6osca9+Z5IvLBEpRTtrUS0IIs0X1IWfFB01095THi9U1YFiRMc5uJDVbjdjwEZCpgRO\nFsplMcCYwniHyMN2YZZzrfffj2lPxPJ72m/McpUB3r69/vqqWbSnPT0VXOBCVioOHV4LKVLY54gd\nTHtfIzrNWb2LPN6XaWfF8d3tIZE3jejqVan0MqKzbB4q8ZilPL5H0a63Q2hFe+t8ybUkC41pD2l+\nxSwRdMyTaXcV7YfhHv/I5Xrz+lue/bTqsvjE49f7z9cBkXGBX/6r+6vvbyIs4VM7wxXtrmtuxWrO\nI4FVZtpp0asz7b6EyDIi35xMu8fx3E1zlH/m1jTG5qSev8eifXUxMu0j1gJNRnRthSYAsJywGhN7\n0R5iUU/d45mRJd/82jNd7qm4xxMTNZb3WmAJLUqNLsajFkWAILJzSZl2ZXwcDKL/h5fOtNPYqjxT\nNlg2kjmzWkJh2qnxoLr5MdSH1yxvYFV1UAMtUS/w+xREXbBzkKn54aduq7+++ojx+Cwdhmk3Yh3p\n73z66GnRLmNETOtpZ/03aURDT3ub2WQj007UP5vz2L/dHkoVWmQyTR7vF/mmFf2AsokYBZDHF+op\nV8tTe9Ge6ZsLdPMruHu8ed34GtFREzpaHNw4hCgsyrTfc8cpfMG8j11I4MMXri19PC6844GLeOxa\nMYffvD3Fv/mme6rfDVm0uxbHY057d1DV0ar1tNOiN9KL9r497UHd4xdXftB+9u3NBJO4Xj+NRfvq\nwt7Tvlr3z6IYi/ZjBDOnXY1aaruoI8q0J3Z5fBJAPhspRnRkjMwj8i33i3ybIO9V0NEoNckihYVr\nk6WqTDspOhjT2PYcGZe9jqc6zhiS1cdD8FyRj9NdZAhOCjjm3PyYIB+st2uWNbCqOmhUlaiPb1/V\nhy92DjQjOsq0XzOL9r5GiC40yeP9jOjqx+SIi4WY4soeoqcd7jYd1hb5Jtw+B6RoP4Hi+N4IyrST\nDa82pt0o2k33+CryrYfZpHksNaa9ZWFpGtGRzS8W1j3eWqBb2HcbKNN+17n6PPc5v4uC9rTffe4k\nnnP32er7Vepr//8+9Fj19Xc850583k218dxTA8rjXRvWo3t8d6hM+2oVibQwjxdm2ql7/JJ72j02\nFmg/+/ZGojjkp/l4Pa8qrEz7msw/Y9F+jMAFtILWP1scACJOPuhpT3topp3K4yM14MBnEaoYQBEj\nOjVSrWcklM60K/L45g0QmROWVTGOgtbXXjyu3+YCja2KlB56rjHtjSZ0lIFXNj840oEK41nOMaEF\nmifTTq/T5THtuSrl3761/pqbrHo+UE97kxFdm+N58SC9px1Kz3mITbkmx/M2j4TUUF9QebyFae+h\npqFFO4sSNae9haXJuB49WRrREaadBTCiM+TxWquBT0877PdY5GFk1wk9mPYrhGm/+1zdBnFjyfJ4\nKaVifnfX+RN47n91rvp+VYr2Wc7xlx99vPr+W7/sDtxyqj63QzLtrmJoXeSpy4IeqbZqmx6GER1b\nIPKNvMZQkW+uOdBnjLtkbt7aGJn2owLbuV21+2dRjEX7MQKXUi1oO+QjA0BM5PHMVbQzgVnPvj5l\nsRwnquN5Sw/kLOcNTHu4HGdBxlEw7Zo8vmFRL6k8XmePSfFZmdH1YOIUpj1SmXaZp8qHY6sJXQm6\n+cHywXLajQLNl2knRftBHrjwcKDoaafy+NvdDwbA82GKdp15FaivS6+iXYl8K+Xx+iZNf3l85JiH\n2grNlEutaCdtG5RpZ8Xx7ce0U3m8WrTbMseVcTqZdtrT3t+IzjQXVTdA2vpgjXYiI6c9ZE+7Jafd\nk2m/ulvfL3efr8/zzpKZ9qd2ZtWm9JkTE5zenChM+30rUrS//f6LVb70XedO4Dl3nzWK9qHmRacR\n3Yr1ZK869HmwyaDzMGAY0RGm3ce/QEq1FYoa0YWMfHNtsPpcj3TT/8QkVor2VTMGHFHDtnE4yuNH\nHDmYCzwaU9aegR4T2bFStDOmFINZ3q/PkC7uoigpiuI5RNeedlJMh5THNzHtbe7xgjKveiFK5K0b\nZVZ7j3EqqgUWQZBNDCkyz7g3vWhXNz+G7Wn3lcfXxQbLZ0jmCwgumnukQ2FnP9WM6G5zPxiFn8AQ\n4EItFnmktkO0ghTtmbVoz3srabiQSKghZqwrfpqN6DY8jOg2EdaIjkWxotrhLUV7xnXTT1MeX+e0\n92XaqWpBN6Jrd7lXi3aVaQ/Zh2wt2n172vfr++UuwrT3jfXrigvErb6U6X/BrbWD/OPXD1aC0Xn9\nh2tp/Ld82R1gjGFrGuPEvAVqlovBNjzGnPYw0I9j2/ps2Wg0ovM417SISiKmtOeFdI93rcV8rkdl\nfTSJR3n8EYCuUCmxLvPPWLQfI3BdOht162mPeb1giaYnlN9JwiDlPZyxpZSINKYdiuyqneGKbItl\nQC02AxrRganu8RFrY9rrot1k2k15fJ/YN8q0g8UQ1PiOp9qHEnWObyraaZtBNqB7fBcjOvK7/EBZ\nACzDQX73YFa5tgsWA1s3Nz5eWiTzIcA1FluQzbS2e6d4Qv0YXrrHa5s0IYp2F9MetSh+Ui5Uwz9H\n5NuJyoiuT9Gu5rTTe1y2LKBnuR75Nr8e6aZcgISI5hjPdqbdjHxTmfZQfYBCSNUjYA5bIW8D7Wm/\nZXujmquEXF5CBAA8eb2eF28/XRyraRLhzIlJNR6X0/0y8bFHr1dff8MXFxuIjDGFbb84kES+by72\niAL6ZvgyNp+7wDCi6yiPp39fEqtF+3KM6DyY9kxj2pNRHr/qcEdOrtb9syjGov0YwTCi01mVlos6\nIUx7ND2p/pIUgwezxRcDQqoZzoVrM2G4vJh2i2szYPS091rsKfJ41T2etUXTERab+cjj+7Qb0IUy\niyAp055nmjzewbTrUWuGEd1wkW+q6Zgf0458pu3aD7+o39+vN7QEmwAnzjU8GpA+Tu4LwJDHk/vS\nj2mvx5VXOe2EaWeBmPYG9/imQjPjfpFvm5U8vo97vL55SBQ/LW06KRfV/auMc0Lv7/mmXA+mPddj\nPJVWg/aIzIxrRnRaq0KoOoFLbZxzeMvjSU/72ZMTbG/U1/Uy+9ov7dbz4s3bG+Tr5cSp+YK2VC17\nnH2Mv0bUMJn21Tp+fY3o6CbEJI4U0iDkRpw7grAb035iGmM69rSvPPqc76OAsWg/RjAYLlK0T8Bb\nd3LVol1l2mkht9+jaOc6c8Ri0MtUevWSWhguwJD5DmVE1yaPt+afV9+rGwsAsNfjA0xqOe00Yk7y\nTOkdU9zjXXFvgLH5MQTTXsraNxZk2k9M6gXEMpi4/b3aUVrG09aiHQMV7Xq/uGC0aO/e0x5HLPj5\n1tUAumS8SaViGtG53OP7Me1Sk50zFiutNm1MeyHjJ+Msx0bupc25YqDP9dm0ARIzD3l8Y087DyYp\nNOb1Ofwj3+o58+zJKbY36nOxzL72SzfqcdxECmCFwR7Qmd0XSi/utD5WSl/7QOPsk4u9SjjIOP7g\nvY/gnZ++dCjvr6/HVq1IpHL9xYr2+vnTODKY9lBtL86e9o5M++YkGo3ojgDcSp8lD2QgjEX7MULB\nYtvjgRLw1p6pCSnak40t5XeM9MfPZoub3Aip9btqBlB5W9QS15l2txFdKFd2XTrblo/MSNEeJR5M\neyi2kKlFO3iqsNCbCTWia9hY0IzJhvjwKgtDbyO6KCbnWmI7qY9/SCdaF/YPaqbdVbTnk1PV16yF\nqV0UOZca00562n16ImlPu5zL48lrFPL4fpsgQjQZYjYz7anBtDdHvi1atOsbnCyKwSLKtLf3ipcO\n9gDqeygJy7Sb6inVXLTRW0MUTLyrpz2kPN4VRRh5Mu30GG1vJEqm8zKZdlqQ36Qw2CtctE8cRftQ\nTPua5LS/4m8+jZ/84w/hf/qtd+Hjj11vf0JgrDzTTqdwxgr/kzl8Nvvo3JTEDJM4qrxohAzXDtAn\ngpDeR5uTGAntaT9im1DHBc75Z8U8IRbFWLQfIwgh1IKYLNASxlt3Hqey/pCPN1R5PKP98ZIvzB6Z\nffeqPL6NaZ9loloMA1BZYq3Y7CePb4p8azH1I0U7M5h2akRXLurDuMcjilV5PFelzm6mvUEeP5B7\nfF20e0a+Acq5Pj2p/65lMO0pKdqRbNiL9jOfV33NxHBMO73HO8vjucq024zoQjDtLsfzNqY9y3X3\neHvkW3nvLMrCGmPUNg/bmPY0F9hQeu9PGGPcrDbl+uS0txnRNW+AAFCLdjI+n5Yp73FyexQhNcps\nAo3uOzmNsU2cppeZ1U6Zdio1V4r2ncPtaZdSKuospWjfrufIZcvjj5o89Vfe9CkARQH571//saW/\nv74Zvmru+3lApr1ksBW2PZCDfB+PBbo+OjHR5PEDefmM6AeXoueobRq6MBbtxwjUPE1CXYwn4K0f\nChNStE+0op060ceML8x+mIvlWDGia5P4ppy7JdVaVFmvXvEGI7o2eXwkSNE+0Qti06iqV9EpqcQ3\nUnPhNSO6DYVp9zOiC1HE2VCyuRMXq2oDGeepuP67lpHVfnBQb3KwZKMofrTxsvPPqL6O5EBFu3Z/\n0FSHtj7s4kH1Y/Iqpz28e3zsZHeb23RSrvscDCOPFwJmmw0t2lsM1DIn016Pt/z9Xg8pqMG0x6q5\naNPmYXke3Uy7DCaPz7VUgxKRpxEdvYeLop30tC+xaKcsOi3UlyE798UsFyhP2zSOkJBCYylM+xr2\ntH/8sZ2lv6d+HFfNSIuOL4nVyDefc20v2utrNZQZXS8jOo1pHyPfVh99zvdRwFi0HyPQBb3QCrgY\n7Uz7Binak023Ed0EfGGGS+gLeo3FFh497SrD5WLa+/W0SyI7l1GsHMuk5Vg2y+PrRVXV096n6JQ6\n006KdqEa0SlMu+Ie39LTPoA8fpYLRCDKEBYpG0NWkHGemtTHLGTmqwtpWhft1TnV2PbJzZ9ffR1L\nPojckTrEC/3e8TlPStGezJl2qqzon9Nu9DeTYnuC5uOSGTntdiO6vvJ4oxhmURH7Vv6+bfPQ2dNu\nZslLiYU3QoxjqZmLNm6A5AKAxIQ5mHomEOoSdfW0R5497eUc+M3Ru3D+z/5nPC//QPW75RrROXra\nFab9cIt2Vz87sJzNBddn36rJu7vg8m66dKWAfhxXrUik5zOOamm7/jsXVCO64rkbCTWQDfP35o7P\nPS95vO4eT+Txq+bmP6LAuih9XBiL9uME6niOWOtVbe5pl1JiKusFS2K4x9evlaAH064v7iK9aG93\nj1eZdnfRHiqnHSw2Ni1cHxRpLhQTsniiFcS2ntc+TJKgTHus9oXzzB35RuXxRpa8mdMeOit5lnP/\nfvYSpGDZiurnHgzMtOdcIKdFe6me0Ir2iDDtCXJc3x+AbSfydskixYiRL8C06/L4aYCe9saineWN\nHgmpkdPuiHyr3OP7KH7INa3NQ1i0p53c7yeiemyLbszpaQG6e3zzBohmQsciZR4PKY/P9ban6j3a\n/24hJPYzjk3M8OvTlyH59F/hRx55UfX7VWDabz5FXNkPmWnXCw0KKukfqvfetTg+SkW7lBLTRF0e\n3//kctl2vUhfNXm8wrT3jXyLls+0+4xRWR9NVaZ9qKjbEf3g2twamfYRRw6KPF5boLVJunMhK/YK\nAJJN1YhOZ5r7LZbdvaS8K8M1ENNuFO0x3bTInRPEpd2ZUow2Me2lYiAk007POeMNTHvewLQraoAM\nUoafEA8yzSW8TRoPqEx7sjym/cYsV8bKYnvRDlK0T8AHyXOmsm0JPVu8u3t8FEFro+mvrDDN0yjT\n7r53gDJKzWGSSCPfKnn8YudeGN4aSdFeMkebPN7Z025h2oHFfSuE1HvaVX+Atg0Qda6dKJs8McTw\n7vEe8viDnENK4DZ2RftNMbZlFe0ZF1X0HGPAuZOUaa/nnos3DrenfU9rJaBYhjx+HZj23dRUFL33\nQf36Gxb6emzVmPZcYdoZImVPs6M8PjF72kN50fTpad/XjHqnY077ysN1m4xM+4gjB6mbp2mu0I2L\n5VxUEUUAlMUnAINp31mQaRc6c9QxammWOxguQNlYmCLrKY9Xo9TULGvTH+B9D13BWz/5JC7upFqf\ndkPRXhpVheppj7SedtEU+ebX0172F4f+AJvpjKq+uWEDZdrj+rn76bAfrtf3c7XPOnEU7edo0Z7j\nyl54pp3KtmWk3jvd5fGRKY8H7y1b5KLwvahftC542uahLBdaT7vdiG6zZ0+7EaXGYjU+sq1Nx6un\nvT7/i6p+jGJYybxvZspTLrS5aKJ5c4Rr4XC7x7f/3eU8fQp7ys/L9qFFP2u64gqRxp8/OVWMtxSm\n/ZDl8ap6Smfaqcv9MJJv10ZPqA2gZeCyZePlfQ9eXuoYTHn8ah0/RR7PNCM6H/d48vzJ/LknlNi3\nQDaatPIAACAASURBVPJ4Z49z++vrOe1j5Nvqw3Vej7KnBkVLk+iItYLUmHaNHW9aoBnFsC7r1jYA\nwjHtKlvIA/W0TxnHfrb4Yo81yuPVwuONH3kMP/I77wcAfNOX3o7P9yzapwHcpSnTzqIYjJFIKJEp\nBdj5/AngXW8CnvkNzUW7pThKc4GTHnW1L4w87o5M+0liRBdKZufC9YNMiyGbXwt6D/6pO6ovE3Al\nezoYaAuMVmh6ucfTol0mVnl8X6bduMfJeZsir1tLrl0APvF6PHzTP8THDm7Cf3vPLfPIN5+c9uL6\n7TMPRUydhxhNXmiJKsv0eWhiusdTxcCiG4hCcMSMzNs0EQQc+y097crGRJQY6qtg7vEOI7rYI/Kt\n3NC4iany5E3MkGKCG7NhTB11PKXEvamT3U1b9XG/vDsrNlNIEbNMNDHtm5MYpzYT7Bzk4ELiyl6q\nRNeFgDtiK+jbDIpLu+bGy70PLZdp19sVV00eT9c4NApN/50L1H3d6h4/MNPus4lENw5OaJFvY0+7\nH57cOcC5k1Nlw2NI9FFWHAWMTPsxgiSLdqEx7QkTyBpkxGkusEnk8VSKCsCUxx8stpAyjOiiGFR3\n5cO0O+WzIeXxUhtjrCoNyg/ca/tZVbADwBs+8ridla2+t+S09xgnzWkvelbr88Q0pv2//9ALgTf+\n78DvfheQEVarKZZuPsbQ/V2zXHMJ78i0nyT9wkNHvhVFu2WDIVWZQeUeYQJXLAvDvpBK0Z5ofdge\nx4H4LXBERU675mEw6+kRYEjPyVyipFi89nuAN/wkst/+Tvzo77wXv/KmTyEzctodTDsL4R6vbR4m\n9XGIWjZAMp7bNw8tnhVAj6JdMReNjXPVthE7MYp2VR4fipzIhUTMLEW7R097Gfd2HmpW9snKbHB4\no0lAj3tT58RpEuHMieLYC4lBWl98sa+xgzpOb9bXyBDHzlUMHaWc5Mu75vm7cGUfD13aXdoY9KJw\n1Zh2qtKII6Y6q3sUtHTztyyGl+oe7zFG3T2eRr4NYcC7bnjl2z6Dr3zxm/FNL3374ORJiXVoz2nC\nWLQfIxjyeMaKhV71e/dCdJbOMJ1LWoUWFwdAZWiYWJjhMmSUWi9puxEdt7s2A8aCto8RnSKPtxjR\nZfMJ4hff+AnjuWqBN1F/GZs97X2KTupyH2mFB0ReTaQbSHF+55PFzy89ADz+ofpxRk97/X3JtIeS\nspWY9WTaT5CCaXCmfT/X8uTn90aqLfAYAycRbNd3taI+BKgRXaQy7V6LZjJH5IjBGCtSB+bzRMQk\nuA9j34BcSCSa47lEsWhLmCheP58Bj90HAPh77FHcjGt412cumQoMJ9M+L9pTvpAE2FADsBiMbng1\nMMRCSDCy+SHjjTq2kqoKiLHnoqofs+WJbgzxRgmowbTHE6WdIoIIJinMHTntXeTx5zWmvfQEWJY8\nnrKvNnaa9osPZfLmg33ij6DL4wGVfd/roTZzwSlPPUKL5kuWoh0A/upjTyxtDEbk24oViQrTHjE1\nw9xjrLRoLp+7oeS0h/l7XZ97nXva9ci3FTsfq4bLuyle/BcfBwA88OQNvOOBi0t5X9d5XbVNr0UR\npGhnjP0iY+zNjLFHGGP7jLHLjLEPMMb+HWPsJsdzvoox9hfzx+4xxj7EGPvXjDHzU6Z+zrcyxt7K\nGLvGGLvBGHs3Y+z7QvwNxwK6ER0AQXOcGxbj2UFdYMxAFqElNHn8opFvXGdkuka+ZVztvY8HYtqF\nzrSrr825xOXdFK99z8PGUxuzxwNGvkkpESlGdBEY7Xkl8vibNBYLn/mbhjEStrA0ywu8+CuK9jBM\n+3Lk8ZZr7q7n1T879TQA6v02SNEuVaZdmU47u8fHtcSXbC6JvF9BIiwJEZzR10+BfVWKeie7hGv7\nGVIuNabdZURXj3ERTwjbGBm5BmPhVhJlQm0lYrSVKNkAyg0K5FUhu+g9zug5NZh201tDGScXqreA\nxYgunDx+8Z72/apotzPty5LHX9whcW9b5nxEndkPs6+dFhq6PB4ATm7Uc9AQTDu9Zqhx11FSE1Om\n/dRmfbyWWbTrhe+qbXrQYjiOmHKufZR3mY1pT8LL453tGl7yeDWJIY4Yyo9EIVfvnKwSXv13Dyrf\n3/fI1aW87xj55ocXAtgC8FcAXgrgdwHkAH4OwIcYY3fTBzPGvgPA2wB8DYA/AfBrAKYAfhnAa21v\nwBh7AYDXAfhSAL8D4JUAngbg1YyxlwT6O9YaUu/DBioGDQBk7pb0ZbO6wEiZpYAy5PGB8pGjWHVt\nbmELJckXF9FUkdbrvbnhinbTHyAXAo9e3bfmHKvFaJM8vr+ZFj2WjEVgRMYfEab9nMZiYZ+Y7mzf\n0jDGAA73Fswy3ry5YcN0u/pyW9Ys9+Dy+P0ME5s52j/6MeCmLwRO3gx89+8CACRhQq/v7ocfjMG0\nUz+IrkZ0cbVAode35P2kv1x3PGexunmYp8Ceavr0NHYR1/ayuRGdI6c9nqIsiKeMV++xyP1jFJks\nBiPHIJLuQrExwYIxe6zjwhtzmvFgrHtrNDPtijw+ThSmPaR7vMuIzkceXzHtMHvageW5x18kTDtl\n1UuoJm+HybSrfbg6tkgh3yv21AG6aN6I6Yb70Vk006L9u77i7oqjeO+DlxVDwiGhFx+r1kOtMu1R\n96KdGtFVPe31a8xWLPKtbDUZzejacWOW4zV60X7h2lLe2208uFr3z6IIZUR3Wkp5oP+QMfZiAD8F\n4N8C+F/mPzuNouDmAP6JlPLe+c9/FsBbAHwnY+y7pZSvJa/zdAAvAXAZwPOklA/Of/5/AXgvgB9n\njP2xlPKdgf6etYS0MO0ySlCtmxrYo1wp2i0FVKQV7b0KTXdPu2hhCyXJy5Z6oWfI43sY0RnyeOoP\nwJEL6czhbpTHW/rFFy06udRjqyaIyOtHsu5pv0ljsRR84Teo308s8vjQRbthKOhRtG/dXH25zetd\n3dDSfR07Bw55/IlzwAveW2zwzDdLJLlObuwNULQrXgtJkRhQwotp13vaS6adGBjyrOhLX9BoyyiI\nowQimqD8kcxTYF9VIdzJLs6Zdl2BoRXEk5NAVmzYbCLFLk7gxizHbV3HaNw7ica0N6iSuKx66o0x\nAsW1nO9XY9zH5uLFE92k0eXx4K3xeaoR3USZa0PmtPMeOe1lHJ4ujz/JZoDEwhvEXUF72m1MuyKP\n3zm8nnYaH2jraafs+26Pz0AXcp1pn5k/X3XQTZcvun0bz737LN7/8FUICbzlE0/in33FXYOPQT9e\nq+YJQOeGSOtp9+n3pkZ0pTz+xCpHvs03FKZxhNl87CkX1haU446Xvul+XNPWv/dduAopZdFyNyD6\nGA8eBQRh2m0F+xx/MP//C8nPvhPALQBeWxbs5DV+Zv7tj2qv84MANgD8almwz59zBcDPz7/9kYUG\nf5ygO0tDLSJkgzyetxbt9cTVh2m3MlwdmHYQpl02RJVNWI69jEMueiNrruwGw8UlrjuOgcrKNpi8\n9cxpF0J7ryhR2UKRVwWtzmJVuOsrgdNP08ZokccHLtpTXR6vb27YsHVr/WVey6uXYUS34eq/Z0x1\nkSfX4O4QRbt+jxMG2y+nXe9pL75mWvtHHxMeqzyeFJvgNqb9EvK527W66aUVT7a+9gU2EIUe+Rap\nTHtTT3sj066NcbOvUkVJBLHPQ03jVNQskcq0J0xYlUKLIHe6x3fpaXfJ45dlRNfc006Z9qcOkWnX\nJb06Tk7rOWEIpl3oRXv58yO0aKZM+/mtDXz9F99eff+mjy9HIq/fu9mKbXrQ8SURw0ZHpp2qgGoj\nuiEi3+yv47OJRO+P8l6a0Kz2wAa864C//OjjeOXbP2v8/OpehocvD9ASqGEdPDWaMLQR3bfN/yeu\nVvja+f9vtDz+bQD2AHwVY0pl2PScN2iPGeEALXirop35Fe2Uac8iS9GuycMX7Wk35fGqAzYXorHQ\nZjnZP9IXy1rhISWqHdOuYLp7vMFwCVx3OOhvNBWjASPfjIWy1pcbyRyz3G7yVOFZ32b+zGJE1ytL\n3oKFjOi2ahn/ibQu+kKrAHQ4jegsoIXfbDbAwl5omzSKSqWbPF5l2kkxyPJeWe2GVJrFEBGVx2dG\nT/tdrDCxuXBlX71/jDhC0tfOFpdPm0Z0kXLumqLK0rZ4TGXTq3jcwqqfxp725sx704hOjXyLIIJJ\nml1Mexh5/JJ62hX3eAvTTuXxh9jT3hT5pv9saKadsq9HadGsFu1T/KMvrFVcn3zc8VkZGHrxwVdM\nHk97wo2edo9N3ZSb18kQ7vF9mHZqhlduKCTRGPvmwo1Zjp/4w/uq77/unluVe+eDS+hrX/fIt6A5\n7YyxFwHYBnAGwPMA/EMUBfsvkId90fz/+/XnSylzxthnAXwJgM8H8HGP5zzGGNsFcBdj7KSUsnEr\nhzH2Psev7ml63jqAWZl20qvaII8XxAk7jzbNB+jy+FBMu9bTHkFCSCB2KWx4U9Fej3E6XyzupXwx\neRP5QGWOqCWXPH7i3dNemrzxhWRFQsBwh44StS+3Ytpd8viWor0cY3h5PO9uREfk8UrR3hBlGALX\nDzLc6Yoh06AqW8IXG7oxGSMKGPgU7WTTK5UTqzx+Ao4Z5wA81A8WCMERzbPFJRhYFEEyrWd+3+xp\nB4A059hIGjZzLK0bi5htGUVmlCCi8niZOe/JlOuxk03pCz2ZdtryFCWme3zD4nnGRSExLzHZMo3o\nAva0J5YC3ca+6yg3LfWNxa352A8ygYyLwXOAKdOuR74BRXFX4vKKRL7Z3ePrOWhv4Mg3xYjuCC2a\n9VaIm8gmzYUr+73ag3xhMu2rxerS85lEDBOaYe7DtJO5yZbTHkohp7vUlxsKbdcjF7JSDDCGSkkw\n9rS7cd8jVyuF6e2nN/FL//zL8f+840G8/VPF5/cHH7mK73jOnYOOYSzau+FFgNI++EYA3y+lfIr8\n7Mz8f5crQfnzsx2fszV/3PD6iyMKqbPD9H9A6Y/UwdNayptbmXa1p3vRnvY2pj2CLBjkyF5oM+ps\n3SSPr5zZc2Wx5Q0jp12LfONSiSKaxKzalW2U95LvT8yl7aUioOvmgsEWaoUHNaLTM5ABFI7n559h\n/nxiM9Iawj2+I9O+Xcvjp7NL1ddDSEApru9nqjlaw1hZQEM3K4wizi/SsXpMto9y6XWAKTGiU++d\nPky71LLFY6Doaa9+aMrj75wX7dQ4Teo9+0BAebyZ0840NREXspJ1UqS5UBMsDHl8QKWK4q0RGfNQ\nG9N+GiSWcPOMaUQXimnnEtNF5fFZYSp4jt1Qfn52kld+LLuzHGdPLjCPd8AljX3VcW6rPvZX9pbD\n/tuwrzDt5hJPiXwbYG60RXkBR1gevz3Fqc0Jzp6c4Ope4avx1I0ZbjttIS8CQi8yVq3ooHNLHDFM\n4/q68uppV4r2Yh4donVD91jwLdop07+ZxNUGLd2IGot2FfS+ec7dZ3H25BTPuftM9bNlOMhz7bos\nvz9KnhpNCLo1LaW8XUrJANwO4J+iYMs/wBj7+x1eplwFdTnC3s+RUn6F7R8AM1B7zWA3oqO9pO6F\nhiRFO7cVJUrkWx8jOk0yySKtaBeNfZox6WlnhizVLNoX/WBok8dzIRV5/D23nzbeuxiwtvijWeNR\n/fxFHbDVvtwEscIWkp52ZUHMgLOfB/zT37C/sDXyLbR7vMCUdWXaa3n89IAU7YdlRGcDPd8NypaF\nQWXbUYyoix8EAGQ1036Aac0ka0qSPj3tkjiv2xQ/yE2m/SzbxRb2lY0cZpuHiDz+RA93cS4t0ZM0\nfYK5peftTHu9sVCZTYYwotOZdo/It9OM7HFvnjaM6EK6x1vl8Yy3bgzspxxnccP4+Zm4vhaGzmqf\n5bxqpUoiZpWd002DqyvCtJ+Ymku8kxu0aA9/3FxM+1FZNO+nvDqGk5jh1Dwi7+5z9dxy4crw3JDO\nrDfdy4cBTj4DFot8I0z9fHNna4O2boQv2jc6XI8253gAqqJgxc7JYYPOe+fmG5vPvrPmXz+xhNYS\nZZPmiG4aNmEQPZmU8gkp5Z8A+AYANwH4f8mvS7b8jPHEAqe1x3V5ToMF9gjD8RxQmfYGJk4qTPsJ\n8wFksRgHlsfTTHgG2TjZRrRob+ppZ/0y0FmTER3jyHKuLCS/6PZT1ddTWzxYCSqdJY9bZJxCmmZa\nusS3jFVR5PHf9zrgX38IeMbX2F/YElk1jDy+I9N+8qb64QeXqyJhtgR5vFKkNY2VmtLl4RfMTKib\nNMr97cFqyqy+z2fUVkSLS/RZlDnfQ2PaAUDG2ubhvrkj/zR2Sb0mbJsjdNNr3i++UNFum4c8ndkz\nLirZOwCzp50y7aynUkVLCzDbdJoj305RYZqFaQ8ljy+Op/laCUTr4nkvza2eG6eT+p4bOvaNtlhs\nbybWtojzpGhfViyYDXuKeZbJtG9RefwQTLvLiO6IFO2XSLTf+a164/Kuc/W658KVAUxENehMcFN8\n42FAZ9ppMdudaS+uE5VpD3NP0zlwo8P1qLSZkOeN8ng3Lu/Wn3vnThafRTdvT6t5YC/lC0cY+4I7\n5p9VU6osikGbwKSUDwH4GIAvYYyVDaefnP//TP3xjLEEwDNQZLx/hvyq6Tl3oJDGX2jrZz/usOW0\nQ5GlNjDtWX1ohS1+S5NlplwsVCwJneGyyeMdE6UQEomsF0sG064VHsDifVMRYTTZfGOB9iszkSk9\n7ffQor2xp73+nkpsFxmnGZ83QTShRnS8jnyj+12kN9yKkD25Dhju8T6Rb/GkiFlDsblTGlf1kXL7\n4Pp+phYVJ887H0sl1oMw7VrRHsVUHu9xjmhPO/QM9AIT8IUNHAG1aK8VP+S9eGbI44FCIq8mCjQz\n7Zt95PFGa0mstgCBO42hCvd43572fjntqhGdKY9vcpwu5PFa0U42eQojusWGpSMXAhGzMO1zVVIT\ndlNujaQ81VOJ1AV0E3p7w95VePrEpNpfvn6QN/oJDAkXQ2j72RBGdNzBdB2VRbPuHF+CFu2PLMMF\nW5tfVo1pp8zlQu7x1Ihu3oe1Ra/NQH4L9H02SIthF6Z9U2HauxnuLQOPXdvHi1//Mbz+Q48d6jiu\n7JktRIwx1aRz4GQN16bhUZl/2jC0ezwAlHlR5R3wlvn/32h57NcAOAng76SU9Mw2PeebtMeMcIG6\nx0cmw8Wael5zKo+3Me1q5BuwWH6uLfJNl8e7br5ClkrksxNtnIoRXU9ZqtTGCNVkDCJX5PFfcOt2\ntaBrjDKz5LQDiy3qC3m8ugESxyrTXsq71KKzZqytsDDtw7jH+5m7KSCxbzexa/PXGo5pF0JiZ5Z7\nb3rQ6DQM0tOuyuMZo0V7+wJDZdrJWGn7C+vJtCuGmPP8+ljrad93FO2shWlX4tR6yOP1DS9dHo/c\nyXylvKWn3WLkGGQeMuTxeeNCJeUCp6g8fuO0GvkWOKfdZjpXJm00YT/lOGeJpNyO6vtn0bQSX+wQ\nh3pX0R5HDGdO1Mf/qsOIdGi0ucdvDRz5Rq8Zyr6GUm0MDepdcBPxLrj7PJXHD8+06/fFqrUXqEx7\ntIA8njDt8+eemIZv3aDX44ZSxDWPkUbO0ejEroZ7y8BP/ZcP45Vv/yye/3vvX8qGkgtUHk/bhWja\nxtBFu3Ce79W6fxZF76KdMXYPY+x2y88jxtiLAdyKoggvM3z+CMBFAN/NGHseefwmgH8///Y/aS/3\nKgAzAC9gjD2dPOccgJ+af/uKvn/L2kNqrAzgb1RFel2lvggFDHk8EChqSWPa4wY55SzT85GbjOhq\n9/iFQGSpUVlwaKoFKo8/vzXFraeK8TTKvmnkG5XHL1p4MK2nfWLGVsXgOEMNqU64mWIAmrx3SPf4\nBsM+F0hfe8nOhcp7teFGWkQHqpseDUV7MizTHmlyadZRHk/v85mTac97bYSoRbvDW8PBtG+0tUxQ\nIzrWL6e9TR7v3DxsY9rJGDdYP6adaZs0iGLIucVLzCTy3H2N2Zl2NfItpHu8Nafdg83fS3Mr075F\nivZF27F8QV//1Kbbv/fcCvS127KlKdTIt2Hl8RtJ/V5HZdF8+YbdcHDZ8nh9nbNq8niu5bR3lY1n\n2vMBYItsiA3R064yr83Pc6UwqH/nalzTf/3J2uv7DR85PLb98p4pjwfUtI2ndoadF9edaQ/hHv+N\nAP4jY+xtAD4N4BIKB/l/jMKI7nEA/6p8sJTyOmPsX6Eo3t/KGHstgMsAvh1FtNsfAfh9+gZSys8y\nxn4CwMsA3MsY+30AKYDvBHAXgF+SUr4zwN+y1jD6XQGwaGL/vf7cvKVo12SZwGLmQMKQdKtGdAzS\nKRObcV4tggG05rQDi+/mMsHrLa/ILDwYzxSm/fTmBE87ewJPXJ9pkW/unvYpkfovxLRLqbhtI4oR\nk02BchxncaOK4MKJc2rftQ02pj140S7aWVUbCMt9M4Zn2ssWiJLVL8Zwi+PRQESuwUZly4JgSttG\nAkbk8fCRx5M2mGyonnaq+Jkz7XT+iHhq5LQDRexbY0Y7oLnH9zOi0yPffDPQi5z2hp522gIzv38W\njjfSDTEZg2AJ4tLsr8FcdJbbjOjUnnYZtKfd/BsTcMwWZNrL8wsM39NOX9/FtAPAWbJQPSwH+f0W\nebziHj/AcVOYzcnRWzTTnnYa9XYXMaJ7ZAlGdKsujzfc4zvmtNPCvnzuENfmoky7a/Nr1XvabYkR\ny4LNiA5Qi/ahmXbVw4BsGh4RpU8bQpzdNwH4TQBfDeDLUUS17aLIVP9tAC+TUiq0iZTyTxlj/xjA\nTwP4ZwA2ATwA4MfmjzeOrpTy5YyxB1HEyv1LFCXTxwD8jJTyNQH+jvWHzYguVvuwXWBEHi8Tmzxe\njXwDFltIGYyMlWm3T5StTLsi8eVgEAsvlhkZY8VmkmMpRY7r+/Xff/rEBF9+11l84OGrzUZ0SnFE\n5PELjNPcAEmUnuoJK03oOkjjAXtP+xDu8W39yzaQ2Lebo+uAKHbDuZCIB8jV3TnIwSCq/nkAjceQ\nHv9Y5uHHpfe0UyWN9Fhg0J52pWhXC9Y+Pe2Mm0w7ZbE3+Y5VhXAHu9yuvlDk8cVjF9k8tLbpaPeO\nc/MwF82bhwkdYz8jOmbxKRFRgnherPMGs8OM24zo1Lk2VKHVxLT79LTbjOho0T54Tzst2jcnzsed\nWwEzujamnbKZgxvRHUH35ouEaafFxp1n6/v20av7g32mlDCZ9tU6fnq0Fj3XXj3tZB2XzIkPxSQx\n0JqCFtZdirgDL6b98It2fQxdo4FDgva007nw5lPLk8e7mPZV2/RaFL2LdinlRwA8f4HnvQPAN3d8\nzusAvK7re42YQ2oMNuBtRBeRxbyxCAXUon1eKAaRpbJYee2YueXxrVFLjBUL/Xkv8QR84UVLpLjH\nz8dHzLQYT7FzkOEnktfi66P34fTDL8H/9nX/Hc6cmOD8e4FqmIY8vh7zhDLtC7KFSk97PFEKnfI8\nqQVniwmdNsbieMvh5fE+RnSAwnLfFtWS2lnOB9mBvr6f4SxuIC6VCptnWyLfNMPGXFjZsEWhMO2x\nJo/3YtrrzbmUueXxvZh2MsbKB4K8/unskv4UAIVZYqs5ITWiY8XiYCHFj37vRJHRL+7aPLy8O1OZ\n9oac9o2eRo5MifgrjqWIJgCfn8cG34Q0FzjNtJx2akTHAva0c2GNfJsw7sV42eTxG8T2ZojebIod\nDyM6QGXarx4S0043gGw97UP0DVNQA76jyLRf3Kmvq1tO1XPM1kaCm7amuLSbIuMST+4c4I4zFgIj\nEHQjw1Ur2g33eIVpbx9rmtPIt2LzQ7k2AxnRuZj2tuPpUqxMk9WKfLt0Q53jh07LacIV4h5//uRh\nMe1007A+V0dl07ANyzCiG7EqsLAyTHE8b5DHc1K0k4VxhVjt9wRC9bTHxmu7dsyKXtKGxTJgFB8h\netorCTIZ58HsADfLK3h+8ud4ZvQ5JL//P+LcyQle+PXPxIbCtLuN6BKZAfOYpEX6u3JuMu1UDVDK\n45W4Nx+mPU6q6ydmRazcXhZ28ZfyRY3o6k2HW0nRPlRf+86B1m/b5ryvFX6h3WcjzZgsMtzAmz+4\nmMK024v2hPF+CwNlHoqM1z/N66L9CVlnvJ5n19WWCds1QSPf0CfyDc1Me4PJ28Ubqb97POlpX0SK\nzoQ2VwKQjPoDFK8vhMSjV9U+3CLyjfxsw4x8C1UnuJh2AMhbrqW9NMd5S5rrpqyv1dBKHx30Gmrq\naVdi3w6pp53OdTbWbejIN7VIoj3twd9qEDxFigrqeg3oDvLD9rUbkW8rdgAN93iFaW+/rg5yUxGy\nkUSVeiHlotfmcAnFY4FuIrUU3AdHJPLtqR21CA612dEVaS6qeTJi6jypFO0D97SPkW8j1geWBR41\nxqIxZjpiXn9Asakf0764LLUte7hJltpWtKuvtWgWKM1pj8oNEFIQR5LjFqZlTT/+YUBKRYJssIWM\nKex7xcQtGFul97TTorH8nVp0ehTtgCZDTsP3tGd6T7sv007k8YwW7cN8kO1nXHmfpn52AIbEOvSH\nvtLiot07TdniFTzk8dM+m10AhCKPn98zZB46w+tuqgdl7XF6FruKJLqVaS+L9kXnIT16UlNJ0GO5\nn3K84cOP4XNX93FxZ6YW7UZPe/391jy2jAu50MaS7mEAqCkWkueQUuK7X/kufNUvvAX/4S8+Xv0u\n5c1Me1G0D+seDwBZXhyrjAv8yQcu4N4HVRPC3ZTj89iTxvMmpGgfmmn3iXwD1D7Ow+hpz7moNgIj\npjKLJVSmfXk57W2KilUBLYJuNop26iA/bF+7zuKuWtFByROjp92j2J5Z5OeMMUUdEuK+dm4itcrj\niXv8Cke+6cz1EPe0D3Tn+Ii0jiyTaafX5eYRNMJsw1i0HyfYetq1mDIXYl7faEaUmvY6JYO7CMNl\n5COzyHB9z1xRSwbTbsuT78+06xJ+G9M+AVeZYgC4/y+BdLeWrCab9o0FS+zbIky7fQPENOM7Mk4O\n4QAAIABJREFU11UebxljeHm8UE3HerjHl683BPYz3i3jXtvcCl60K20bsXG+2xyIKdPuMqKbIO/n\n1k3mobKnnRpinuW1Cd1FeRpX5DYAIGISt9KNsJae9j5GdMU8pG14aceALgL+zX/5EH70d9+Pb3nZ\n2/Hgpd3mzUMyxu24fg9qXOkLJcbPVrSLFA9f3sN7PlsUwr/xts9U11yacZVp31Qj35riNbsiF9Iq\njweA/YNiPnz5Wx7AC3//PvyL33wX3v2ZQm3BhcQ0v4G7o6eM502I+msImTfFQkZ0h9DTrkh6JzEY\nM3uuT2ry+FBmgyUUeTwt2o/ImlnpaT+lzjF3nV+eg7y+ybEKrC4FV3rS1aLdRzauX6sl1L72/ve1\ny2OhbW5zjU+JfFuBc2Iw7QPPhS5ccTjHA8Ath9TTrigrRnn8iKMGpsVBAaoxFmtg2hOyQIqmW+YD\nqMHWfHG2s8AilAuYDJe2IcCFhJQSb/nEE3gvYWUK1+YGWSpguGAvspNrSPgrpl1d1CsLdwD41F8C\nu2TxuXULYFlU2Yr2RSZiISUS5VhOrG0MnY3oAMNMK3xOO9dc9n2Z9rpoPi9rR/ehmPaDjGvtBS1F\nu3aNZHnYD5JI62lvYocNSKkYTmaKPF5VqPRy67alWBAfgHOivqevylO4LE9V39/BSL97m3s8q+Xx\nbW0BOqxGdI4Nl0cu7+HPPvhoMd69DO9/+GrzPETGfSqu54hF5ssIlGmfy+NpikWeGYqnjz9WXK8s\n26/MKEW8UYyLMO3l3xiiqGti2vcOis+W1933aPXYf/Gb74IQEntpjmeyC/WDT99Zj0/Un0lDRJdR\n+Bbt5w5ZHq+Y0Dk8PCZxnaktZPgNTRfT3vUePAxwIXGZusdvqXPMbafqe3mZxUc5tlWC0dPe0YiO\nXqubpLhSIgkDSL0VN/FJh552Mr4NlxHdCuS0P7UiTLvLhA7Qmfah5fEklSD2b4c4KhiL9mMEpscD\nQXOzbmDaE1FPDNHUxrSbruSLTLjWfGS90OECf3jvBfzgq+/FP/+Nd+KvP1FIJ2c592Da6TgXY9q5\nzhpFJtOegKsLdwC4cC/w1Cfq712sLI19m/89i/flagWSxeW/U0929WSyscCyYSLfFslpJ+7x5+Q1\nlJ4AgzHtaQ95PDhSHva40XucRYnWDpE3O6jm9T2eSrVI1e/BnV5FO5XHl206tGivmfYr2MYVdqb6\n/nZGpNMtOe00x3u346aXaIt8Y3VO+++++2Hj+Y3eGmTDayuux3Vtf4FjStm4uGTa63FKkVWxhCXe\n91BxfCd5fd3y6XxjRPFAkBAyjNlSLmQdK6lh76C47nTj0j+773PYTzmeFZHje9fzqi8T0rK1VHl8\nQ0/7YRvRqeZZ7uWdWhiFZeb4gszmKuDyblr5OJw9OVE2HQA1Au7SwEoKfa5eBdMzCtU9Puoc+eby\nXji5EdYokR7HjQ6bSAc60y4lIMTK5bSbTPshFe3kfqBtQgBw5sSkUijcmOWDkSiAek7WMfJtLNqP\nEZjFPT6imdxNTDsp2uOpxYhOYaEWl8ebRnRaX+58sfznc1ZGSuAHXv1eSCkLeTxlt20yfrLQn4Av\nFqXmYtq1fvENvWiHBO57bf2ti5UlBVL59yxiLpILYfa0a0oDoMhpr3DivN+Laz3taR5OSgsUPe2b\ni7jHT7erImkDM2zP5b9D9rTfBL+MdgDGfZIGZtoVeXystUOwvNnMiLDsB5hCSTOiPe2snzzeNg9R\nlcoJ1AzqFbmN3aQ2o7sDpGhvYdrLfnGg+1xkMMOR3qZT5LTPco4/uPcR4/mbTT3t5PuTZIyLyOMV\nZUXFtKstT/rr3lsV7fV9L6bzjRHNiA7okSFPwIWolD069udFu+50/pK/vB87sxz3MFK0P+251Rgj\nkVWfNasijz9spp0u2E9O3OMc0oxOWTSTYmzV3M9toOy53s8OqMz7pWPOtNPxJAtEvrki1WjKSwim\nXZFLJ/7XIx3feXEZ+LWvBF7+XNySfa76+Wr2tK+ePJ4xptw7+kZDSCgeBkp6xWBvuVSMRftxAmFl\nGDOZ9iYjugmRIsYbtmKYFiPF+yyyg2/0YWuuzdP5Yln/UHjLJ560RL41M+3TBY3ouJCImI1ppyx2\nbjLtQNHXXsJV4CVmJFRXphAoTneTqV+54N1mWl+rDywS/hCL+xIpFzjFtAxpHzAGnHt69e2z5gv+\nQXvauygVNDVG6J44Ux5PW0ta5PFZfY/PMFX7YQPK4yUxogMr5fH2TZlr2MbB5Fz1vcK0txjRbZGs\n9K6bDFxokW9MNXFMUGwevvEjj+OyhXFr7GlPqISfFO373Yt2pqUFAFA2FxhPcU1n2h+8AiklNvK6\nLUZszO/7yCzaQ2x46e7xAvW1tTcrFnD6OD93dR8fffQ67qFM+23PVs5xmRAweOTbzI9pV4v2w2Xa\nNxuiJIc0o1PkyJTZPAJMl1q0m+ouhWkfWOar+4+0+ZEsG0ZOe2em3R6ptkWN6AL0tLsi37r0tH/1\nJ18MXLwfuPIgvvaRX69+vgrZ3yvDtO+5mXZgeVntmUsev2L3z6IYi/ZjBHWBN2crFPd4982+IWnR\nbulpj8xe6SBGdJG5WM65NGRrL3vzpzDLlhP5Zu13tbz2pt7TDihsplseXxcj0x45zvacdtM9fosw\nm5hu+704KTwqNUCgHd6cF6z9aRBn6w3PzQQAuLOW0P7/7L1nlCVJeSb8ROZ1dW/Zrq42Mz1Mj9Hg\nBjc4AcJKqwU5VszIrT4WCUkr+7HIIKSVQWdXq9XKLBIS8gYdpD3IgBCw4pOEEAgYjAABwwyIMYxv\nM22qusw1aeL7kTcz3oiMyJuRGVndRdd7zpy5VX1vVt40kfHEY94ne3cBaNDTPonkTICZoJ0oHVjY\nQBCdIo9X2eFCeTxh2nknJcGTUsIg6zDtnAbRZeOQ3v6wzucR9ETOwhXU0z6r5Ru5/2zl/Hl5vH7x\n8JP3ndd8GqU97dI+VjimrAzTrsjuT14Y4aH1IboRUdik95cSRAc4SnCO5OMZkbZ0o9EYccxzoB0A\nPnzXGTyGESXD4ccDHQrak8lf0xPVLaJWWCjdp33iPORtVg0lpt0M2gcN9moPDSBprzHtawv5+cPq\nYBfl8WrLt0vs+NH9afn2TPvQ0FKt33XNtOs97bNBu/jcsVP/lL2+7vz7s9eXQhDdJcO0b5s97cDu\n+dqjyMS0X1r3T9XaB+2XU0kT+ryk2+eR8cLucTGh9+cW8m/QyOOrM+3FnvYojnOSz08/uIE7TlxQ\nJssaJk6a1I+rBdHppLNArp2almmnZWTaNenxVRZAdOnxiscZUJj2blnQLvYx/Z6jiZsHWMqKL0gK\ngGXDuzV17KnZy6ZB+zCIcNBKHi8vQLnoQyttnt7jijy+hdDYeQGAxLQP0YEvMe3uPO0ezc7QZGvQ\n2kYPnIQj9ma1AdQE0QFVmHYU23QQIYxinDFM3HtFi4dkH+kiYzV5vLJIA0j7iWii3e4n7juPHpHH\nZ0oWDdPuQkGjMu2RJ66n0XiCrUmo7Ql/792fy8aBTW8RWDgiM+1sl0B7Saa91/YzmX8Y83qBjRVK\nDqK7OEw7XRikUucyvbsvdtEe0jqmXW7pN2kUCKhWpiqs7iiIKo0rZaqIaS8DZk0t1ehikwsAWjVj\nwTRn2OmIZ/ylANovHabdLI8Hdq/tm8kOsQ/a92vPlS49Pi+f1Q9CFLQvL2t8z9KENpXHO2KxFXY4\niLhWSvqv95+f3aedgNI+RpUZbE/LtMvp0rNB+2ymPf0+VeTxicRX9bTnFRHztO1TWUZbI+F30Z4F\nSEE7xwKoPL4q03432ab7GgaxIo+fFUQnXyOuPXEUxHleW9t5wVg5T7teHt9BiK1x9Ykg17Se9Np6\npn3EO/DmDcdUF0QnhTgS0G7raVdbvuXk8QnTfsbgzSvbp71L+s6rjHiZYuoiDSDtJ4tDLYP9ifvO\noxdrQDsTUwKPcTDEjjztCtPui2MwHg2xYZCSD9b/LXv9UOfaxP5CQHs/Y9ob9rSX7NMOyCzTbofR\n7Rgkx2o16WmnYwxl9F0vUDZRj8zwtLd9L1NTcN5sboE6VtvK489vT/CVv/p+3PTf/gF/86mHZn/A\nsuj++MwuPZ5zrjDtRB7vnGk3ZSwU76MA7fJ5GHbFvO1ietr/5NZ78b1v/jguKAvSTVuFTFWUHg8o\noL1BT3tosOfsB9Ht156rnHQWkCb1PmSm/W9vO4H//Q9fwEPntjLvYMwZVpc0rKejILq8PF5tW5UA\nD52U9I4TF2bL40m7ugFG1Vqp5Vi4tOe9K6ZdA4grBdGpoF1Nj4/BEFeTx7fpPibf09XkbxxG6GMs\n2tW1+zJ7OKsOPS6b2F/JzmIN5zFuiGmfjMdYZomMn8MD5laKP+DlF6BcVRzzrCMAkGfaO2yGh54w\n7SN05G6EEmNfTx7PNL3FTfL4ETpoLRzS/ht0n6GgnZMgOsv9zXex8LTp8SaJbK/Q0y5+bsfi87aM\nGFcWD5mmiwWL8unxAPCv96+jEwhbR2swHdMZy4XRNeJpbwngPRkPpYWFI4vi+NB2b6fmrkte7LI8\nPoq51FJuYGillhaVyOvyDpqsEWXaC+TxVILsXh4vzjP9O00tnLosCibWNKAdUCTyDcp8c/J4y2fF\nR+45i4fWhwhjjv/ylk/h1IXR7A9ZlMq00/7lYcwL09nptdBpefBI6mnfsXWDyqVlpl28Z2sc4mNf\nPJc8H7fPAn/3U3jB+tsAcByh4aeQu3NcLKb9gXM7eN07bsff3X4q929VCB4XNdPTPr87nvaqGQZ7\npYqfPvv1JVVMI49XW7WlIOIT953HD/zZJwEAd3zxAfzB9D1D1sPA16z1UEYvbflWiR1Ww9NkeXxr\n6gPWTXCDiKPrzwLtApQO2Ajbkwicczl0a9Y+GtLj1ZC3XJ92tSw87ZWC6DhXet63AcYQopUtrCxg\nR4Bjv6MHQtp9JKB9+j1HjibOkzCW/exlQ+jS8lvA0ScD998KAHiKdxfG4bOd7Jta7bF4oAfdZXQ8\n80Q52bd8+0JXpQIjeHl5fGmmnatMu5xCvzUOre+brDStJ02gfYgO5pYP67ejZdpJdwguJhG2cv5I\nbVGmLHil6fE0QXql384kgt0iTztZ8Gpz8XlbT3vM5cXDdCGWWg0YD3JMDADc9tAGvqYl7rF2nyzE\nej4wbUXoCrRHcayAdmERCCYjCbQfP9hHy2d48PwQB5mwnmz1jkx3Vny2z8bAtNd4FHP4XoXrcUbR\nsXe+25IAhq4uZoI8BTlqGj8tKkF2wWamFcdcsjnQhYM9x7Qv6Mek1UEXdz+S3DvJ/a+xCzooFaSH\nMbcac1XS5OfecTt++/95quHd9iWlx/sMjCUS+fQ8T6IYPcPzMNdOjdSg61YFEhg97cnv45jjZb/1\nIXzh1Ba+6rGH8PtH3wn24d/EdwH4O7YmLYQDQCcS42bguPtL2Xrg3I7x3y6WPH5dksfn7521hd3x\ntNP7prMvj9+vvVwywzU99b6eaf/Zv/ls9vvb7nk4ez32NMnxgARY/UweXwG0R5EAkcBUlir2sYMI\n2+PQyFDO7NNOQTtGORalTMUl+rS3WQ2mnYCRlLEbBfYt1fLy+OQ4hkwcz5QlBgB0LSYfBIj0nDPt\nMRZpcrxNCF1axNf+FO+uxjztHQLaw7kSPe59WZHiciIbc50dQgaahcx+IED7eIY8Poh4deZM42n3\nDenxQ97FYMUA2nVAnwA6CoitmXbdwpyy4DIKogykMwbceKVYXCr0tBPA2qJMu2V6fBjH+QVOQGba\nDfJ4AIr9hCyMKWF0QwdZFaE6ZhKJezAeShO+5bkOnnQsWURYZkLCH3anKpa2UEstt8TnXHavoGUj\njQcubq92Gp5VzLQ3E0QnhZN5TGK69gTTTsDE2rxm0R+716tdJ9+2mQOoaf3v/uxJfMIQnFmlQolp\nT86zFEZXsCBN79WrWueBW38DOHU7APm6dTGnoMeMbjt9Fj60PsQXTiXjzHs+dxon7/pk9p7HePfj\nOibmvwDQCQlov0hM+7jg7+5MiajdrnNSEF1eGblMgHxTOQuAOQgz5rgox8V17YP2y6gYdEy77LEN\n4xjDSYTbHxY+XRpUFvia5HggJ/sFMJ3Y2wJiMRhxsFx/5BZCnCuYCNl42tPvtW7JhuR997ogOkPL\nN1qmPu0EwCy0xPGzZdtj1Zc7PdcRAe0rIMnnZaXxgMHT7gi0B7EZUJQt4mt/ErtbCr1xWXMTAdrj\nMqBduU9ceuJmhzhGxX3aJXl82yiPb9ewvwAAizWe9gKmfWn1iH5DOqad7KfPwwwo2nrwtXkQylhJ\nA4AO9Du46kAKRPmMlm9iv71IHHPbiUzeppMy7eIYeLEsj6fS80VTS0UljM6Vp13qZEEWVyKFaV+a\na+OJx5L9oeMT763kPrvUEtdgU772siF0aVHQblowaapoC9NeEWhvKIhOZV+lNmB7ALTTe9rItEtt\n3xr05moWWG0S5Ceaz99x4oLmndVK7dMOoPT5ps/j10VvBP7+p4E3fyMQjjHoUhVI/XuaHjO66JYu\n5Kv36LnTAqQfZBu4lp2Q/r1NAjyDi8TeFqkao5jvutc+jrn0/Fqay4P2RTJ2VmlvWrbUrgZUGPWl\nwLbvg/bLqGjLN+Fpl0FEFHP8/R0nwRDjf7d/C+/uvBbP9m7P3hMThkQqMpGa98RE1FZ6F5MJfZyC\nYcXTfr5gdbuw1RIgedrTECPbiVVOHq9huFqI0CXp1TlWvbOQD6jS7PeCL46Hra89inm2gEL3MyKu\nmBVG2z5ZMO3tPNPuSh4/DiMFUFRg2tcenb08xNatF4/Klh9SGX+J4yepMdzL4/MZBvk8CPMGZHm8\nJDXWgfaKvnZdtoYJtI/RwdrSvL57gI6dZ0wJo0vu7SpBdF5BtkYHIU5uiHFudb6DK5eTMVBS+/gd\nyL3zII2VfjRCGnRkO5Ex2XSoPN6LQ2ky9azrRBK/tDBG1Swq0+7C0x7Fkt2AEV96GMigfbnfxk1X\nJwCdjk+Pu+548oJ8drElxtgquR9latOSaV/sieO/2SCjpCsKwAvl8Q0F0VEpcsvzFKb90k6Pj2KO\nc9sChK8OTJ528ftmmfZ6oD3QgOZhQ6qKVJVFmfaiZxsNS3t6/OnkxdYp4NRnnV6bccyRkquMydL7\ndFxTx90lLiw5B7GRY9pb4TbSMVt3jHejZo3JTY2FptqehNlxHnR8tDQW2kUC5HWWLVdFiYmW50kB\niS4zhC5W7YP2y6j0nnZZHh9GHG/75EN4qXcrXuZ/EI/1HsB/b79JbMTExs6JSfUSmQxar5RG4v08\nZYQV0H6OMONqwqs0YdaBYrL/aWq6LWjPhVSlk1ylnZe0gLB6vbyRon7eBIwMajDtWo8zgIic8yVU\nBO0aT7srpmscOmDaCQhZYDuNMe08FOfYM0i8pVIWdlw+9LUt/pR7p5AZUILoJHm84ucGajDtGk+7\nrud6zBliv4PFuZb+ftH1aQe0LROt/eI6NY0SxkeDnQ7OdwlopwuHGjuRJyfRd6bH03YftWoAKJ52\npU/7s64VoN3MtItpgY/YyWIcj8Q2ODz4HTF+xJMR1ofimC3OtfG0q1fwzU87hoO+2Mcbrrk6eUHk\n8YueGLub8nLS63yhBNO+W5NTXZVVBTTVp52GfqltwC51pv38ziTz4y/NtaV9p7UqBWrtrjy+UCml\nlI5tbVJVAQDtlnhmFDLtpgWcOJKY9rrXpmrXoOqToYFpX4VQI6yxC7jek5l2jwsV5cWSx6ugXWW2\nXakeyxYd5xZ6eZYdkBczd4tp32tjUJnaB+2XUXlUHp96XZWWb6MgwofuOoPn+Z/RbsPvGUA7mfQt\nkBAx24k95wS0Qyc7j3COPCivPzQgMl4uT5h18lkC2vssmXRXYrhYMdPeQSj7Wg9cJ2+kqDUYBe0+\nkX9aqxaUSf10/2JmYNovFXm8yrRX8bQTdn4Bw8ZYHh6J6803KSdoNSmP54oEWZXHsygL39FWruUb\n+Tcphb4ayEyLyuPFOJR/0A/RweqglwQv6awkpkUSApTTyZU1067rEKF0Xjh9QRyv1fkurlyZm/7N\nGbkagDYTwl4er8/WYES1wOMgm+D5HsPTjovuBoslPO0+uJM8iDimi7G+BNp5OJbG4KW5Nhhj+KVb\nnoRjXdKOcm7aapSqunxx/w0dtZxUy9bTvluTU11tlpg8A80x7XTC3PaZ1Cf5Uve0n75AkuMXzAuw\nlGmnzLzrqiuP1zPtzYD2VJXVKdn2LV0IZFDeE00w1yYt32rur7qPtA1ieiwoaJ/DCHNEIXmMPYIr\n2JncdhemhM/FavlGz+PLv/xqfOQnvxLXrg3Iv+/uYiFVFC3O6cdIuuB5YRQ05i+PcmPQ3lH7lKl9\n0H4ZlSSPZ3qm/cTGCGHM5d7dpFpzBgDV7meApItJBp5tmXbKyMQsz8K1WSgl8q4OutlDtIUIfirB\n9FrSgkRWXTmIDqggj89NlvPAI8+0q6C9iGkXE9qBJ45f7V7Tmadd7Kcsj68G2l3L42unxwNAe5Bk\nIiBJmJ5MGmJEQjFp8w29xqVSmXaHci3tdaky7YVBdGrLtxny+Mp+w7ynXceaD9EVrFZVpn2qArGV\n8mul54xJeRBnN8Q1ujro4NgUtEu2GNNCDvl93xNhkzaTivwiTT49PgrE9bnYa+H46iBjWRdMFhTJ\n0x45kcdzqqDyPLTI9+fBWA6iSz3hcQSMhFQ1GweIxWnBI/L4xph2sW+lQPucPDndzaJ/r0gV0FwQ\nHendPW0Dlg4jYcwvaU8pVc7Q7Ae1ZE/77vVpB+zavulY4MaY9szTXm6RJmXaJWUkAEy2nTLtql2j\nP4NpX2Uk4wfAY737tdtN85AuFtNOF1IXei3MdXypFaXLjhBliqq5TIuFvbafAegg4o0Fh9Jz4nt7\na+GwTO2D9suoPCqP9/Ut305sJIOR1LubVG9gAO2MSeAqZXGsmfZIZmQAKInbkQTaF+daOLqUPGBn\n9mgHcn3aAfuE31wAlE4ez0L0WFXQLkBHn3rabYPoYp6130v2L9lPyrRXl8fnJchu0+PJolEV0O55\nCNtiEYJNNgveXK3imIPF4tqxZ9rdpsdruwUooL1w0pdr+Ub+TdkOYB/ullVJpn2EjrC/9Fdz/16G\nxe5W9LTzXG5FPsTx7KYA7WsLXRxdmsPNNx3DAlloM45DRA2w2hV/x0a9EMeq4ifNB8iHggKJbNvz\nGB57dBEAxwq992lmgLSQ68bTHivhg/Re8fhE6tubST2H60i9o+gtieeA1PJtN0A76dFuzbRfPHn8\nYhFoJ4yjywk+HV9anpe0ASvJvl7sOklA++Ei0D7YrfR4HdNuI4/Pf74pVYWvCaIr9rQn/zYHRakw\n3pRVIDWvTdWuMYtpP4ByQX0pqXWxPNIjTZeIuYbCJcsUVRQVjTuSdaihsVFm2j1ZHn+RFllc1j5o\nv4xKDqLLS7p9RHhoPXlwDZieae8NCgAU8bUvTluJ2U4IOFlY4BoWroNQai2x0GtnD1g5hM4woe8I\nYDpg1Zj2MI4VRjPfPq+NSF5EWLpKAmyF8ngi658jAMBWKpZPE0/2j5NJuSyPtwmiIxLk6cTZZXq8\nzLRXkMcDiNri+3gNgPZRGGVBZ4AsSzaWLwMqlyv1kQ7EKT7swkmfkh5v7NM+BYJVg+g8XbaGDrTz\njmC1DlyT35ARtNMFpeTarOIX10nP6YJXungBiIn8r37zk/A33yc6F5hBu9jH1Z6YZNjsZ05Jkybx\nK+c8rRQMv/jGI1jAUCTctweyyoYG0TE3fdp5TMd1D6wtL6w8dF48b5bnpvs/JO2pUmk8ILWLG0ig\nvXl5fBlPuyoD3c2SQ/PKyeObkkynPucq8tRz2xO8/A8/im/7vY80mtBOS2Lalwrk8SRH50yT6fFa\nT3s9pt2lhSRSVBUA0PFLetqnY8qc2mFnvKkw7fWuzVktCNXU88csljufF5tppwupKVjvN5RTUaY2\nx1ThYx53FndhbMx52vfIomHZ2gftl1FR/5BIjxc3egsRTqwng9Eq9CDHK5JQE0Z0CSlorxNEpw94\no6ubiz0T027oJ69h2q2D6AypzYUt39p9YPEK8XNJT/scaR21Y3ks82niqaedyOPpebaSx+eZdpfp\n8bJ0V5McXqIisgjhNwDah5MIXQLcjHJtWr6sxmicaVcCz8oy7WN04M1Ij990EkSXMu06eXwHa+kE\n+aZXAI/9evH+/kHg0OP0f6DtwNPOFZUKS0G7nsWmgZjtmEz8TKCdgNaVjtiOjQda2+IP8uJRm1Hm\nNdn373zONfjDWx4lPjevjEVKEJ0LUBdLoL2VW4h9mCTxC6ZdtFNEn4B2MobPMXGsLx15/MX0tJeT\nx1MpbXWbS7707KuYY5Qd7/7y4w/gA3eewYfvOYtX//mnnO1fUZ0qybQvz7UzFdLmyO0YTqu2p71h\nebwKiAGUZjVT0EnvXwA5pt02fFctdRGJMSb1ah+FETYI4/uMQ+XOZaokvVhMOwXtabjeoKGcijJF\nWXOTpz35t+bHRjk9/ksviG72E2i/vmRKy3ApwVgnNkbwEOMwO6d+PKlC0K4w7bxmEF3GYOeludnf\nmYYWAWqPdsNKOfW0V2TatSFVQI7VlOTxrS6weCWwfl/ys6lHOyD7xcl3smXaTX3aY8K0L1dt+aYJ\n+3Iqjze1o7IoTkB7K9gqeGe1GgaRfD2WSo+Xwa9Tpj13vv2ch748015CHu+k5ZvZ0y7L4w8A3/Kn\nicf54U8Ba4+RwLlUqqd9Og5xzmWffkHFUto5A5uORbHUq50w7cTnipBYi4xtHcW+H2gT0G7BPuT7\ntKc9703y+GTffY/hGQfJuZs/LG9YCqJzI4+ni7HwfOkcdZj8nZf5BvDpdwABDaETAXr0vFN5rUvG\nmJZtn3ZJHr/b6fElVQErA7GPZx2GqYWxPGEGVKa93Hj3rs+IxO4P3JkPAmuiaAvHItDfkl/6AAAg\nAElEQVTueQwHBt2MZT+3PcGRpRLWKMuqLY/XHOumPO3+dHws2/ItZdp7Gqa9r8i8bcZttULF0w4k\nzHQ6pu1MImn+9/ilAHjAsLHWXLawLeTxF8nTTs6jTh7f1FhoKnmxsIhpp2NjM6BdXqiR5fH7nvb9\n2lNF5fFe5mmXg+geXh/iIDbQYYabnrAcudJ42usE0enk8W1lv5baIZ6ycyvWcL6yp712EJ3Ge5/I\n4ynTPgdc+3zx/queYf4DNKSK05739gnYupZQMVmoWUbV9HiNp92RPH5rHCpMewVPOwBOwH47dA/a\nR0EsAw6NxDtXijxe5zusWkm3gIIgOhYWMzU5T7uBaZ/eg9VbvhHFT1F6PO/KYBhIroVrnw8sHM69\nPyty789Puy9EsV3wDSeTPU5AbOzJxzMtqfUkBe0lmPZlCtotfH5hHGvtL0xZqEmLTpiwdUq8VlU/\nRH2V9GmvP9Hhkqfd044fQBKNsvA33wn89fcC73q1+AyVx5MFDzrGNsUu2fZpl+Txw+ZSktWKYp4t\n7DImM29qHVroZQFxpzfHzsCH6mkHqoH2JxyTx3yXagBTnSLp8UVBdIDsa29KIq9r72Yjj9cx3Y1Z\nIaYLNO2y6fGZPF45dpOtxIc83U4U81pAi+7jN8TvAX7r2biF/VP2u6EC2lewAWMduTF7eSnK42kb\nx7oKBduii5OLRaB9FzztQWS2RHwpMO37oP0yKa6wcEyTeN5GhIc3hriCnTVvqMj3THu1Tz3tW7aD\nR6wD7Wam/dn/+uN49r/8v3hX96ewQpM/jZ52mh4/BMDry+M1qoUkPV5ZRHjOq4Gb/xD47vcAK1eb\n/8D8kezlYihYBtuBODakS3MK2qsy7YTtStOyXcnjN4aB0o6qGtNOP9dtBLRHEpOpbTGo1q4y7a3c\n3yuc9BGmfaj2adelx1dm2sXnWEF6/AgdyT9ausi9v9wWx9dqf0mLsph55LW8MAcAx9kJHIxOi8+W\nAu1i8XC5LcaJTRum3eBp150rQOnlu0X2dwbT7sbTTo4986V7hYL2R3WHYA98OL8BKo+nfni+G0F0\ndp52mpIcxlwKjWqypNZ0nZZsb1Gq0/KyhSbOE+DuonSe9k4FTztN+QaAT9533vBOd1VWHg/ILeEe\ncXTs1NIz7TbyeF0QnUtPe3EQXRHYFvJ4lWlPguBod4M6Cw3p8WKI8drJG4HTt+O/hm/MrKKjIJJk\n2v1w3byxwwS0p0z7RQKBEmjPmPaLJ48v0/IN2B1PuzoGyZaN/ZZv+7VHSvU/Cnm8zLSPgljblzKr\nppl2KYhO30otred4t2HtoX8EABxm63iud5vYkGmy7LezCaPPOLoIKjHtWk+74lfuqUx7uwc84Rbg\nypuK/8DSsezl4vhk9to2STXZz3yfdnrOpZT2iunxvYxpdzMh2BgGcp/2ikw7I6C9EzUjj6dBdOU8\n7TKgcrnyG0YzmHZE5Zl2dCApEomKpFPb065h2j0N046uxGiVLsLELrbEPtrsr9zFQjwmaYhjGyG+\nwrsN7+v+KObe+GTgxGeSfxiTa80k4e+IMLVFX1xDNhMZk5IGyj5mf4eC9u0C0C7lnDgC7VRB5flA\nS/a0p/X0rr7FkimIrkOUSE31Jt4qGe5GS2KUdimMrmy7t7SOEkn3yQ198KxtqS3fAFRiulSW+G2f\nfBB/cuu9eHjdzX6qNQ6jLAneY8BBVeGjFAX1NHXeZekWWCMLeXzTfdpnedrTRYNJGOfa16ULWbr0\neEBWidRhjdNj2FHInvmpynIYyEx7d2KyhS4Cy1dlP6ZKQJdKOZui51F42i9eEF2Zlm+AvHDcmKdd\nsejsB9Ht156siOvTkKUJ3lT2erSIaS/paV+qmB7PdJNlJUwLmK6ctt4iffb53qfFD0Xtt8h3mMfQ\nHrTnjmXqvRcP+jlMiJSflQN0aS0cTSSkAOYmZzNgaPvw0gaTQZb4SmUlj89LVG2Po6k2dgIsoD5o\n9yjTHm0XvLNaDScKaLdNj2du0+PzzKuXC0fMJJfBSAaX6e+mNeKdbNKd7Lc7pl1uPVnQ8o13cKAS\naBcLSotEem6zv7Fu8RDyvdNChP/V/r30XcA7X5W8PHun2NAyCXyj1aagXSzu2UgGTQn3pkVOqRUP\nlcfngugUebyDiT5djM0x7cRi8iT/Hv0GqKedLMi2uZj022Z+lC3J015CHg8ojNIuhdHJioDZiwtU\nAn5iww3wpECzncnj7fskq5Prt3/qYbzuHbfj+//skw72Ml+ULT8430XLL54a0wUPV8dOLV2fdpvg\nM20QncPe2HT/vCw9XgZIdzx8Ac/8hffgmb/wj/j8SdFObVjgaQfctS9LAZyq0FyaKgx3JjLT3h4Z\nQPvy1VK2zsLF9rRr5PEXteUbZdrLtnxrKO8jVLIW9j3t+7UnKwktIgO+jh2eTvCurCqPl5j2qTze\nko2T+vlqQqrSwfervY/jid4Xpc8+3rtP/GBi2gFJLdBnI2wMA8QWsrM45vpjSRZAUvlUstNzgE2Q\nit9KgPu00lBAa097FMFndD/ziyBSVU2Pn066z2xOnPg3t3d2MtkcZ74EcGzKnxPX41zcAGgPIjn7\nwVIe33Etj9d62hV5fMyBCyeA1z8e+OXrgPs/It4fyEz7THm8g/R4b0Z6fDXQTj3tBLTb7K+UrUGZ\ndlm5II2VD09Trk9/Tvzu0GP12ycLZAueAAxWQXQG+4u0UEOuT4lp33pEvN6NIDppXPekc0QXvh7D\nDaBdkseLBcN2LCb9TYUvbVq2fEvet/tMu+S9t2ba3QDPWZLpqkx7Wp9+oEC+XKPkdm+zQ+UOk/ec\nagi060LndEDeVLpj6DQ9XknpBtRzHeEdn34Y53cCnNka48W/9oHseTcyyuMTMC35s2vkGaTHS2Xa\n0+5G57YnGcjrtT14OwaV6crV0vz2kvK0t9OWb820cSxTF0blFgylILrG0uNVT7v9ouGlXPug/TKp\niKs9nPXyeGAG014kj9d42q0HXJoeX7Cw8BXeZ4u3U5TkTRYe5jEC53bS2Sjm8HTHkuznPJWdFy0g\nmGrxyuxlCgxsH7jURxoxP1s44KbANCt5PEm4n066h0HkhO0Kd0QYTNRdtFvwIOX3mwXto5ry+A4C\np/K6pE+76mknKhUWIQxj4AO/AuycSbzXb/5G8f5QBe2QtzWtFovhIa4M2j2dpFtz7CK/l0n/rIrc\n+/Mt8bdseqDTe4cG0XFFuaB8Kvnf6c+LX62ZQDvtNS5Au1Wf9lgd0/MLIEZ5vMS0m+XxnjNPO1FX\nMLM8/rrgTmjLwLT7sQBMjfVpr8K070Lgklpl272ldWRJLH64YouDmX3aqzHttGyAa9k6uSHuwVl+\ndgA4SlUKDcjj45hD9zVtQKLuvTqpetWim/E1THsQcdz9iKzm+t333w2gIIhuyrT3HfmzU0CeZ9qT\n+QC97hd7bWDbMPddvlqaH13s9HgdaKf97ZtSHZlqkwDwpcKWbxfZ074P2vdrr1SsSro17HAarFQY\nRFeyT3tVT7vEyOiC6KaJzZLnWVdlmfapt8lm1a9Mn/ajPfK9Tb7WoloSoP0okvNRr+c9GUiN8nib\nIDpxfOlq+RkHoTzxiLApFdu9ATLTPsCONo23Tg0nass3S3k8QkxKBjOVqXzf7hbAWLJgM604CoAz\nXxDvCch9ROTxY3TkNjuM5cCgDcCk5UldLFJ22EcMeXGGVblvAOl+W/DEfW2zyMB1gZiQPe0tFmGL\nK+PMeBPYmPqyvRawer1hHwVo75OJq43FJBc8qOliYUyP3yZMu5oerzLtLiaAdFz3W0oQXTJ+rGID\nB8LT6ieTMjDtftRsn3bOuXTdDKrI43eJabeVxzfDtOc97VUmzUVgqAmgdFIKoZutmDrSMNNuyh6x\nYtoNx9rV4paunVpb6dOuZiG+4R/vwvrOhHja9fJ4CkDr3NcZ0660lUyZdqqwWOvFQGBY3F85LoH2\ndP4ZRPyiAPfhRPzNXmdqpSQL3E3le5iqGtPelDxeHoP20+P3a09WbPA3UxCRTviL0+OLgug06fF1\nQLuhlzygyM91vtEifzb1tFfo1W7u0y4makd7JdrPFRUJo7vCAdNOU6/pfkplJY/Xy1sfcdD+ho2E\n980j6g3r7SgetJHjATsJoiPXd4UgOhuP4qzSpscDiJm4f+IoyDOraeVavin/ruz71rgaGJH7tE+v\nRcYQQb4uvW41WwRl2vu+OD9bNuBJTTufFlfyNbiy0CDZDVavNy/kkHG0BzFxtBkv49wijXm8BEgI\nUBwr6fGH5A0Tpt1nMUZhXNv2kkuPp33ap/fQExS7k1Qmpj0iQXQO/bppjQLBTHZbsj+yqBZ3IXBJ\nrQuWremOSL5sR0F0M1u+lTtHdHL98//hxkpsvU2dpvL4Ekx7E8eOlqkfu52nXf9eV9JprRXCl8+T\nCswmUYy7Tm9l+9BjeqZdTkJ3H0SXzk/pYtVVXbKArVryVmSmfdGr3orXRY20TDsJ77PMkqpbsqe9\nZMu3hhYz6YJXe9/Tvl97tfKhRam/WU4abiHEGivoVVkEhjWedusgOq5huDRyT0l+fsNLlH1cAG56\nufmPaJh2W4ZL36edTNDHtP1cFXl8HrTbLoDIoJ0CDw2Q8Fp2++l3gClgaSPMjkddpn0UROjFQlLH\n5qqF0AGQWr4tsB2MHU/qK6XHe7JcPXDMtLc0IC4m9zgPx3n1ws40fIcG0aEte9qBnE1lFMTWLIMK\nND2ygBQxGWi0OlVBO1WBENBuc/9I45De0z7AEAtMmbB//v+K1yY/OyCNQd2KEu+8sqK45duhlEEc\nrQPx9LrtLuaVQOT7ekhAa+3FJa4sxlLQPmXBnsAMfnZA6dMuzq8XjbP2TU0w7ZtjO8l5WhKj1FDg\nklqbJcOg0moiTE2XKF4piI6MK1ev9qVFiOaZ9tnPwQP9DtpT+f+FUejcmuGCaTcdJ1f3STgjvyCI\nYu28amMYiJZvKtM+2QQ4Vzzt9YPoVNC+jGSOQZn2K9qEZT9wrbyh+cPaIDqgerZL1eKcSwuUvazl\nG1EnNLCAaapREGWLbC2Podc2w8rdCOikC4e+b2Da77sVOGOwYl3itQ/aL5OKjJJuWUoppXar5Xe1\nKc9ZETakqqedxzo1QF7uKTHtj34J0J2Cu/4q8B3vBI4+yfxHqKd9Oule37Fh2mNlsqwJeBsLtrgw\nyd5UEtOehKNYP2wj8Z2oxFd7Djvzdt5xxqTJcypxrcu0r+/IPdpZDXm8/JDdcc60jyYROgQUFuYo\npOV5chp5OCl4s13lWvxpmHYeToCJIgE8d0/SrFn1tKtUuwYMWocjclXxI67LkMnXZbtnofygRUE7\nkcdbtaiLNIuHgHTvHGWapGEK2k1+dkBictqxOO42E1StHQJQxsvkO/c7PhZS4FPEsgNKy7dkf2qz\n2NTT7un7tD9jcDL3MfEmYt1RguzSz+80MHnesmSv05K8m7uVHm8ZmEfB6enNsRP7kATkNH3aS8vj\nQ7Gdju+h7ctg0EXtTEI8cC551lDGtQxo9zwmt31zLJHXtXsDzAy8riaaoDjADWiPYw4qvkk3rwIk\nE2g3etp5DAQ7iqe9BtOeBdEp8ngmy+O/yX8ffvbED4g3DA4CV3158nr+cLIAS+YTlDTabVY7iHi2\neNP2WXZv9Clo38WFBGqTW5xry7Y6pXYnPV5c923PkzsapM/1t36P2Qpxidc+aL9MKjZJumkQHYvz\nzBGtWfJpZSWSIcb2JLSSVjKVkQFyiduARh7/8rcBL/pp4D+/H7jiKcV/RGLak4dGLXm8xksqVaum\np71iejyV+FIvLtMxwjYhdGm18xPnR2oy7RvDIOuBCkCyXFiXxLQPnQRq0aokj4ccBMgjd6DdlCZO\n25TxKJRVIABw9u5kgWfqNQ+5hxAtDdOevw9tfe15xY+Zae/2C6w4RSWFJIrja9PyjetsOpCZ9iM6\n0E77n5dk2ttxNXm80dOuSY8/stgTk6miEDpAOifpuap978TKs0cjjz/WI/fC/BH58+q1qDnHTbBL\ndEJe1s8OXCym3c7T3mv7WJ12Z4hijjNb9ceiSJow64Loyp2jMQGcHcWWQAF91bowCvAV/+uf8Nxf\n+ie89RMPSs+tMqAdkGX0rnu1m8C5Cczrii5uLPfF9TAM6l+PqqIiHVvaSss3nQRaAu1qejwAjDfd\nedojQxAdRBDdIrbx860/lj84OAS87PeAF/0M8O1/lYxXZP474NtIg0er2sSqlo5lB+Te9rvZ8s0m\nAHNBYdpddBtSS7VtaBcNLzzo/O/uVu2D9sukzJJuORgrlbUDyMtoi/zsQAJapyy2xzgWMETMLVka\nXQAUAQstnTy+uwAcexrwvNcAy1fN/ht08J2CfxvQHpv6I5sC3iox7eJ7pEz7OIzt2BApAXuGp70S\naCcJ2FObwZmaTPvGUGbaKfC2LoVpHwdNeNot5fEAOHmfS9CeLCblmXYKNBGPEwkirbN35Vh2ABpP\nez4Q0rqlo7qwQKTYkcK09+bqM+20B7hVEJ3OpgMoTHtB9gdQGrS3QjHm2izMaVv8AVpFRCaND8fA\nubvFZ9QQOiAXRAfU98FylWmXQHtyjg52yL2wcrx4g0TSn4H2huXxdkz7xWj5Zr+vrr3Z1EbhTxVo\nlVq+kfe1fS+TogPmdnA29aYP3Ytz28l186N/+Wmsk+f/ymD2ggcgHzvXTLtJBm/DtNPFDXo9OmHa\neV4aD8jnehRE2kXdBLQn36OnMu0AMN6UpN7bLph2Jm9jccq0j8MYh9l56TkBvws8+dsSH/vzfgw4\n+sTk961uphDyEWfjztYuM+06Pzsgy+MfWh/i+//0E5mSpMmii5JFfnYgscqk8vkw5s5zSDiXrVwt\nBbTve9r3a8+UObSItnxTmHY14K1MujgJDksTNm0my4wGFmX76GeBTz5LAPM8CW8q9NnrikyYB1WC\n6ExWAxNoq+Jp769mn1tkw8y2YMMmcZoeT9s4meTxtkUyDFK5WV2mfX1nIrGjVXu0A8i1aBk5YBho\nDSexvIJfErTT9/Fwgs8+tCFNuKtWFMda6Tll2uMwzHrhZnXubsXPnoJ2BbVrAs5sQXsh0+7JQGNu\nUBW056XXgB3TzgxBdHR/tfL4tPwOsHKN+d/bYgzyyYLJziRCXNK3musIognETK+HI4s94MxdwK89\nAXjXD4vPaJl2DWivO7mi4YPMl1Ub0wn1gKqnbvpP4vXx5+a3J1kgkvHCZTurtGwl57r37po8fmy/\nr64T5CNHnnbKEndbXo7BrVvnd+TF0g2pZVVJ0N4k026Ux1fr077kGLTrsgsAOYguXRRRq9DTDgDj\nCzJrXAMUz+rTDgArIIvYnXngx/4NuO5F+g328r723Q6iowuoFKjPd1vSAsq7P3sSb/ynuxrfH9tW\nkxTY28y7y1SsWDY8E9O+h2sftF8mZW5TJgfRSUw78VUDmM20AzKQqxBGRxkuKbWZAM0BRmJllPn2\nLdXI4sMgC6Irz3jmvcN51YJUVVpXMSb1ak9ZPZsHBDOlx+sSrW2S49PSdAt4pKbEcmMYyKveVRY8\n0vLbGLHpyjjjCIabMz5gVyNVHl/G0w5IgGVjcwdf9xsfxAt/5f212cwohrZvt2SNiCYaebyeac9Z\n0zTyeBsgnOyj2dMeK/L4waCC+gOQrpk2r+hpp0w72UdqLdHK49O69oVmuwwg9WlnwY7EmJRdmIti\n6I+lZnHl8GIHeMcPydJ4QO9pl4LoklmQc0+7kofxnc85Dm9CFpOueS7w7/8n8NhvAL7ml/PbI2Pq\nSkucV9eBYFV6tAMXRx5ftu0SrSOOw+gkMFfD064y7WrAWd2i/l9AgLte25MWGYqqSabdBM6t5PGh\nHrS7SI+PIj3TTlu+mfJtNnYKPO0AMN6Szk89pj0NopPB4TITY80KeY3jz5U7VajVzech2T4D65au\nRzuQSOVf+Zzj0nsfPO++s4FatEPALKYdkK9F123fdG0I6f08CWOgAUn+btY+aL9Mypw0LLd8k/qf\nz62IgDegHLDrUabdPozO0wXRQZb4LjMCOroLdgFqgMy0V0iPzy+A5JP4paoKPImvPU2QtxnkeEy+\nkwF4ZFVFHk9VFdMFmrrp8RvDQGbaywJhQw2ZONfhsKArQpVtB1EmEwdQiWlPP39ma4xP3n++1v6Y\nQt645GmfABOFaT97j9yjnSfvL0qPTxUGVkAYU6adaRYPIQfmAcD8fEXQTuwoHS6ux8qedoM8/jAK\nzteL/2fxH6ALoJNtpV1Puf2MYq5dpFFT/gHgBRfeCdz/4fxGZgTR+Q487YmdSOnTThYOD3SBn/na\nx8kKkO4C8KwfAL7lzXqbAQHti21xDFxL5CXQbsG0L5Egus1dYto3K6gCji6J4+iCLQ414WdV2rUF\niqfddRAdDTqjVQZ0pNUoaDd8x0uHaZd7YadFmXbTXGB9GGTXgdnTLs5PnUWGLLAN8jYo004BPPoH\nUFiKeg/Y/fR4k6cdAH7qax+HP/qOp2U/11nwKFtSu7e5Ekx7g9ahULOYJMnjoxgIml/IaLL2Qftl\nUjFXJ3g6pj1S/MRLEnNuy7SnQM4qrErq4axvtbQCOrmr4HmmnvapPN5mHxPgQXU4DTDtgMS0H2IJ\nQLAa5KR8ABpEp5PHVwBIvbwV4pGtca1wkY1hIEmaKx+7aY18cc3GrkH7pJqnnWnAL1BfKpbvapD3\nOLNYI4+fbALr92c/pky7XxBE167KtBvC8gBZxg8ACwv1mfZWTILorFQqejWAlK/BDODh+T8BrF5X\n/AfaMmifJ8FLZffTqJ5SzlMXEzzt7t8U70t97MwDrn5OfsMaT3sd0K7aiRJ5vFiM8+NJkp9AsxZm\njUck3HO5XbGtX4nalNLjywM6mWnffU97WdC+Ni/OQ11rE6CGQOU97aVbvoUqaHfraVeZ9rTKSuMB\nxVrgPIjOxLRbeNqNoL3+PaI7z4C8QEODDWk+Cm2z1jeAdplpr9HyLevTrk+PB0T7NwDFLDugTZDf\nbXn8aKJn2tOiC3F1rAVlSx53Zt8/TbZ90yp9VGuN2j1nj1X5peP92tMVxdAH0XmUlQllpr27CMwt\nASnWqehpt5m0yOnx5PKkoJ0pjIxtdWgQXTXQPgscSVWVLSYLIKl/asOiNR0FHlQiHepu+5pM+5q/\nA0RpYmxoNfmhtTEMcIw+YGsy7WN/gHRzfHSh+M2WNQwidCV5fEnQ3srLzAEXoF2VS2vk8aEmiA4A\nTnw6ezlEcsw9dUlXDaLj9sm5+S4W4o+ooH1pcQmVqkUBIWHaK8rjKYhlJsn7Ta8A/u3diaz7ea+Z\nvX3agz7YQZ8wFGUnWsZxiLbxZBFuYA+iHUzP+eKVwA9+DLj7vYn9Sbe4IDHt05Zvk+pAKZ9jIAfR\nIRwBwU7WvQCtXrG1AJDUFCsdcbxcTwKr+MQBhU0aJh1UilohuSh5X8uNv2sL4jzUDREF5ElzCrSr\npMfn5fHimgwsJOK2ZfPcoinzrvrcp1XX0x7FPPP3eky2djiRx3PF077+ABCO0PYF6KWLK1cd6OO+\ns8l8kIJ2E9PeX6LjYf0gurYSRJd2N+Lw5PnkLNBOSIRU2n8xmfY5zeITzQPYFabdUh7fLNNeQumj\nm//soarNtDPGVhlj380Y+2vG2F2MsSFjbIMx9kHG2HcxxrR/gzH2bMbY3zLGzjHGdhhjn2GMvZox\nZjQUMca+jjH2vun2txhjH2WMvaLud7gcyiiPpxM0xnG4TaQjvSW55VZFT7sVGNG1fAMkwCAFh1Tx\nYnfyTLvNwFsmH0CqKi3fAO3igh3TTvq0k30bc80tuXK1/f6Ra+NIV0z46kz+1ncCdOmDvI6nHcDY\nF8fQNWgfBVGlIDqmYayB+oDDHDYp/l430j+wovs/kr1+hCf3cA5kuPC0q23KChaTBpWD6MQ148XV\nWr7RFmUmT7tUz/jPwGvuBG75o9mAM93H9NEYTbBENmvHtNNjOd2eouS4hpH+50efnIyZj/sG4Mqb\n9BtW2oAC9TztoXpdMj+5Nul4OSRWgzILiOQcL7cIaHfsL63ap73bEv2BJ1HceGox51xRBZTb14OO\nmXY6adbJU0t72pUgug5l2h0cS5PE3ga0H1oQ1+C57YnT9lV1W76p9gKph7eDxG66H9fjAeDXnwj8\n5tOwduoD2vc/6oBYpKQMvDaIbuKOaY8yT7s8LiTdjZJFhGU6n5wlj5dAe5oef2l42tPqO2qXV7Zs\nFT5ySKfbYycFYfqpp52OP9GeZ9pdyOO/CcDvA3gmgI8C+DUAbwVwI4A/APAXTJn9McZeCuCfATwP\nwF8DeCOADoDXA3iL7o8wxn4IwDun2/3T6d+8AsCbGGO/4uB7fEmXEWgyJgG6r72eMCCqPL6ip92G\nHYbB0y4z7Yqn3bYkT3uySGGT3p1TLcyUx1cEnpKM3741nSkBu93WsNerX2a/f+TaWGuJxZ46k7+c\nPL4m0x60yDU7ds+0y/J4+yA6l/L4HDhKmXYCIvuh3iIQ3f8v2euTPJm45Fu+5ffb2tMeGcYhAGMu\nT0BYp2LnAALoWDjMGL8EPJWcxKhp5+lrk5pC5w0vKsYkifxKR5x7G097mZZv17AT4j2zZPuAdE48\nB+nxuXOejZfkftk+I16X6WRBJs9LbXf3kFpVg+gYY5K/s2mJ/DCIsglrV+lrXlQHF8S14pppr5oe\nzzmXQHvS8s2tp93E1tuAdgqGo5g7BW8mRj0q2fJNPX5zHcdMO9m/14a/nalkHv2e79C+/6oD+rHc\n1PKNetrryPmzlm/IbyOVyMtM+yzQLr5H72KBdnL+VE87oPZr3w1PO2HaS9w/VAXkomsOLd34k1s0\n3Aft+AKAbwBwjHP+7Zzzn+ScvxLAYwA8AOBmAC9L38wYW0QCuCMAL+Ccfxfn/DUAngzgwwBuYYx9\nK/0DjLHjAH4FwDkAT+Oc/yDn/IcBPBHA3QB+lDH2LAff5Uu28hJFceoZAcQLMd58wbsAACAASURB\nVAE2vUV7pl2zEmkzkfK4GAAYYbhob+vluvL4bp7BtmHhzEy7YcByybTbrExK8nixb9cd0UjADlYA\n7UQev+oLW0Vd0C4H0dVj2oO2OIbMMdM+Hk/gT7MNOPPKMayAtLhD+8fWlsebWGxy78xFetDeCcTv\nBWg3B9GlMvxqTLve0z6OlcdR1ftGAu1jCWyV3l+y4CUz7Zp7nHlJi0bbIosSKy0C2ktOtEJ1TNco\nflqIcNwjTHsZ0E6eDZmnvY6vNI71XRboAsgOAe1lFodpEB1Jj3ctj5fYawt5PCBPTl0zSmptVUiO\nB4DVgVg4Obc9qd0ybzbTNRt0RjHPAp59j8H3WAOgXb+NMqCDFgX5LheMTIx6UPL8SJkAvsK0OwBy\nFByt8vWZ7z+62JNaw6VlBO0dN2yxaPmWPzfLUyWoVRAdnd+yZN933dMuyePzEK7X9rJs5lHgvg2m\nWnWYdttF/1k1K4huEsX5TJ89VrVBO+f8vZzzd3LOY+X3JwH8zvTHF5B/ugXAGoC3cM4/Tt4/AvDT\n0x+/X/kzrwTQBfCbnPN7yWfOA/iF6Y/fV++bfGlXLgDK0HtYYjx6S8CB4+Ln5RISaqmVT3Izr1u0\nU6OyVCZNlsU+ykF0NT3tU3n89iQqPbiFOeYoL0uVqirTrtlPq4mBruc9AE9lC1s9YOkq+/0jCzrL\nJAuhDmPjmmmP2uL68Bx7maKQfM+yyfHKe1162uNcO7V8mvh8NHvh4oQRtGuC6Cqkx2tZV2hYpJyp\nvmRJfumxBLbK7i+TAjFnhDj2V2UrT9kirM0yBe0lPe1lzncbkcK0Xz97w66D6GKODm3jmF5HdEFu\nh7TPKxMuShZ0FvwmmXYyIbVg2gElcKlhpl1iuywWFzotD8v95HqJOXB2ux7bTsGcr/WUzr6OKEuc\nWgxc92l3IY9X379uoyicUaa5SNk5SqAw7X1HIFi3H5HZ0ZrVUr+dO7Y+FHtZWuNNyavtxNNewLRb\nBdGRceeiMe0z5PGMsV1l26WuFSXGSJlpb67lWzpmSEGYQZzvnrPHqukgunQUo2fmRdP//3+a9/8z\ngB0Az2aMdTnP+vUUfebdynsKizH2CcM/PabM5y92nd+e4CP3nMXmOMRir4UX33i01Oc4V5l2cuop\nQ6hOnp76SuDMnQnL/riXzv5D7Txo37BgGTyD35UChgPMImVYVxoGG0gYrjJBGpF6LInVAF5LBstA\ndcaQME7zFTztVLUgH0vlOx64rhpAIkz7AmmhUlseL/Vpr5ceHxLQjpHb9Pg4GAHpoawI2p3K46Mo\nY/4BkFaE4nz3CWjf4V30Wf5cnSoB2lOFgO2EJe/DJmqayNGkl4LBcIT5fhvIbDBlmXb9Pmrl8QNL\naXxaZBxaagXANLW/tDzeNKZLfdoVT/uBMky7W3l8GHN99gO9Z6zl8eIcz/viunENjqu2fAPUMLpm\nQTtlu2z3c22+mwHOM5sTyattW7ogKF16/Fs+dj/e9ZkT+IEXXIdnX39Q2oYcQpffhosgOhfyePX9\nLs9xYJDBl1UZBCEJBGwxCdy5kMdTcBSX4P4We20s9ds4uy0InJ7Ozw5M+7QT0BlESdtIDVM/cz+n\nx6vDNKAdVeTxl0AQHQkF1YF2IOmOkO7XziSyUt/Ylu0YSRcVXYP2SLtoSPq0R3tfHt8YaGeMtQD8\np+mPFGw/evr/L6if4ZyHjLEvAng8gGsBfK7EZ04wxrYBHGOM9TnnO+p7vpTqnjPb+P4/+yQA4ElX\nLZcG7VEutdnAtO+cFa97S8BgFXjZ75XfQcq0TwPF1ndsmHbKcOlbLUnBIVWYdrKPPRZkKaKbo3Kg\nPYzU1lr0WLbzoN0B096v0E9eZtr1+QAAqknjAYlp78finJy3Od+kOOcJaG+5Y9r7Bx8FTLuZTU79\nW8L0Vnj4qxVGMVgUCtBus5+Glm91J308EtuKmJ+1bKNAc57YX+7kV+JJ7J7cdk7A5GkX+50uyNk+\ndHNMO2FpDg3aolNFnaLnIppgsSO+SBWmXfJ465j2+TXrXQQgyeOX/DFS0F46iC7HtOcVPy0WYylt\n49keAAtHZm9Yw7TX8rTHXJbHZ0w7OU+28niymNf3CGh3LEOvGkQHqG3fGpbHV0y5B5IwujtPJ8Cl\nrq891MrjyaQ5jHHqwgg/8bbbAAAfuecs7vqFr5G2ITHt08/SIDoX8ngTW28rj09VCkDSf9xVRYZF\nhbJMu+ppl0CwA9BO1xTiMkz7XJ5p14bQAcD4AnyPodf2MApicA6Mwkj6DmVrNtPOKwfR9abz27LK\nKFcl9Wk3tC4cdFvA5u7I97ctcz/oe1x72ulinNnTvreZ9ib7tP8iktC4v+Wc/x35fZpeZZqepb8n\nZurSn5nZI4hz/lTdfwA+P+uzl0LRVaotq/A0AzsMyCAupOnx9BSULDIRSyf2NmCEEZcFlcRTWWrt\nlm+epw8UKTmxmoSx+VjqJvUuPO3TIDqrY2nw5eb2sSpoJ0x7LxQPvnPb1UD71jhEFHNFHl/P0/6Y\npzw7e31teA/++c5Ham0vrW2lR7sxVVxXDTHtnJzvGHqVCpXH38cPI+D5h/4pnkgEc8yGgxCeKOZZ\nInnyR8R+XrHoaA2ZMem6WemKB3nZRQZm6GKRs5YA1Zl2cjwXPHHPlJUzRjGHR5UV6bFkTHtesXot\nUKbtmOdWHm9k2iXQThaLLZn2gQTaLyWmvbl+xGrJEtU2cM/7gN95LvAPPwvMSDU/uOAuQV7ytKdM\nV1tm2u94WIxBusA1CqhTaf1ekMc79bQbmfYK6fG+J8vNXaTHk/2LWAl2VQPaexqVF4AsMHbgYKEh\nC2fUeNqXsI0+xuiw6bZbcxIo19YlkB4/miGPV3/fdIK8bVhnk/J4Ov7cwO8B/uCrcOC9rwGQ/H4f\ntBuKMfYqAD+KBAi/3Pbj0//baKCqfGZPVhV/JqAJT5NAnOFG65XwFqpFWezpoGazAi1LuvWT5TWf\nyFuqgHZAGnz7mcyp3H6OwyKmXXMsHaTHz1dh2iXgQR6YKmivkhwPSOnx7eAC0tvv/Ha1yUv63bpS\nEF09pr1z5PGZ5+64dwpv+/DnZnyiXG2NQ7n3a1VPO9lGXTaOMu2x1FucBE1ysbiyyfs4BdnDd47P\nYzxlfHP4rq3x89Vl2ilA5A4nF+S6We6Iv1e6r7xB8cNaOqa9qjyedLHwaD/5kn3aOWSmnc3YzzJ+\ndkAL2utIaqM4loOg0rHclB5fquVbfvIMuJfHV2mjlpbMtO+yPP69/wM4+RngQ78O3P2PhZ9dm3fX\nqz3QBUEpgHuWEksnj2+rYVI161L3tNdNjy9q+TZ04HGm4KiMPH5prjWDaScPm0miDJJ97dXGn2Km\nfUtuHzzLzw4oz8DpvLFhFY1adCw2gfYBafvWNNNuG9YpBdE5T48X1/3No78GHvwXzH3mzXgmSzjZ\n8X56fL4YYz8I4NcB3AHghZzzc8pbZrHii8r7bD7jNh76EixZWlL+ZszLUsmp1wLNvjlYrag0TLtV\nmzLKtDM9037Qp0x7xV7OhOWam8qcyoKmSRijxUxMuwa8VWWLNUF0Nuec0SR+X8+8AqjOtLfnsom3\nFweZx6uqPD6VtvaYO6Yd7R7CAzdkP56+8xM4vTkq+EC52h6HeslvmWpRpl08gDeGQa1evxS0c0Ob\nsgFh2jcxl4XOpXWSixT0nKed3DP96fVozbRzQ3gakLeV1Cly3SxR0F6aaTcsLOjO86CqPF6A9nnC\nOpWdZMWGFn8A4OsUAWVBu0YeX4etCWMue0pnMe1lQDuZPNNFPpdsZ0D6q3vMPEE2lexp38UwqF4L\nePBj4h8/8abCz9K2b/WZ9rynXWbaI5xXwK065lHg39Ew7ZdKyzcAWO6LY7cb6fFl+7TLCx/ug+jo\nokJckWmfo8nxtPtGkKgKKdNetqOGWiI9Xu9pt0qOB2S12XSeQtst7kZJQXQGebxrO4SpqoyRCw16\n2ul1uUK6GlzlnQYwvS8u9/R4WoyxVwP4TQCfRQLYT2re9m/T/9+g/sPUB38NkuC6e0p+5iiAAYAH\nv9T97EAykKVz6R2LxPOIc3jMxA5rHlS9mU4DfREGJA0U2xgGiEvuJw2iY74eDHshAV1lkoZ1VWPF\ndBIoD2ca4qaVx1cF7bSfvD3T7kl92smDVQVjZSfzuiIS+TTYpSpoT5OFXTLtANA99uTs9WNwLz77\nUH3j9NY41LOHZcogj49iju0aD1gqj+dkUY4ueFH/3jbv4QSXW5Wd5IJtGAfK5Jh4sKk0sOy9DSRA\n0zMtHjpl2sU9t0xagpVtMSOnx5MFkLbmPFdl2ukiCOxBe77F34zWdGVC6JTtpOeqzuQqjMp42m3l\n8XrQ7lKGrno1WRlrAamFXUyP3yxq+XbvB2WJ/MnbgHe+GrjrPQAST3taTXjaVaZdXRhQWWUVcCbb\ncOtpd8W0LzYmj88rFtTfF5XsaWcSuHPdp5278LQPSBhhkMwj+l260FBt/EkXOdq6IDq2rYTQ2THt\n1NJUdVGhSkme9jJMe4P7VmWMpOOTa5UCXdRqc3E/rk753EkUg+/L45NijL0WwOsBfAoJYD9teOt7\np/9/sebfngegD+BWkhw/6zMvUd7zJV2exzDfsZfIG9sDqa/TqgqGyURsbjpQcl51smxIuKdVWR6f\nnzCXPZaBQYYMwKBaqJoeL77bAEMAHFvjUEroLSrJl0sXQNSU7io2iOyzBLSzFLRXY4zTFVvJf1b1\n2NE68sTs5ePZvTh9od7EFEgeNjIQsQmi04N2oObEj3raySINtZZk/j0Ai0srGqZd/Hz/OUVGRics\nJLHbZlIQxUrrSYlpbwa0L7XFdktPErh+rNQy2JXT48WiHGWdyh7P/LGcsRBbpkc7oGXaq+ZUAMl+\n6tPj68jjxfntkKmCS+BUCIRL1GKD3k216LOLLlIBAIbngROfEj+/+WXAJ/4Y+NObgck21hYoaK9+\nngF50iyYdnE9jadBdLTUvISJIu2m/wdcpce7Ae3LEmivd+xoUcVCj3x3k9ddLXqMmgiik+Txyhzo\nymX5md1re+i2/NyxXWyR4zWDaa/uaZ+mx2s87avsgtzurQzTTkgpmqWxmxL5Mp526Xw3GJRXZYyk\nauGtid2i/6yi9wc952ueICvifaYdYIz9DJLguU8A+ErO+ZmCt/8VgDMAvpUx9jSyjR6An5/++NvK\nZ/4YwBjADzHGjpPPrAD4r9MffweXSVXxtcccBeFpGqBZmWkXk6k5MqhtlPR7SaC9SNKdlgPQnq74\nlvXXBHRSzZRbSMdwlWGOdOW3s4mtz3jmI64WpkX269jTgZVrktfP+qFq+5YWYdoPd5LJcxTzSv7s\nhGHhsjzeBgyb6sgTspeP9+7FKQegvZY8nlwjqmyv7H2iq9gkjzfs26G1NZxmcrslCtq/eEYF7SQ4\nzRcTLhuJfL5NGRmHnMrjxXWz0CKgvSbTrgXtldPjBWjv0daTJSdZxYGYmv1cOV5uvyjTPlVnna0B\n2nNBdOm5ocdySFuN2jHt7VjczxdGYS2LCS3bgCW1djeIjrBKvsazecc7xOttwqecv0/ytLsMojN5\n2k9sDKXPjJVgOcq0N9enPX+NdHwPvbbdlLipIDq6f5RNLSuPD5RjKLV8m7ZQq1My0y4fs//4jGPS\nzymYU0H7dStkvKIsdzgC4lhSB1RNaA8yeXz+80dxVpbHz2r3BshZSGR+27RvnJbkaTelx3d2h2mn\nYyRl94vK91g2nnKeAHdXRZUoMmgXtkC+x0F77bhextgrAPw3ABGADwB4lUYicS/n/E0AwDm/wBj7\nHiTg/X2MsbcAOAfgG5C0dvsrAH9OP8w5/yJj7DUA3gDg44yxPwcwAXALgGMAfpVz/uG632WvlLRS\nVRIcFQVAadnhquwrmSxT8FX6gRbH2VKS589gjgAnQXRzbAxwi2NJmOqcNEzdT78LLJRry6et7jyw\nk0ykBhhhhC4ujAKsDGaDRGZs+eYD338rcOYLEgtdqcjiztHuCClheH57Ys1aTMI4D4Sr9I9Xi4D2\nL2MP4s83NgveXK5yQXQO5PGAS6adhjjqz8PC4jLihRggpqK03RsAPO4KZeGOgPZ5Ig3cGoUl+nZM\n9yvGrnva51sRgOR41PW0N5Ue340paC+3j+MwNh9LdSHWawN9eYHGWBqm/fzOBJxza4k4kLZ8owtx\n0+vRtCDXsWPavWictYdKLSZVQLZadZLjgd0OoiNMO9NMSj/3TuCrXpdXWTGmMO115fGaEDmfgbFk\ngh7GHA+v2zPtEmhvSB6/OGdvgWgqiI6CYgm0lwTbgdLyzfcYui0vWyCp2kJNtx/qM+zbbzqIX/77\nO7Of04WgHGhf8kQKVbuf/BdMH0bhUAKeVeXxaes8HdN+mJ3HAesgOprrUm3humptjUO85WP34+P3\nnc9+Z2Tau26VFUX7lJbNmLvQa2WfLdtquUxFkjxenJ+DbB+005pSdvABvNrwnvcDeFP6A+f87Yyx\n5wP4KQA3A+gBuAvAjwB4A9cslXPOf4Mxdi+AH0PS/91DEnb305zzP3HwPfZMzVdIX8ylx0tBdC49\n7TQ9XuzbegnpGOc8mbhPd61FwYYpFK8qiy35c5OHSlkJfxBQpl0ZNNX9PHBtPeDZGWR+zwEb4Sxf\nKg3sPFMQHZB8/yuejNpF5PFHO2Iydn5nguMY6D5hrHEYO233ltXcMnb6V6K/8xA6LIJ/9gsAbqq1\nyZynvSLTrnrt6oB2KYhOSjzX79vi0go6K/MSaL/66uuwcrKNlu/hR/6dEiFCJiwDsiBX9r4Bkkm9\nkR12WaRjw7wXIgXtpW06ZAGB2nS0XvFBSTCsFmHaO1ywj2UngOMwUlQLdGFO2c/FK8qPQ+R9cz4H\nwql6ZhhiqW8/uQrjWLJlZGDdlFdhGUSHYAdLc22MgmQcvzAM3ID2GsnxgBpE1yxol+Tx0CxKnr0T\nOHt3vo1rsIMDq2J8OLczQRjFmR/dtkIpPT7ZBmMMHV8AxvvPydFDKtOussT0/+q/Vy0dW2/box2Q\n+7S7DkFMiwb5VfK0Txc++h0/O9Y7k3qgncr3qXcYSOwZz75uFbfencxbUrCugvarlxjwQLqRabu1\nFLRPdiTgWTXrpSg9vstCXOc9LH5RKoiOKElBO340D9p/9/134zfee5f0O6OnXVIpNMi0S8nx5e+f\nfK92BzZIKIuGZH62SkD7ZS+P55z/HOeczfjvBZrPfYhz/jWc8xXO+Rzn/Amc89dzbk4j4py/k3P+\nfM75Aud8wDl/+uUG2AHlgi8bWhRzeLQjXlHfbqAGaKfp8XapvkEkS2d9SR7vmmnPg/ayLFwggaMZ\n8viyPlJTEdZpHmmv9nL76ZnyAVwWkcevtWTQbluTKMosAACchNBl2z7w6Ox1Z/O+2turJ483M+11\nJvdyEB31tOuP48ryASwcvlr63VXHr8e//NRX4YOvfSGuOagsulBpIEk7t/HzFbae/Po3iNcv/a3S\n29QWWfAZ+GL/yu4rJ/56v1Vg05lbqdZlA5BAezsSoL0sMzIJY7OnXd2nxSvL7xcZK/ptwTye3a7G\nwuaZdk0QHS1LeTyCUSOs9qZTpr3ZST3d/gI3TErv/HtgtC7/brKDtu9hZQo+Oa+XXyAF0ZEAtW7L\nPNUsYtpThr3daj6IzlYZpn7GJWiXmPYWlcfbe9rTBQ+XPmeZaVe+92Qbv3TLE7Nj88rnJLyeuuB3\nbEBUDSnTnn2BHQl4Vm1Tl3naNUF0APBYRuYClvJ4OlfZDXn8+7/wSO53Fzs9ni5WLFgy7Wm94R/v\nxLf93kfw0XvOFnyiXEnXJVlMWiGNxaJRfaXlxayGZvL71WQtVkhfzDPtVHquufEdpLJTeUoZ6dgo\njMz7qG2lNld9wizJ4+284iFNj58lj68L2skEtm+ZIC+B9qrHaVYR5mbVFwzKuQq92idhnHUcACCF\nvtSt1spVwIPJ697Oidrb2xpHMmi3WWCgfdpV0F4HcBCgSW0bvkEef/DgQRzmKxjzdnbcB2tXo+V7\n+geDRp0CWHraY8A3dbG4+tnAf/xLYLIJPPalpbepLXI+qPewfHBnlJLzyuKhcmSqSuMBaZLaCgVo\n356EpaTokyCEx8hCLFVP5UD7FeX3i1w7AwLaq3aFyHvaNUF0tMqop6gKJxzKKd6OZMr02WozIU3r\nYnnaB5GhO8YX/g541JfLv5sym2sL3awV2yNbYxxarKZyomCzRRLfOy0f0LCdgIZpnyGPdxNEl99G\nFdC+0Gtn0v/NUZjYED17C4laoSSPr8C0U7XCdMFDarU1rnc9SueZ50H7sSN9/O1/eS4ePLeDZ1yT\ngGFVAn1ojoDJ9pwC2ocS8Kzqadcy7f3VTL14g/cQ+b1dy7cup8/A5oBxWrp538Xu017F0w7IoXV/\ne1vSZOyON1/Ap1/31bX2R74uxfNqhW8A4ADYfp/2/dr9kjztFky7qT2QU3k8mYgloD25icoAzVEQ\nKfs4K+G+IssOAO18cnPpYxkZjiOQn9TXaacGKL2cE9BeFtjJ6fHNM+0rnhgM1ytM8BN5fDNMe++g\nYJQXJ6dq91XdGgfoMA17WKaKgugcedq5R5l2kzx+GdceWsAfRi9BxBn+T/girK4WSL1pj1oonvaS\nlc/WUFoR3vDVwI03179epUBMwrSXuMfjmEvH0vMKFg+rtnsDpHvbC3eyyTnn5diRSahkGFCQr47p\nSzZMu/i+fXIazlZMFo/KtHyjVWbBWGHaKeByxWpTS1cV6fRc28/Y5nEY5xhll0XvwX4oWCXceLN4\nfd+HgAtEDgxkoH11IM5F1fMMyIDbL8m0q60lx2EetFN5vAry6+5nWlVAu+8xaUHH1eJMaAyiK8u0\n59UKLvtjRwZGM/njyTV15fIcnnntarb4eHSphyceS+aV/+HJV6AVkWyDdl+5p7eV3vJVmXaNpz0N\n4VWrjKfd0LViq+HMCiA/Nzi00DVes7vGtEsWovL3z4JGueRCqRIYWr51EGRKVT/c253B95n2PVhS\nenzlILoZgLhqEJ3nJZOyKHnwdxBignapG3IcxGbprA4UlZFRmooy7Zae9jAU34XlPO3KfpbtjWwq\nwjrZ9mr3qae9KXm81PKNMu0V5PFNedoxZdqndQRncHZ7jEML1be/PY7Qa0Ae78zTXiI9nnUXcc3B\nedwcfiveEH4jRujiQ0UMG+2NzcWEy8bTHnOl9WRTnnYK2qn/vsR4OYnkcUjKg1DB8KBicjwggXZM\ntjHfbWEUJPfN9iTEYAa7G07EPZYLxKwjjyfbImRxLaa9L90r032rI49XmXbyTHQlU6bg9eC8xf09\nLcYYFnqtjMHeHIVGD2rdotd1NyBM+5EnAKc/D5y+PXkm0xR5AJgkY/bAUXCVxHQR61ihPD5U5PGa\nPu1tqeWbA0+7I9AOJLLvdKFofVguJHZWUW8uvWbKLjbrQbu7FoThDKZdV4wx/MX3PgufP7mJG69Y\nBP7hL8Q/6pj2rlhAruppD7IgOvJ9D1wDPPTx/JvLLMCSfWyTZ2DV/Stb4zCS1Ko//uJH46sfd9io\n6titPu1VLURVWmiWKZG1wCWlLwAcYJvY5j1042H+g3uo9pn2PVj5EIfZFcYcLWaSnutA+3L+d2VL\nCqNLbpwyzOs4VJn2WaC9DtOuk8eXO5YRAUc5pj0nj6/JtNNe7Sz1tJdl2glb2BTTThQZ88RLWWWC\nnw+ic8e0Y0m0obmSna3dq30z16e9Imh3GEQHU7eAghDHA4MOrj04wAhdrC10cXih4JiTCUuHTFhs\nmPYgis3haS5LMwYBiUJiViWLh1R2XjBW1mHa6SR1smMtBw0I0z4zENPK006C6FrU016RaY+57ClN\n1Vi6e8Zrl7vvJVZuKDPtzkC7GCMoE21TlKEv+3yxrSCKMZyy+B4D2hORLo25A4l6Ja073q58OAHt\nlNWsowgIjfL48ky7FMKmYdpdgHZd67SqoH15TlzHrhaMTPL4stYAXS6ArNB0KY9XxoXAzGT22j6e\nfNVyEnRI39eek+xXCJT0+IoSb9GnnXxex7R3F4Hl47M36LezsdbnEVrT7TYdRHeGLCCuLXTxAy+4\nHtcfMs9/d6tP+3ZFT/tihYyQMpXeH21Ni7+D2ECf2PriPQp/95n2PViyN6lkeFqovI8GqOnk8VU9\n7UAy6ZreGykIKyePt5gs193HjkYeX1a1EFIGW5ksq0E/dSb1gLSftky7J8njG/K0E3l8PyagvYKn\nfZzztLtj2iloOcrO4vbNEUr3KdPUtpoeb+VpJ+nxLuXxXM+068BR5HWynuNv+Lan4K2ffBBf98Sj\nxanRlGWIqZ+v/D5Pcm3KGnpwtqhNJ4DHgJgnY8ysdOx8KnvBsXTFtAfbEttZxocYEcUPn7V4WNHT\nTudW5yrKpsM4NgTRae7vsuop+tlogqWeOJ+ugBOdKK9WYNqB3Qmj21baLrEheQb1D8he3VBut5ay\notQbW4dppwxx2SC6cSHTnraNcwvatS3fKrJ/ctu36tYCWqYgutJMe0iD6DSe9kaZ9pLyY/o+VR4/\n2XYi8c487Uxh2tU6+qRyzyLGkn2dJGFmPUywhZbVwnWVOn1B3LeHihbWpzWgC8BN9mkn33uWMoyW\nTh7votL7Q7JZTmuVXUCfkA0x6mdPXIzaB+17sBYqyOMDEp4Ww5fXmHRs12KN3uJkQtVlAcBLBtEF\nkRJSVZDaDLhj2q097WEWUpU7duv3yz9X6GsslUYeX3byR0G71xhoFz6wHpFlnquSHh/GjaXHY/EK\nxGDwwLGGDZxZ3wJwuPLmtiehDLgrMu0uPe3MpADRnHtG7p0br1zCjVeWWMAg93UrHsNDjBieFcsw\nDmOli0XzTDuLxuh3RF/YnSDCYiFoL+h/roJhR552THYwPy/OWZljGhDQPpNpJ0qTmUWunTlfnKsq\n9zSQTKS0QXS6rIWyYzpjSVDlNMBvpSP201V6PO1ZfnC+KtPefBgdBWAL/MtqYAAAIABJREFUvTaw\nc07849yKflE+rSnbOefAPwyoLd8oaDfbAlSmfaLxtLcJaz9xEkQn/03GgOc/utoC3FIDbd9MLd+C\n2N7Tnh5Dl/L4iGw/B9qDkkFfY5K90FvMy+P79ReSSnvabdrftnsZaJ/DBFvoN54en/a6B8qB9n7X\nzSLcrNoyyeM5B+56TxKOev1X5j5nksfXDXIMY40dYlqr7EKWCwUAMd+bTPve3OvLvGjgQ2mgSVaz\nY6acdnWCx3xg9csq758E2qcgrDzTbhGWV7U/MiC3fJvK43cm0cygF865JI/PMe0XHoLTIsxTOuCU\n9rSD7mdD4IikZ3fHZ7LXVRiHnKe97S49Hn4b2+3kevEYx/aZB2Z8oLi2RiG6rD5obyPET77kMdnP\ntTzthvR4HWj3ehUWvDxPSvRPF1hsJoA5QLwLfdoRjuRQoxlywXEYwTNaiVRPuyt5/LbiK7Zj2nOL\nH5FyHfUtxkqyrR4F7RXl8WFsCKLTpcd3LK5Lco5XOuKclm2JOauoHcAN074boL0FDCloP1D8nJwy\n7a7k8VJAGVkYo8BTLdXTLgFOP9kvKq933af9/3z3M/H+H3shbjhcjQRowppBj+NcW9yPuv7yutLJ\n4ynZ8/D6EN/8ux/G1/z6B3DvGfs0bbFuwqsz7SOSvdBbyrd8c+DLFunx5BrTMe1XPKX8RmnbN2aX\nh1S1ThPQvmbLtDe4b5smefzn3wX82S3An74MuP3tuc/NG1h5VXVjW+n8vau2IQRwABeyDkzA3mXa\n90H7HiwpiK5seJoUUqWcdhV4HrhWnvTalgTay8vjx4Ut33Sg3c2EecGj/TaLB40w5mCchFTp2uWl\nNX+k+v6lRSaxti3ffKnlW0Ogvb8qPF7jjWyRpkrLt3EYNedpBzDsC/VIeP7+gnfOrq1xqGcPyxQB\n7U+5coCXPllI9+sADpphwIvYYaC6rFvT9s2GaR8FqvS8KaadSi13ZOn5jAngKBeIWdBKbd6dPH6e\nAJsyLYRCanlSj+P2aflnGxsCGXdpG+CqoD3HtKdgvY48HpDO8XJbbN8F2xnHXPq+ByqGi0mg3dFi\nglrUKz/fbclMe38GaE+ZdmfyeD3T3ilStqjp8RRwTjMV6Od1IXK2Rb3hT7pqGY9a7Re8u7iWJXm8\nK6Zd7N88Aa9lk/N1YX4UtP/ZR+/Hx754DnecuICffvtnrfdP6xVPqyzTPiJMe1dl2nec+LK1THtv\nOd8d6agN0067qEznt47Ou6lkpn323Jwy7cMGmfZtE9P+0d8Vr//yFbnPmeTxo6DevZ0x7Sx/Pg6y\nC5laFdCQl3uk9uZeX+ZFV6nKSqVpb3HOlBtGndQfemzlfQMgga0UhJWTxxcw7TrQXisESkz45glo\nn9W/dBzG5oUFAPjqnxevv/7Xq+9fWtTTPg2i2ywdRLcL8njPk87DQSSr5+s7E3BuJ2NM+rRTebxD\nTzuAaF74ellNRUTO016RaV/uAMuSvNL+uGVlDKLT7Nt8RWuApFCxy4IAdNLzhph26uPdOSODkplM\ne4yW6R7PpcfXGIP8tliU4zFWW6RXe4mFkFBi2pVH+dYj1feLnBNnTLukSknT42tansjC8mKbMO0O\nGO2NYZBN+Bd6rUJ5d1FJ8vjdYNq7PjBUgui6i+bxaZKXxw9rBdEZPO0FTLvKrsl+bF2fdree9nZR\njkeJoky7K3n8kCws0jT6UUkAJn2/Vh600/rgXWe0vy+qUAeG0yrLtOfk8XK4JGWLd4KKTHsUA+B5\nVZxqMzhwbfmN0s4k04Xrs9v1gm1nlS3T3m/LKoXKc4oZJbd8I9cXVVEAwLqsbDTJ4+sy7cLTnr8u\nV9kFDIg8Hvugfb92qxYlT3vZxHMK2mfI42uDdtpuKZnoDYNo5sM26dNO/a4z0uPrhEAR8DHwaLp0\n8cNhooJ2FXQ87ZXAv/vvwEt/C7jh31ffv7SoPN6WaSfgiLUajK8goP2qTuL1CmNuLRlrND0eAFsW\nbd862ycqbyeOObYnkSL5tQmiI9dyNEGv7WcJwUHEqzNdsWnBy+G9I2VBVJDHByF8RsMmG3oEUTC9\ndVqSWs6SnueD6JQFELrPdcYgABisZi8P+iLIsVwQnQXTblPk+3XIV63MtEexfK+k97VWHl+NaV/0\nxbjhQqJMJ+Ez/exbj8j3HqmFnnvpdO7Pk2tlrTMB0sXaznyyMMKY+TrVeNrrMHMRYYhp2KPKtF+3\nJhajVXZtEom/n3naHcrj45hLigDql69SUhCdo3NMz+kquf5UK4GpZIvBNIjOoo/2rIp0rdSyP15B\nHt9VQfuOnLNQkWkPYy5L471WssA59aRnZZM9RBeup8/Aqp01ypatp73le1n4Y8zLKzRsa2tsAO1q\n4OVn/0r60bSApKpubCuVx+sWkw5AZtobs+Y1XPugfQ9WJXk8meDlevqqEz6HTPtyR9yEsyb3o1zL\ntxm95GuFQOU97cDsfcxN6FXQ0RkAz3kV8JRvrx9CByhBdNOWb6Og1MqpFERXFEZUtwhre02PJsjb\nPcgST3tzTHt3VYD2+dHJyttJ5dXV5fHkXETJ96Vtg6r2w5ZBOwWaunvHAdM+ZRlsWBApEJP5bu4R\nXVHZ+vYjVknEOTWNlK3hAU/4puT1E76pno0IkLzma564d8ooqOKibA2qumgPYFVkWy0WZ6BmZxJV\n8jsnnnZderyOabcA7dTTTpj2kxdGundblZQcT6XxAdl2HAH/90eBX7ke+J3navtT0wX25ph2sd21\nFtkHEhKK/iq0pWn5VkceH0h92sW9fUbpPPCSG4VVqSg9XjDt7oLoaJhb22dgNceg5QaC6CTQPuhk\nw2QQ8VIJ8lRer5PH06oS/FXMtJeQx8cxMCbAubuYC+Z04Wk3WnMOPlr87sqn2m1U8rSnLY0DJwoQ\nUz2yKcadMkw7AOtuJFUqZ80BkhA6NZD503+e/H5apk4NZRelTFUURHeQXUB/n2nfr4tRUr/Nsm3K\nCj3tymC+VhO0k0FNBu0zpOeql3RG2ypXnnbau3HW8ZzJtLsuAtrnvWQ/g4jPlDByziUZstco0y4A\n4LG2eBDbMnOTSGXa3YL2+UPHs9fL4WnEJdvnqJVOqCTfVEV5fBoYRid+VX2R1A7BZ8rjK9479L6Z\nyuNtVscnQcHiocsqYNpnTQDHgdJLXt3Pb/xd4NW3AS/7/fr7ScDUKhNy0TI9vWkQXS5o8unfI14/\n51V2+0S+L+MxVvri+qnCtucnzqk8XnN/V/SWrnSiTK2yvhNYLxiqdZaAzIxpf8/PAb9wBfCuH0nu\n2796JfAvf5D82+nbgdv+MredxbnmPe10gWfNN4B2E9OeyuNJ2FkdeXxk8LQ/eF5mXynwUJl2Cjg7\nDfRpDzWAtk4tNiCPVxlM2jKvzMKZtk+7AbSbQsGKKvOKs4pM+2QTSFWVnflkYVll2ts+0ktoFMSl\nQ/ho5RcMp+fqJb8IgCVj3de/wW6jZD/XeuJc1B1ziuq0pacdcLcQZyrOuXSdZosEW6fyTPsjnwPO\n3Jn92BjTnsrjNZ72VXYhU6sCM/KoLuHaB+17sOQejFGplVeaHp+bhAZD+efV6+rsnsS0L7bFTThr\n0jJSg+gk4KHztNeRx4uBt0tA+yxJ90xPu+sizNMiWSWcdSxjrsjjmwr8AiTQfoUvgIct+ByHUXN9\n2gG0VwTTfhRnKre9SVetuw5avmVMuwPQTvu0s8ZAO02Pn4J2i8lUGIqJTbOgnYRv7ZzFgByO2Ux7\nVOy7ZwxYfpQblQDZz2VOQXvxtZnvYqHc389/LXDjzcBNrwC+4kfs9ol+3ziSQtiqgPbQxHap8vjW\nnFAxlCkyPnjRGMdXBVP3xbP2idi0aLu31fmpB/aDr0+k5x//Q+B9vwjcoaQif/yPc9tZlNpsNS+P\nP+CR701zHYzy+GmfdkfyeNp9hTLt3/kckdj9S7c8UQKhxX3ap6CdvL8KeKPl0s8OyCopV4FkNBB3\nvtdCj3iUS4F2Tds8E7tZCbTzmp52NYQOyLV8Y4xJ1gB6T5bez1y7yen2rnsR8Kp/BX74duDIjXYb\npaC9K85FUxL5OObSdy/LtFPQ3kSv9lEQI4Ue3ZYn7tHz9+k/cPqO7KVpAamujD9dkDOlx89DYB3f\n35ugfb9P+x4sz2OY74q+w1vjUPJV6Upm2pWL9exd8s91Q8vIZGqpJQa1WZOWfBBdQWqz30lSQKsW\neUB0OQHtM/ZxEios3C4y7TREY2MY4MiSGdSGsRKm1VQQHSCB9kPeevbammlv2NOOJQHar2RncXZn\nIvXYLVtpsrd2MlCmJHl88n0pm7k+rPbwZ1IQHZV0O5THd/J+PpvwmCAoUPy4LL+dhHANzwHgWCN+\n8R3bhbkm73HCtC/GwuM5y/8cxlyyv+S6Q8yvAbf8UbV9os+HOKwN2qMoQoeRa8QURHfjzcCcxZhO\nrQnBENccPIjPn0yUPvee2cZNj1oxfHB2nZVAexcYK8FKt/5G/kMnPgU8/K9S+yiJaa/ZG9tU9Jm1\nDHGdY46CdkOC/EQnj6++n5RAoJ72b3n6VRgGEQbdFr7pqcfw9k+JINC8p13Xp90d065joetUE33a\nN5WAr17LB6bPxlEJYCN72ovl8XWY9m7V9Hi13RuQS48HEv926uc+vTnGFct2bWDDODa3ZtW1fitT\nBLQfIErSs1vuQPtnHlzHb7/vbrzw0YfwVY87nKlPFrotaYGtqPpS2zf3TDsNbZauoXUDaD93d/bS\ndN/VaTcJiK4GOtDeYjGuZCJ0sdWk+rTB2mfa92gtWPrao6Kk4Z2zrnYrKQJgFghon+XpG6tBdEXy\n+MFaPaaLPCA6cXkGO2lLZ9jHJqqbb/kGzD6WcQxzPoDrWhAAcJWL5GJbb3YetLtl2jE4iGC6TrnI\ndnB+48KMD+grtVC4SI9HlExIKNN+vqo83uhpdxlEl7eVjIK4dDptGJBroslrEtB2NQASdVJRjdWF\nuSbvcQLa5yOxj7OzNRpcPKTb4vWZdk5CUEPWFuO2yrQ/7TvtNtyS06avOUiY9gq9p2md2aby+I7c\nRg3I7lsAwPHnitcflxdKaHq8K0CnFr1WFjnxCUvyeANo17R8G9aQqJo87b22j+97/nV4+ZdfDcbY\nFIQmVehpn4J2uq0w5pWtTYDq966vlpGD6NwAty0FEPXadvJ4nZrABM6pdahshVFNpl1Njgdy8nhA\nDl07XSGrIopUlU+11o1SkWfgckds22WC/E+89Ta8+7Mn8eNv/Qw+eZ+YU60tlicHbMJXq5SUHE8X\nhCjTTvNUzgrQbqq6THtQlLUA4Abvwey111Qb5IZrH7Tv0bL1tccS065crC/4CfH6615fe98o2Fpo\nkVTfEhNRnxmAR67VUs3U5lYXQPLAbvEgA7gzFxbCGB7bTaZdDHpzfIjUBzZLhhdxTWpqU0VY2+W4\nOmgf54LoHDPtjGHLF/1Zt89VC6NLF8kqTwa08ngqsazItHPKtM+yltSXx8/7Yj/L9k4Owl1i2gFp\njDhAQPss+e84iNDarXucgKm5UKhUZrWenBS1pfv/2XvvcEuyulz4XTvvs0/u06e7p3P3RCYzwMww\nwMCgZEQygiQDfiJeE6iPn9ePe1H49HpRUfGiIqDoh6BEJSs5T4ZhZrqZ6e7pHE4+O1fV+v6oXbV+\na9WquKtqd4/n9zzzzM6nusKq9a73/b3vsJWyPJ72N1p0HJ/aISLvtj9uKEMo9FvYkyJol5j2RtV/\nYXvuUuC23xXP7/lnYEVMDKl0ejmpwWRI0fv/hEXAUBR5fE8nj0+HaQ8yOKMRcOpEXWdExxiT+9rV\nyK4Y1dfI74epRqXoLip0+tbQbKFhWq76gDFbBRFXHk9j85yFiVKxIC3OOJVk+SM4pz0deTwgS8HP\nJpDH9/1MMIcpMr+dKYl/v2q2OEz98KTYP5+454T7eHNYkgWprJl2X+f45cPi8cW3iceKovclN+zw\n/ObQTLvp39MOAJcwMTZ7MMUFUhug/QIt2UE+gmmRSQGcMnBf8gzbXOn577J7IIctArbGi4RpD2Ea\n7Mg3n8myOtgO4xwP2HdDcpOoDcBiGBvi7WnP+BIqVd3BpQTTZaLDFhdMi/tn3qdd5FhM9MXkdrEZ\nj1nqGRZqGfa0A0CzJCaynZVksW9N14guKWjXyeOHZ9rpwpy0iqyC9sqE7NQbp8jK+XhBbGfUFXKj\nTxU/+THtdDEp1IjOw2JnuJ3EPb7WE9sYO8UizW2kCwDckpjEJA7olGk3GblOKmPAaz4B3PbfgZd/\nML5yijLJ7SXsSxW0E/f48QrQ9Mmy3vdUYOeNYsHB7AJfeof7tupVkUVeMj1Xxiwij6ftY7497TY4\nGkshp51z2dm8FADaKdOuTtT7Gnk8ILPi/SEc5GUWenimnTEmXyNDKiqkfvZKCYwxVCXQHj7W9jQ5\n7YBeIp+k3cB16daBoyju8ZI8fgDaKzp5vJgDnFlNo6c9XaZ9koD2hQSLCrpSj8f9BMDPT0afEzVS\nannxK1/QTpn2/RS0y0z7O150Nf7x527E0y8X9+mhe9oD3OMBYJy6x28w7RuVZ9GLJIqhlkX7XT3u\n8UXg2lcAN7w2HXBHZIuNgvi7oZFv/YDIN/UCG5ZpB6SbhCP1Dbvh5u4eD0hmdGMRs9oN01LYwnx6\n2uvdBThr93GZJQ/TPmyclqY6VQHajZVkWdaue3zS/nsd055C5BsnoL1YCpDHp2TgOFEQ2xnV9ZVG\nT2beWkIc5KcswWKHZf7majZJ5PHlrpBgh41D3b4VbJY3TClM+9SQ7tjMEJNZS11c2HED8JQ3A5Pb\nELsok9xalJj2w+eaQwHkBY883odp3/c0e7Hhx94qXrvnn4Az9wOwZeEOu2lYPNToNEnR36xZhOV0\nwBDgL483u4BlYoy4xyd1mjYUlj0oSi2QaffpOa+klNWedk87kG5f+zoBWI4jd40a98WUx1OFgg60\nJzH2s4KM6KIw7ZI8XtPTPpDYzxM5OHVQj1qGackALhV5PFGbESVpIhWSptTf+dEZsRC3e3ZM/bhd\ny0eBT/8mcO+H3ZfGMo58W+/4Me0EtO96oiBfWueAtrgPl4sF3HLxnHSM4/jj6Mpwe9ojHIusSYOM\nagO0X6BFB98ooN0kk+XMow4IgKkT0B7G0nQ8kW/UiE7T0z5sSXmbA9Aet5c0a+ABCPkYgBlmD+Bh\nvfetXsACSNpVabgy1wLvYwr2Snvcm1g36552AEZdTF7N5tlEvyFAOzXXGiKnnXM56zcFpr1I/4a6\nYJPUhA6QrpkGAe1RZW2yt0bWTLsYI8YNAYjDmXZV8ZPhbbIhQHupI7ZxrWMEgk67lSgPpt2UXKeT\nxJZRpt0qpDBpdopmj7cWsKlRce+LzZ7pGlglqXNR5PGsCOx5kv1471OA/U+3H3ML+O5fux+j7QXL\nMdVHUYoa0VVMwnISPxSq6PBUr4laRZzjSd3jo0rjAaBa8meOdfJ4QAbYUdtxdKWLlBu2hl3YoqXr\nFZbk8RGAjZ9D/oTGQT5RlJoZwGhGco8nTHtV19NuK0BoTzvNKo9apsUVRVwKLXcUtBOlQVry+KBx\n66rtU/o3Pvc7wHffA3z0DS6jLbXQZi2Pd/CIaQArwmQSM3uA2X3i+aK3rz1oLIhbYUy7VBugfaPy\nrImqGHwjGdEFyePTLgK26kRCG0XyKZm8BfW0DyuPB6SV3XpEefxImPYp0fuzjdmTx7DtbPaM/Hra\nAel4zDN7NTW+EZ2ZrXs8AGtMALliQtDe1DHtcUB7oehx6KY97UmZdlhie4olCtqVc5SCnbhFZPUU\ntEeWx5s5Lh4Spr3RF4A4vKddTbHIRx7PWgsuiDAsHjiB8SwspNmmozDt1EwtiTyemeI8sdJU/Cig\nnTGWihld1zDde1WxMJA+60D7zhtlNvsJPy8eEymobDKZfl87va9WDB/Q7se0A0C/hUqx4AJtw+JD\nSaYBoBwC2msS064Y0fnK49OJfUs78g0AputyG8Qwpcu+lo3oIsjjNWZ+gJ88Pr4ixQwy/Oq3gDCV\ni9aIziuPl3rakzDtfjntwxQB7WNMXM9pGdEFRdtdvcMHtN//ycEDDjz4GQBKNHQWCh8d0756zI7F\nBIDxrbZakoJ2jRldNWAsiFtO5KR0zAnp5RSvjGff2ppRXZhbvVFyT3tseXx+THuNXDzhPe3qZJn2\ntKtGdOmCdkcevxba065OlnMA7ZPb3YcXDUB72OS52TXy62kHgImt7sPNLmiP2dNuWpnmtANAgbCv\npY5Pn2pIaY3o4vbKKRJ52tO+nJCp4WRhTgLtqkyVyorjls+EJerN1iQ97Z6YsrSLLCTVe3GY9hzl\n8dUJsSDZb0m5v0Hxk11DlcenybSTaYFlSLFliVhECtrTkKc6pYB2ABJoP5wwq13qZ29UUCgw2T2+\nPms7xj/rHfIXyeIq1k+7D2mc42LKoJ1zLoG8okF62ulkNcjDotcEY0wyKUsikacZ7XGYdrW1xo9p\nl+TxQzHt6fa0Aykz7eR4TlQ1THskebzeIV8H2pP0EYuedt1Yyl2m3LciR76RnvYEoN03p32YkhSa\nBLSnxLT7MfazjQou0sX8rittfoP7/XjMhKm4Rc9z9/w/9FXxASdSb9PF4jUdaE+RaXejCOk8ko7L\ng2JBi5jneW2A9gu0pJ72CBckz1UeLwYWCtpDe9qDALEK2ofpy3WKMu0DeXwUpr2Ql3TWKTLoXISI\nTHvXzC+nHZAA0mYMQHuzF7mvlHOukcenz7SXJ4U0vNpdDPikf60Pa0QHyCDflPPikzI1VB4fmEFa\nT55fLS10sfhMe9/McfGQtNDUuoIpDQMkXpO3DLeTMYkF3VEVk92gVh3vOJRmTzs5d7ilyOMTMO2W\nOE94Vkx7276W92wS4PThhEy7FKHmTEYp0/4Tfw687t+Ai66TvzguFi6xJkwuZTO6dEF7u2+KiWqp\ngEKXRL5Rpj2onNg3yUE+AWj3yWjXVRC7lr0RHQW06cvjky66OtXUGHzVYgKbnl9Pe9V7/SVZABE5\n7T7/1rC+dsk9fgDaSaIPzB5gGh6mPU7UH+d8wLTT+3QaTLu4B1a5WEhIq6fdz9Du6u1Tep+I0z+Q\nn6/abvPjJPItN9B++/vEBy5/rv3/TfvFa4qDPBCsuolbWnk8Ib3cSqO9dkS1Adov0BqPaTJh+WU4\nZ1EEbFWIIUS0nvaIWdOpMO1kxXSwnaudYIffXFk4p6Y0THsoaFeZ9qxZTQGGt5fsG3Ic4yXD4uBc\nHAcAmTDttWmxnWP9hKBdm9Mec4FBYtr7nmiouDnEhmmBwYdpVysl0F6HmFxENaKzjNEw7WWiqggH\n7UqcWtbXDpHIb68IpjRovOwaZnZMe4ARXSJnbJOC9jSZdtmIDgB2zIgxPYnbNCBPHN0JJQXtfu0l\nY5vEceisuGwjZdqXUu5ppwsME7UykAS0D3qQh3WQj+ocD0Rn2ilQpwB7KKY95cg3AJiikZ0p9rTr\n5fHJe9rHUzKiM8N6h8Mc5HXyeCXRB0YbtXIRk4NtNiweq73E2UY5mjUFIoDMS0pWxz1H17vG0JFl\ngL88/mq/fvbTP5Sfu6BdjNtZyOPpeT49VgZO3gOcuNN+oVgBrn2l/Zgy7SE97VHnEX7leC1IRnST\nF3k/uAHaNyrvijuIW3n2khIwTEF7tJ52HxZbnZSm0dNO3OMnB5nTfTO4l9Tuac9Rdg4AUzvdh9F7\n2jOc1OtqQrg/v6T4FTRgT1ijGi85E4esjejGZsV2TprLAZ/0L0denaY8vlIquBEtFo+mnqHVUeTS\nLOh4DwXaxbVNQXsUcyQAMGksXeY97cS/oL0ANhhbWiH71sNiZ937RsDn1pKY7AaNl92+hYLk/5Hi\nvlSN6KTIt/iTP0aN6NKUx1enxLZ2VwGjhzmSYxzUGxpUdPx3Gc4ooL1QkE0e104BAGYa8oJcmiWD\n9lIy0N4fZLVL8vj4x5kCxTDQXmO9QWYy94wdfv3Y9PEw0VBZy+OHjXyT5PFJjejowkRIT3sSUz8j\nqKcdiMC0a4zoADn2refta48jkTd0UulU3OPFNrJ+RzKaXEiBbfeTx/ua0J2+T36+ahvBNTJm2qki\ncKpelln2x7xAmKxS0H7q+55FhnSZdqennfx7yfzZrQ3QvlF5V9zcTk562gtZM1yEaS/x6Ex7tx/g\niKwOtvUh+nKdIoPvbEXsnyBAPOqe9u3MZgzDFkBaPSO7SChdXfF8dxV7Hz+K/1V+DwAeuYfTmYRl\n3dM+QUD7NF+R2KGo5TC11WGiZFQHeUAyo4s7ubfjEiOywykx7bWYTLthWgBV/OQxDg2kl4ybmIHN\nYjcjMO25mk0SefzmomDaY/W0pzkOKUz7RLXk2iKsdw2pdznSz2Uljy8UZLa9vaiA9mQTaLoI7oKl\nFvG/CDJypKB90Nc+IxnRpc20i9+brBaAHgHtlXH5wze9Uf/eQBFA5fFJGEPJPT4IDJsGKn//XHyh\n+pt4a+kD6Jtyvrufu3taTHsWkW+yEd1wwE02orOPSdz5nn9Pu/f6My0e+z5o6sARrTAH+Y4m8g1Q\nHOS9fe1xzOgE007O5VRy2mWX+00NMeakkdXuy7T7mdD5yOMnMu5pp4tTk9UC8IOPijdveL14PD5v\nR78BgGUAn/xlaR6QJtOu7Wmf3Aa37cKpDdC+UXlX3JVXyxIXQ5497SVLZtqDpOce4EEnopUx4MoX\n2Y9veH06veRk8J0uRYum63milvJ2j18EwEOZ9vWukV0klK427Qee9yfu0+cUv4sr2WEsRVx5Fkx7\ntvL40oRQaMxiFSut+DdZe9KkGNzEBu2yPB4AZhrJ+9o7/RBlBXVw3XljrN+WSmopIaA9whjUM2Uw\nzPJY8CLeF3PMZnfC3eNN+drJejuJPH6OCdAVyLR7+u4zMqLjJgoF5hpihW2X9ueoPD5Nph3wxr6N\nE9YrMdNOJ5QF+/p0mUEG1Kf9v0wURy7TnkYyhE/RYzFXIWNGZdzLMHDjAAAgAElEQVR7b7r1N4En\n/wbw3P8NXPpM8bpGHp/IiE5yjw+4Px/4LNiJuwAAryt9HoAYPzjnMqgmv1NJSx5PFwUy6GkfVh4v\n97TbvyuxkSELKkH7UMe0A/H3pzCi82PaE8jjAcWMbhD7ljCrXasGSDmnHf2WMuYMf33rFibmxn1M\n6EwDOPuA/NrqCcAy3dYKIHt5/Jb2w0B3MEaObwV2P1H+8PPeKQxXj98O3P537lvVlBQ0gLi2pWNe\nHvOa726A9o3Ku6iUrRPhBsvzlMcTpr1gdNyL0rR44GQglC18yd8Bv34/8Pw/TWc7yQ1iuhTN5d7O\nac9IlupXtSmXGRljXUxjPZTxanVVEJexER0AXP8q4OIfc5/uZaciT1L18vj0jehQqmANtllViVlY\nXjgT8gVvOQC5wAbnASvGPw8UeTwAqa897uTe9oMIuHZe+gHgmlcAL32/5PQfu4gLNTXhiXKzzTVK\nzSnifbGlYE8Ue6YV2MfZ9bTAZN3TLoDnLMRkNmgc6nnc47Ni2u39JEvk44ESRqIIkWZPO+AB7apU\nNa43BCCfy7VyEWgviTfrM8H7esIrj88y8o0yaHMV8ts6aXx9Bnj67wGP/zkFHOnk8Unc4yPmtD/0\nH56XHIatp0jXCwUmPXc3OTX3+LR62tMzoluTQLt9TGQjuuBjo7LsdB/SxTdacSXyQ/e0+8njtQ7y\nFLRHz2rX97Snz7RTdU8a8nj6G0+/fB7TY2W85ZmX6U3oFn4keYYAsCPX1s9IkW9REqbi1nKbpGws\n3Sne2HWjN7Fm/grgKW8Wz+/4gPswbjJCUGkNEktVaWEcQDpG1iOqDdB+gZbU0x6Jaaf9rvkx7TC6\n0oQviKXphPWLM6Y3lUha5AYxWYomj/fktOfBFjImO8gP+tqD9uV6V5XH5wCQAGB6l/twlq1GdlS1\nmRaeuREdAKwUhTy8uXQy4JP66vRNTILI/6L2jtLSyuOTszVepl05L7ddA7zoPcCVL4y9qVJRvwpL\nTKDC2B/AAcM5L3iRFfbNZeHMHsS25y+PF8Bzkkdl2gNaiYYtOqYN2qqGYRILlGlPQ55KSzKjW0Ct\nXHTZRNMKVyTpSmLaywW5nz0sKog6yK/boJ0uJKRvRCd+b64UAtpp0Qi4nuMeL86hJJNng6j5AkH7\noa9JTwuw3DlMkLO7nNM+jHs87fdOp6d9Os3IN5p/retpD5EQBy1K0N+hFdeMLrSnnXorqGX2Rc87\nK8itGiFZ7XHMJbX9zSn3tMNoS9t3fCkk6i6kLItLc6Z3//Rjcdd//3G8/PG79F9QpfFOrR7PXB5P\nz/PG6TvEGztv0n/hpjeK/X/6+8DiwwDSZtp9yB+VWd9g2jcqt1o7Bdz599jzwN/gpcUvA4iYbZhr\nTzsF7R3X/RPwZ2kM04JpcQUQZ3x6EgAyUSRMe0gvaW5xULRiZrXn3tPuFGG+NrG1yDJvp0e36LDX\nhVJmPc+tkgDtnQSgvd03Mc1IHnKS3HM6eTC8oD1qW4FTttdCDos0ZMJSicu0GyaKLOdzsiakzJuK\nYkIVlNXukZ7nKI+ftIQ5YlhPe2ZjpRT5Zh8vOfYt3gSQ9rSnMmmmVZdBOwCF+UrQ/qIy7VFM6Jyi\nKhaNPD5LI7qZEmEhw0C7hmkfG5Jpp33Rvgz24iFg4aD00hg6gmn3MaFTnycxT3MqduRbaxG46x+B\nVf97RZpGdFTK7LClcUgaOrapIN3wUZ7EBe1WKGhf1b8OeM0SaSuFwmIDyXvaHeWHFM2ack47ei1c\nvFksOhw4E7BYEaGWWj33OpqslVAtFfUMu1PnDupfXz0uyePXu8GtqXGr0zdd3FEqMJSOf0e8ucsH\ntNcmgf23ief3fwqADNrTYtrlON6qd7F1A7RvVG61dBj45C9j5x1/iJ8q/ieA8BPdtDgKnPa05wna\nu5L5id9E1Jkopb4qGlSEbRgviJvBSgDQHAnTDih97eEO8s1eDGOyNIsAkBmsRTai65lqRns2LDsA\ndKpiot9bSSaPnwYB7UlMETXyeGlyH5tpV2PKMjovyWS/TJn2CJM+j4Q/j2unrgftQaCkm9e+dIoA\nwYZBQXuQe3x+kW8AMFkXvx+baafy+Ex72u3Yt02E2T67Fh8kd9We9iFB+3SmRnQEtBdjgHaNS3d9\nyJ72fhR5/MHPe15qoEOYdn2+uPq8n5J7fGhPe78DvO/ZwCfeCLzvWbKRJqnJuuxHMgxAkuTxWqY9\n+NjQhXJ67gHAYy6aVD8OIHlPe5WCI3ptBDHtkjReMVeTzkt7MWleyWqPWqa2pz2FNsFSTUTHmV1c\nNivO9QOnhgPt1DxzbiLCAsPaCfKEXHMrx1EuFlxAbPGI5F7EogtTl9RWwVaO2U/KY8DWq/2/eMXz\nxeMv/B7w7idi9w/+wn1pWKbdVx6/wbRv1MiKAIQZ2ANE2CDuiTBKw8QtqFSmvR7O0tj/Bo46I5Ms\nuqKZRZHfb5C/GxRr5I2lyx+0b3ez2v23s6nK49O4WUUpsqI5y1YjM8bdvgraM+hnH1S/JrbRWj8b\n67uc25GAM8QwLJEbuySP7wP9Di7uPYDSYNEqiRGdzGJntEhTqsKZHJR432X3o8njc1ID0CLOxDNF\n0dIQFGk1Svf4GgHtweNQhoofJfINUJjEmD3tFLRnbUQHpMC0q+7xEmgPWaCjoH3gHj9eLbkRaO2+\nmUqWs1MUtE8WkjLtXtA+rHu8b+SbBrSPs7aWaQ+Sx6eV014Ki3z7yh8Ko6+lw7ZSQFO1ctFlw40Q\n756wokz7hMaILgx80XsuXQgGgO3TdfzBC6/C0y+XI3PjMu1aQEyBUFTQXlMWETRGdJsSXs+Gtqc9\nhXkFY9J1fnFdLOAfOtdMlHvvFDXPnGtE2Faq/tjxOPK6Hfs2Xs1GIk8Xbm+uELZ/+w3Bc83LniPf\nX87ch613vhNbYY+xw0a+9d2WCBW0E6adFdJJnxpRbYD2C62oaRFzQHvwIKG6NmfOcFHAZXSk3hq/\nCV+nb8qgrVjJfrJMbhB0sSDciO78Z9pb3VEx7WIw3IS16EZ0OTLtFlEDsFY80O6sBM+kKY83u8Df\nvwAvuP01+KuybbIYP/JNZYczOt6MydfNwEE+mjw+ZzAMSKB9mgnQ3uwG9bSPTh5f7QrTs7A2ncyO\nN13UdYzoakP0tBN5PEt78VAD2qmb87kYzJxTkhFdKaY8nva0r9kTasaYFOeYphndepfELjEK2vWM\nqls60D5kTjvtafcFwwPXeFpj6Lr7nO77qiKPp/3nuRjRnbwX+Mafya+d+aH+s5CNRIcxo9NFvsUx\noqNqDhW0A8CrbtyN977u8bh8q1jYidtuYOiM6KjZVydIHu8T9wZoI9+k6zmGO7s2li6tRUOSEtHo\nnsH2aXu7DYvj8EKICV9AnaWgfSLCtq4R0L6dgnabgR/PqK+dnt+PZQfEG37SeKfGZoG9T/a8vL9g\nb++wagDT1DDtqjx+bC574jLDunC3/L9q1afhMF1TrIUizNDVqb6Zcx+2yrRXKWjXDxxdw1LivjJm\n2QHJAGWMi4E2OKddBR45XUJST7udGRw0qV/v9FGmzGteiwtUHs/WIhsv9QwLVaqyyJBpZ8Q5tNQ+\nF/BJbzkTpuHl8QS8nL4POPptAMCPF+/EBFo4tBCSc6vZrkATxzSrQkF7z/37YdUNc7jPosikkJoH\ntvvnEdNOlBrl3rJ7HMOM6LKLfAth2mMCkiJXJlBplk4eP6Sbs8y0F4BmDNDemBP+Au0lwLAn4bMN\n6leRnkSeniPjLIY5psaIjka+tXvxJ8/UPb6kuy+un5UXQAY1ztruPg8C1JIRnZlcft6L2tP+xbe6\n579barwWKcmscYg2iKZGHi/ltIcskNJFoZkx/0UyuigSn2nXMJoUGOmY9jP3A1/5X8BnfotsRDjT\nPjNWgSPcWGn3I2+rlmlPywhzkkQ7rp7EZWQB5MEhJPKSPH48ijw+mGmnDvJpxr7R83sXJ9tw0fXh\nX77lVz2LJ3uYrUoalmnXGiSq8vgLWBoPbID2C68KRalPcxrrobnD/byZ9mJJ/A1uYboqVsh9e9r7\nJmpStmJ2TKtbZLI8ZpCopbCc9lEw7WRldw62vCxwcaFHHJtZIb/FBXLj3sRWI/e0dw3l+GfItJcn\nRSxTee1orP7DtgPa02TaH/qS9NZj2BHcf2I11g2s42nbyBAQEzZkbMDuRTaiy9NoEpBA+wTEwpwf\n0845H1zjOcr4iyVpLHIWhIKN6JSe9lSN6HQ97cmZ9iKRx7PU3eO9TPvmhMycU5Tt8fa0h7jHF4rA\nuC72LRszOgrax3gM0B4ijw9a1PIrKo/X9rSfvV/7vQY67vjRVhdMSKWX0x6hp/3U97XRdEFMOwXt\nNA4rbkkLMVWvEV1YK5IE2hv+15vsxp9C5FsQaP/aO4F33wR86fflfUjSZuyN8p6XxQKTExgiXj/C\niC4DT40JkmK0dhKXbBEE0MHTw4B2wrSHgXajK8YmVgS2XSfeW/HK44MWgeMWvQdMUgIjbHwEgP1P\nA958EHjKW9yXdg9A+7BMu/BaUED73KXi+eZLcSHXBmi/EItMVGbYWujKa9/gKOXR70qLTOxnqmL7\n/HvaLdQkpjUH0E72Y7Uv+qzCmPaRuMdXiSqA2QN7EOPVIaA9l4x2p+i5iTUst7qRQHHPsFypNQD5\n5p1y7b7yZvfxNf3v48t36SeTunJuKjMS056kp51MHh75pvTWVYVD6JkW7j8Z/eafmzweACoCEDQQ\nB7RbKOfe0y4WOMeJmsZP/uv8O3JfXNC0PYUx7dkZ0SlxhJxLRnRBvfban+PKBCrNkiLfvEw7nQRH\nLU9Pe5OYVYYx7YAM2gd97TMZmdFR07K6ReKmYhnRpZTTHtbTfvZB7ffG0HH3OW1nmFUAZ2UIZphW\nX8mC19Y33iUeTxFgeSaAaR8b3kG+b1ruGFRg4pgkNaLTyeOdovuzH1O5IFy6yb+TAjYqge93gK//\nqfwDhRJw6bOBW35Ffl1yZhfj9aZG/GtaK+FPDbTLbTCXzovr7cDpdc0XohU9b1QTQU9Rln18i91C\n6dwHVo8B5w5K8vg0mXYqjx/ndC40rfm0purTwObL3ae702LaTU1LRKlm58Q/8x3A1S8Fnva7Q/2N\nUdcGaL8QSzKjW4dp8cCV555pYYwCokp2gMgtMjmbqogL0Y896vZNOaM7axM6QJrwVXrEACrA4K3n\nYQtzAu2aPuKgDPSuBNpz6mcH7OM+AHUlZqFurkeaAPYMy2VtAWR6jk5vvwRHG1cBAMrMxH1f/Ac3\nwiasHFXL9NBGdP6ThysLhwEA9xxd9v2MWrmavBFprQDt0YzoaizncYgw7WNkcuF3TooJc9558rJC\nBbDHSr8Fr666yJnmvixVBHDnJmD2hpLHl3i+TDt1j19IANqlnvZy0WWtAABT2zXfUIqoonSxb2n2\ntNP7ac0ivbShTDuRx/d18vgkRnQhPe1n9Iuj40ww7VQZsVlxz07NiE7KaddMgZePAj/4V/H8J98t\nHi8cdCM61aLXSNKFGSnurVpy477i5LQvNqPJ4+UIvXjHWwuIGz6g/eDnge6AFJnYBrz4vcBvPgy8\n8kPe64n+xmDBC5D7u6OqZ7Q97WktGk4Spn31hCSPPzAE095WFwyDajC22NuzzVanXvJM8dod75di\n34JiTuMWJbbGTALaaxFBOwDM7nMfCtBuDZW8YOjc45251s1vBF78t8DcxYl//3yoDdB+IZbWjM5/\n0O2b+bGYbhGmfLpMmPaAnnYJtOfBtNem4PgDFHurLugJM4DKPWsa0AAljjM+JkuWxdHrkffy2kan\nqBkdW4vkNt01LBcAApD8BrKo6Rtf5T6+af2L+OrBaIZ0TjSRxLQPK49X6ipmOxTfHQO020x7Tucl\nUX00BgstUWRtnb7l9sADyGccIqC9bkYB7fbrmbHYfkUmq1uKNviyuB3dqKuuYWa7L6We5+ZQRnRF\nCbSnzLRXJ8QCQ78J9NtSVFIyeTyZOJcY4MQZAZIhqG9NjEYeXzUpaA8xoiPXMNq2+WGd9L+2E7jH\n98N62mk/+KZL3IcNiJ52GumlyoPlnvYMc9oPfl70su95sm2c5bDtlgEsPqT93W1TYs5ybCmeJ4lT\n65JzvDgeNZplHbJASs+v6QCmXZbHJ2Tao/S0f//D4vH1rwaufonXgM7dYKJqWH7EfUiZ9qgLcY48\nvpw5034K+zePw4lTP7zQTJwQ0SX30XoYaF8lcW/OQuHjfka8dvc/YrpMCbP0QLtYuOWomWSBJirT\nDgCze92Hu9lpMFjgfLhr27en/VFUG6D9QixFHg8ET5r7poW6xHA1fD+bWpELZbIkBgu/CUtnFEy7\nxh8ACJ6U9gxL6b3PYTsB27hscMMpMo4q+jiz1tF+tNU3USU3KpZ13r1a5OY9g7VIk3yPPD7jc3Ti\nhpfBhH1TfFzhAI4fiiaRd43oGJkgD5vTrtR+dgJ1dGKB9m4/x572pEx731QWD3O4dsjksGaKiaSf\nVNCZNEmqhTzUNGTh56KKWFzwUyZ5rpe0Fznpoll3bajIN9rTXkh7LGLMY0Y3l2CCT4u2mzWsNcAY\nyM4rE/5ggxZ1kF+3QTs1oktijudX62QiXjZiMO0UHC0dAUxjaHl8YE875zLTvuPx7kOZaQ8C7cQ9\nPibIpBXa007AIvYMnK7nhZTXr699zyYxLh46l8xBfF1h2p2KI4+X3eOjMu3xgJKzDb7u8Q5oby8B\nBz4nXr/mZcE/7Afax6l6JirTHsC6DluSmuYk6pUids3aC6cWB47ENJJ1KjHT7mzP/tvEPmwv4YbW\n19yPpCqPH8zjG+ig4CxwlcfiAeT6jDt3qrMe5mHPd4bJajctjiJMlNjgN1ghX6VpDpUKaGeMvYQx\n9ueMsa8xxlYZY5wx9sGQ7zyRMfZpxtgiY6zFGLuXMfarjPnPkBhjz2OMfZkxtsIYW2eMfYcx9to0\n/g0XVI0JOe5shKz2viqPz2OyTCaRm6riBuvHfHQMU85oz4NpByTA5ZiLrXcNX7l017AwRhnhPNhC\nzd+qo4szq/oJaatrKIs0OW4j4JH6BrUbONU1TDRYjmqQxhyOzIje9u2n/jPS1wRoTzGnXX2LcVzB\nHsGhc83IrFynb+bnW0F62l0jughMe9ew5Gs8j2un0nBBd9nquivwYfL43BfmyDWzpRSeZNE1FHl8\n1kx7WvL4cgYLiIpEfrJecgFes2fGlnrTe+lkj0yMo7DsgIeFA4D5CXE/8xu341bftNxJPmNAsU/U\nP2GgvTohzLSsPrB8ZGh5vNTTrsrjm+eAtu05gHID2HyZ+9YYOu6iH2XaVXm83IOdjjxeK+Nfpe0Q\ng2M+f4V4zaevfe9mCtqTgTadczzglccHSYhp+4XqC0BLMvaLCZQcI0/J8EvHtH//X2xfDMB2Fp8T\nCgttTe6Ao37E6gm3FYEu4JyLmNXunI8NloGCTwHtsCzsnBFj8InltuZL4UXHnlCmfY0w7Y6bfaEA\nPFZAomuXvuA+ziKnfYqYu8aSxjtFJPJ7XDO6ZCoFzjlMi8sse7EKVwLxKKm0mPbfBfAmANcBOB7y\nWTDGXgDgqwCeAuBjAP4SQAXAnwD4kM933gTgUwCuAvBBAH8D4CIA72eM/fHw/4QLqMgkxQGaQUxX\nz+AKaM+3p322Jm4IZ30k3Z2+lT/TDkj7ckfFvtFy7i8l6o0CeDilMJzn1rsSu+FUs6cymjkoK2gp\n7RtRmfaxHOXxAHBs81Pcx1uXvfnButIa0aUhj2dFKS7F7Ws/toIo5ZHHZ8kOk/NwHPbkJKoRXS3v\nxUPGJDXNxCD2zd+IzgTA81cEkAnvVgLa/QBe12PcmPI2Uvm0Io9fbRux+g6LXOzrQhZSRcmMbgGM\nsUTGVU7Rc3m8K/pqI/WzA1rQvmVSgPZTq3qFVNySAF61BEYlyWGgHZB7O88dUNzjE4B2CoZVpp06\nx2++TNo+Wx4fzrRXhnA7p0Wl4Fp5vNQOMTjmmylo1zPt++bEuHj4XDOyTwotnXM8YCsXqNIgaLxd\nakaTx1cSthv0DMv9vMS016bFfcfoAJ0V2zXeqWteHv7jpQrpF+fuAsocTYRYi8e0O/coANGuiyhV\nGROqG8sAWgu4aFpc48dTAO1qeoKnVokRHV1EuOpF7sPtq3e7qrFMQLukOBwOtO8u2GNlFAJAV1qf\nhUeZNB5ID7T/GoBLAUwC+MWgDzLGJmEDbhPAUznnP8s5fwtswP8tAC9hjL1C+c4eAH8MYBHA4zjn\nv8Q5/zUA1wB4CMBvMMZuxn+V0vS0B+Wq9k1LMYDKQx4vJpFTJbFti0090Oz2TSWnPSemnUz4tlXE\nQOsX2WJHk+U8oXeKHLc668Lievlns2vkr6yg1SDnJ9YiMXPdEZglrmy+wX28ff1ee7UmpNo9+zx1\nF24K5WQLDFQGyIrA8//MdjYd1NWDvvYfnYnmRNsxTFkGmGVkIk0yGByzqEZ0ufe0A5Kk2Zlk+PeK\n2w73rryuUA5URaRWZEyfLwrw5Qfw8u1pX0etXHAn+T3TihXNU87SiA7wgHZAkdPGlKPTSK2x1pBM\n+8BMayvpdz61kg5opwBvolqS+4ijgBPSV45zByVmb1imvaj2tFN2evPl0vbZ8vgB077uz7RnYUSn\nlcdLxoM77f9Tefy5A9rfnR6ruI7f7b6J0z7ta0FFoyhpxjYgs+1+wMayuLRIHuRAntSNny54eqK1\n6Hn31T8WbHBjHnjsa6L9AWefA65EXuppH4Zpr6ZIBkixbydw0bSYZyVl2tvkuCaSxwPAzF532ypm\nE49hRwDIrTTDluMeL0XfpsS0J5XHa9shNkC7vjjnX+KcH+TRlt9fAmAzgA9xzm8nv9GBzdgDXuD/\nMwCqAP6Cc36YfGcJwNsHT/+vhJt/4ZXkHj+Qxwcy7ao8Pg8DKGGEU+6tuL1VFtcPuh2P3DMnoEn2\n5Y6aGGh1EyvDtGBxjAZ4KH/LOZ46M7pm11CcpfNm2gVrOMtWoxnR9VX3+Oy3uT97KVa4vU8njCVg\n8eHQ73QM073mANiAIYn86qoX23E3N/0S8Cv3AI99NbDtWvH2gGmP2pPb8fSLZ3heUqadtQd/PwLT\n3s+YHfYrAtonHaY9oKc9d9NOQLpm6Pl1xg+09xWfkrT3ZUVm2hljSuxbdIl8kUyismHa5Z52QI59\ni9vXLklU21SCGhG0j8txUACwlTDtZ9Y6iVhYtegxmKiVZcfuMCM6QM4uXjgoyeP9lChBRRfjPVFq\n1IRu/nLp/BpDxwWhNPJts9rTPkQPNi058k2ZAlumLI93WF+6wLF4CDD15//eueH62qnDN+1pBxSJ\nvM98b7XTh3MYJqolvZJgUEkXQeiCpydOjcz78E0Sm/fU34p+T9f0tSfpaXeUH5kw7YCQpAPA6slU\nQHs3Vk87HZvIAgJjwJ4nuU9vKtjKkDTd4x0iZhIpMu3MXoRIKo93zuGKupD0KKtRGNHdNvj/ZzXv\nfRVAC8ATGWN0bwd95zPKZx79pTWii9PTnsNElE6k2ovSqrlOIu8xohsB0769KgbaEyveQddZAayP\nQsYPyPL4AcA9rZnUN3uG0nefM9MuKUFWo8njPUx79vL4sWoFd1pkMnb0O6HfafdMxYQuQT87YCsJ\nfvx/As96OzA9YBa2Xu2+fQk7hip6gbF+tDpqBFiWx5z2tMeKfFNbS0YA2gfHzi+SyVbSjGAbiTpl\n0hItEad95PE9U20nyranHZD7Y3Xjjl+VqTy+nDFoH/RND2OcRxegKk0iQY0qj29shtuX21oAjB7q\nlaK7TX2Tp2JGty4x7UWZaY8yfkry+IMSSGj3zdjRSxT4eYzoKGjffIXEeI4zO6e91TNcQFgpFqRF\nIvs1YkSXGmhXtnP9tHCOH5sT1391XPYAWDqi/e1hQXtLMqKTQRuVS/vN96S4t4B+dkBm2uOwm7Qt\nw+PSrVssmtkj9VmHlga0zyVYhDMsDgYLE4zM5dKcVyh97dsl0J5MTdOOKo/nXJHHb5Xf33OL+/DG\ngt2ast5NBoa9f5pjuaWTxyeYCxEH+UyY9uIGaE+jHAcSj8aIc24AOASgBGBfxO+cBNAEsIMxFjpz\nYYzdofsPwOVh3z1vioJ214guOKc9d2MyRbIYDtpH1NNOBpqtZWEeoxt0HQlZPWdGWPe36oFM+4hk\nyE6R/lxbHh++wtszLFnGlsM2N6pF3G4JQyQ88u3Q73QNy10oA5DMOd6valPuynOZmbiUHYs8ufem\nL+TFtDugPUrkm9pakr883mHa/ZiQrqEqPvJi2sWY3jBEaoAfOM7ciV+RxwPAvjkx4Y3atgEAJTKJ\nKmYN2gfy+Ali4hVXFkoXoMrrGlOysCqWgPF58dyRyBO2Pc6ih19Refxs1RRgs1Sze4PDSpHHV0sF\nVyLfN3mkcZsWZdqlnnbVOX7+csWfpY2za12pV3luvOJmlDtVLQ3nbu9Uj0a+qTntun52d6PIIsfC\nQe1v76UO8meTMO2kNUOVx5dkMzpdRXWOB5Ivgji90QVYJLGE2eanOiZ79y3xWowoaF85CkBm2s+t\n9yItKJmW6uXUSDcKVQHtlGlPo6c90IiusyJSLcoN72KJk3oA4AmFB1CAhfWYi5d+1eqZbuvBpgIx\nXEwij5/e7T7cxuyxOwoBoCttRvsG055KOTMoP4cl53V6BkT9ToQ8lkdBEUAcLaddNVbKYSJaV0D7\neBSmfQRRamTCt4n05+gG3ZEz7Rp5vJZpV93j8wbtCYzoPK78OSyGjFVKuIMTiWgEpr3TN91oQPtH\nUgTtgCKRPxRdHm8oGehZKlWqsrQViOEeP4prh0wmJpk9yTiz1tVODrzbmL88vtJbBmBPQHxBe9b7\nUpLH2+f7JVvEawdORwftFU7l8dm7xwNyxvVaTAMmCoiK61SCGpFpB4BxktU+AO1bUu5rX+uK/TpX\nIedCVAnw1E4xTrTOgbWXsGNGnEdHY2aN075oSZbdPCs7x+YqCbcAACAASURBVE/uUNQ6XRxaaEr9\n7HMT3sk2NfNLKj8GZKd0T0+7BNp3yu/RdoJzPqCdOMgfXkjAtFN5fEVl2sNj36JmtAND9LR3nbg3\nBRwxpj/3oi52OTXt7Wkfq5Tc9o2eaUW6pg2LK9L4lNV7lN1ePYFtU7LZpM67KawiR76tKs7xaove\n7D63TWeStXEFOyL5JQxTdD43Xyb7N4k8vrHZTbqZZeuoopfciM7cAO2jKufsi3PGR/4O5/wG3X8A\n9Dke52PVpuH8k6dYC0WY8eTxebDD0kRqSWbaNUDEE2E0Ann8FOkl1U0KXKb9PDCic9hALdOuusfn\nHvkmjv21hYfxhoffBJy8J/ArPcPMXR7fqBZxt7UffT64OZ59wO2J9at2z8QMNV9JKo/3Kwra2eHI\n8vhuz8BYlj3OtMixcXLaO0a4nLZrmMo25s+0b6+Jv396RTMOjSJLHrCv0cH+KFg9d7Kpk8dzzgfj\nZYb7UiOPv3ieMu1r6jd8q0T6XotZGCRqjOio87ZfEoiuOOduv3ARJtgakaDGAe1qJBSArZPiHpiG\ngzxVEGwqk3MhKmgvFIBNlD3+kQTajy3FA8aUJZb6sSVp/GX235Xk8W0cXWxJC1RqPzsAbFe27eRK\nG3/xnwdxx5HgMVstwwroaaegXT3ekjJBb0ZH5fEPJ+lpJ8BqzNPTHk8eHxT3BiQH7Q7T7onWAvTy\n+DjXDSCxr8NktZuW5Xqu2NuWYj87IC9G/OiLqJnrrsu9aXHfpCS/4pxLC4aBoP3UveLxzB7v+4xJ\nEvnrCz9KzT2etkdsKg7JtBcK0li5hS0NwbRrEg025PGpVBgrPql8Ls53Vn3ef3RVsSStak1jHZ2A\nQbevyuNzyR2W2Q8K2nWRHV2PvDd/I7pxU5w+OtDuDCbnQ+Sba0SnmdS3usbo1ACAnNcK4LLOvcCX\n/9/Ar/RGIEtuVErooIp7OenEuf+Tgd/pGCamqRFdhqD9ysKhyPJ4qy/OV6tYTVcGqFZF7kcFbAVs\n3wwD7SOIfAMk0L6tKv6+n5pmJIaYgDYV5Kwm1tEx4cqPabfBxyXzYtJ7MKI83rI4ymQSlRvTnlAe\n3zMtN0TiosIKGB/cVxub4yUyTBCmfeDynLY8fpXK4wsJQDsgg/ZzB7GDZE0fi8m0twlLLEl7qXO8\nk3euxJf2TY57jom2EDXuDbB9Cpzj2jUs/MI/3IE//vwBvOa938VaDOkvHac8Pe26jHZ3o+QFDl3t\nIfL4RxZaUgxelIrMtPvM95aJPD7IOR5IbkTnbGNVitYaXNdpMO3086vHXdO/uH3tXqY9ZdC+50kD\n/wrYC3Nf+H+Gksh3FQWIxxeCFm3l23mj/jNEGbKNLaQG2mlryjQjY0QSph2QQPs2LMZKJqHlMO0b\nRnTp14OD/1+qvsEYKwHYC8AA8HDE72wD0ABwjHMe7y5zIRd1kGdr6AT0ePU8Uso8mPaAnnbNgLvW\nNUZuRFczxDrRSU1PuzOojmRxAfBxj/du57qHdc2Zaa9Ooj82L7/mw0w41TUsNHJWgzi5xP9u3iRe\nvPfDgd9p9yzZiC5tefxWAdqvYEfR7nSirTz3yXlQyvic1PS0A+G9aLYz+2gj3+bLYnv1C3Mjco8H\nJPC5p25vm2lxT9qGPQ5lnCWv62nf3IAzj3xksRXJ5dewuMR8sJzc48dJrnycySqdOO8pEwY3LvCg\nTLsb+yaOURryePrvmilSs60Y4GSOsMcK0x4XdLQkpp0ATjWjHfAs/DFYuP3wkviYRh4PQFpUuPeY\nfb9u9kz88ER0vsZXxg8E97QrHgC6alRL2DJQVBgWj70PpZ52hWmvlsLl8UtEHj8TRx4fxz0+kGlP\nAbSXquL64ZYrBaexb+eigHaTyz45aav3qhPAs/9IPL/jfbixJpQBcVs4YmW001Y+P9AuMdjLaHaN\n2OaSuqIS/kmkoDokzvdb2OLQTPuGPD79+s/B/5+lee8pAMYAfJNzTq/KoO88W/nMf42SzOjWQ3va\n887A9jDt4wKEn9UAzaOLLVRHHPlW6Cy5g+Va1/C4DtuDCVec2UfEtDvyeC3TPiLDL6cYw+Jz/hr/\nbj5BvLZyLDAH3V5Yovs1e9Du5OB+yrwZJh+gkSPfAJaP+n7HE/mWphEdYLuID3opq6yPi9kJLDXD\nWSRmkPXKrK8dIm1tkMWhMDO6rmHmr/gBJNneLAE3fmqa3JM2nCIKFQe0A14Zf8+wUIGBIhtcT1lk\nyWuY9lq5iN0DJpFz4KGz4Wy7aXGUmRILlXapTDvnEtMeh4Wl99GdRQLa40p8aU+7I4+fSlceT/9d\nM1RkGGchkQKq9dMeCXqcoqC9Tk3Uzj4oHm8eMO2FonRt1dHDHUcEaJ8b158n1KGbViArqVRg5FtQ\nT7viAeDXSjWMRF5yj/cw7eHy+DhGdPTf3jOigznHhbzCdEx7CvJ4QHGQt5366TlxNoI83rA4JiSm\nPUIMYty68oXAJc90n95kiTbAuKA9cj97e1kYO7IisONx+s9R0I5FGBZP7Mwu/XlynU/QyLck8nhA\nAu3b2GLibXTGH4/XwqOsRgHa/wXAOQCvYIy5ZxtjrAbg9wdP/0r5zvsAdAG8iTG2h3xnBsDvDJ7+\nn4y29/wsRUoZlNNuGH1UB5IRDpYPix0j8o1zjsMLzZEz7ay1iIum/M1uTq10s58sB5VGHn92vevJ\n/G12jdEBj0HV9j8Jv9T/VSzzwTYbHWD9jO/ne6YlAcA8mPZauQDGgLOYxtctEbeG73/E9zudnolp\nlqERHSBL5NlhD9OqK2ZQpi3j4y31tIu/Gw7aR8S0E9neFJHzaWMdPdF5o2Had1XFdqpSam+bQQbb\nqOlpB9S+9nDQ3uoZ2U+iymPifmF0gH5LNqKLIY+nJkhbCgQI0xzkKEVNqtYGRnSTKRvRkX/XFIkJ\nlJzrw0oxzJPl8XFBu9ieMQd06JzjnfIZRwBg84T+/k+VALTiqCkCI9+Ceto1HgC6kmLfYjrIB7rH\nEyDnZ9a1FCPyrZqQaXeOs6T0cdRdKtNem05mAEcXk1bsloW5EDNjtUzLks+rtI3oALt3fN+t7tNt\nTCzkxGfaxTGoVwJA+7HvwbXv2nq1/1xpkjLt9oJYGhJ5urgwwSnTPjxo38oWE+e0O34QG5FvEYox\n9pOMsfczxt4P4LcHL9/svMYY+2Pns5zzVQA/D6AI4MuMsb9ljP0RgLsB3Awb1P8z/X3O+SEAbwEw\nC+B2xthfMsb+BMC9APYD+N+c82+l8W+5YGpMkccH9YGQSVe/UPM6TWZRdNWtvYzNDTEIqQPu6dXu\nIPJtBO7xpaqYPHATl0yJ/agOukcWm6NlsMnfmyrZ+8qWz8qrzs2eIffd521EB9u9mTHgGN8sXiSm\nMmrZ0ul8QTtjzGXbP2Y+Sbxx38d8v9MxTDmbNOnqclARY5lNbCXUdIdzjoIhQADL+tqpeN3jgeAE\nCyAnsKkrIo8f5+LYHfdpgRmZ0SRxkN9G4idPK8ok2ywvY1WSRh4PAJcQ0H4wgoP8etdQjIEyWORk\nzMO2j9Oe9hgTVXoOTxaGYOkk0O7taU/biG7SEiy122cbpZRoOtmILl63YUuSdg/u99Q5vjIus9c+\nbTaAP9PuB9rjRMD1/SLf+m2bQQdsBlPNvgY8HgC6oqA9roO81NMelNNumPjcfafwU3/9bXz0TrHQ\nEEseLzHt0fefcz1pM7prynWiqhWilgTa7X/fVkKonI6w6NXuZWxE5xQBnbPWgvtYd38JKkkeXwoA\n7bSffddN/p8jTPvWAWinngdJS2qDsYjqMOlcSNrOxcTu8W7bhqQA2QDtfnUdgNcO/nO0IvvIay+h\nH+acfxzArQC+CuDFAH4ZQB/ArwN4Bdc0XnDO/xzATwC4D8BrALwBwCkAr+Ocvzmlf8eFUxS0h8jj\nOTGpMoo5TUKLJXIRc0yj6Wa3rnYMaXudG9tImHZAkjjvGxPboA66jyy0RmvwRiY5syWxHWpfe6tn\nKlLznLcTQKHAMFEtKaD9iO/ne4bpOpEDyCfhAHBjZL5GmfaAxYVO35Ild+okJY0iAGGctUMd5G0w\nLD6TOWgv1wFm3zoq6Lvu4GE3236viwqzr3vOivmpVAhor5liknFcA0q6Rg6A2K9IT+BcicjjlRaY\nrpGDsSiVx3cJaCexbwcjOMivdVTQnoE8HvBEjE6QnvZYTDtRi0xSIBl3wj9OAN+6DdpnGxUXKK11\nDAmgJSm6GDHWJ1JtxQg0sBoUtJ/FpkZFtIh1jEhxnU61dCwxdY6fu1QmDDTRke5HfXva9ed6M8bC\nDGWVpci3weIKABtI6Mw8qQfA4sPe9wHsnRP/rkOx5fHRctrXOgbe8pF78K2HF/Dmj9yDg6cHxpWE\nENnks/DhlGxEF10e7+xrKfrUYVnV60T1BYhak9SMbgDaYy56rXX6GJfmFBkl0kwI0D7ZP+s+Hk4e\nHwDNovSzA/ZC5mC8nWQt1NGJfT7qytlOBgt1Mw2mXZwjW9lSoGo4qJquQSLFERugXVuc87dyzlnA\nf3s03/kG5/w5nPMZznmdc3415/xPOOe+R4xz/inO+a2c8wnOeYNz/njO+QfS+DdccKUa0QVNmPvi\nQs0NtAMS+1HoLEnyJmokcmQA2kfS7woAY2KyvHvM36jqkcXW6LYRkICsw7QDwCfvPiF9bN3jHp8P\nAFZraqyMY5xMIFf8e8WZ0UFh0HbAi7Vs3c9JOdFEKyD7qLMCWPrrqd0zs3WkVX5zAu1QB/luP+d0\nCMa0bHuYgQwjagBeruej+AEk0F7u01jHjseYxwuIc1SpkEnPTEGM2SqrtNLuZ99m4COPpw7yUeTx\n611Dco/PTK4oGZ8uSpFvSZn2iaFA+zzcJNrmOcDsgzGGLbSvfUiJvDTJ71HQHkceTz7bPAvGuSSR\nPx5DIi/J4x15r8453ilimEfBVbnIJIBGi26b/LfjMO0+Pe0d0mLgZ6pF2femvt1L6mmPLY8PYtrF\n8+8cWnTTAywOvP3T94NzLrX8bJsKvg8kjXxzJPzTuuhTD2iPaULnfo+AfQ3THuXaWesYuTPt1bZY\n+ImbENGJ2tN+ksS9BYF2xqTzdQtbijRmh5WTEjGODgoYnDeV8eSL8Io8PinT7ozz46PynMqpzsec\n9o2KUmSCN4FW4OoU6wtGycyTwQ5ykCcrwofO2dsnyeNHxLRvr/obVR1ZaI22V5z8vW1jYmB7z1cf\nxpceFBOI0yud/DK7A2qyVsZxCtoDGOyCISY3PCeWHRCTSwMlmO7iBge6ejfijmGiQScCcZyaoxZh\n7ydZKzTepuNhh3M4L6n7swvag2+2Ut99ntcOAe2su4JGxb7ttfumRy7YUXPa82wtIUBhkvQKqvL4\nY0ttJcEig7Gy6jWiA4B5kjUeRWq53uqixOzzwgLLbjFOcZBXjeiiuibTxe+hJvzFMpGpc61E/uSQ\noJ0C1WqXgvYY8vhSVVwf3ATai4kl8m2JaR8cZ8k5/nL5CzT2jezrlz5up5zzTsqXaY+oWjAt7vqh\nFphiYEfHfD8FlaJM0NWu2TE3ZeHESjtWj24g007Y168ekP/2lx48i0/ec8I9f8erJUzW9PvQKcmI\nLoF7/IzOOVy9TpKY0AHanvYtcZn2bl/pac8ItE9shbNAV2iedZVny+2+x28oqCKB9n4b6A4Wl4qV\ncK8NRSKfDmi3z5XU2gTJNs5jCcvryULAnGsnc8+hEdcGaL9Qq0on9s3AyLcCiYMyi6MxVkJrQXb/\nXPMy7dVRyVLJhb2lRIyqCGjvGRZOriiT5bx7xckkZ2vNxK2XisnZb/3LvegZFjp9EydWOnLvcI4g\nmNZUvRypp51zDrMjBlqW435tkImRUSYTtc6y5tO2EV32TLvYjgm0QuXxHqCZx7WjSTIIm5wWJNCe\nZ1Ri3V0EZGYP+6bERF2NZDq71h2NWR4ggfaGJQCEyiodXWxlb5YnuceLazMug91ui/HUYOXs1BXK\nvaZaKrgmY30zumsyPYfHhr3OZ3aLxwMp9U7CFMd1FleLOo2X2+fEG3FAO+Axo6MO7XHM6LQmakFM\nO02hIOzYbz1TAfekpupl6Rx0ioLdoKJqII9zfIeC9iloS1Im6Jn2SqngKgI4txV6UYpzLi0+jAXk\ntOvq7Z8WCyTbpmpgIddaUqbd2ddTOnBUVfZb0p72SW9P+6ZGxb2mV9r90PtNbkx7seyeFwwce6v2\nfjEtHqs1RzKi8zvW1My3sTl8PFUc5H8UIfEjrFr9gacBdY5PKo0HgFIFvZo9fhcZx5lT/orMoFrX\ntm1sgPaNOl+KrGxNhjDtBRIHZWWd4UwrwEH+nV844EYGHV4YMO2jiHwDpAt7c1EMRD84voqVAZt0\nfLkNi49Qwg9IQIn1W3jny651F0LOrHXxlQNn3QnCSHvvBzVZiwba17oGyhYxUsuq90xT1KW1VyGg\nva0H7Waf9GUXStn0TNUoaA+Xx3dG4Xhejce0c85RNKlZXs4LXmQsunRC7CtVTXN8qT06IzoC2uuG\nABAHz6zjKJn4H8tjG33k8fVy0WURu4YlSY111WmLdoR+IcN9qYB2xlgiiTw9hxucgK0kY5LGafyS\nLaS94HS4J0BQtQhoKUqgPUZPO5Cag7yHaedcn9HuFHWPHyz8vfNl12IqIKqMMaaNfYvKtNPzwAP+\nqTzez3iQLoj4MO1AMol817DgELOVUsGzqHCRT9ydU9T7IuyzgOIeHwO0C3CkMaJLSx4/Nisc6Xtr\nQGcFhQLD/ER0ifxax8inpx2QGO+La2LsXmwF37tp0evHt6e9Sc65KItzUgb6Eh46sz50VrtDEEqL\nNkMa8hbIdrbOHYUZQ6HglOu1oGvbeBTVBmi/UIusBE+wFjp9C2dWvT2agAraR8McobWAm/aJidUD\np9bw8vd8C0vNnsu0y0Z0o5ksTxeauHyrfeNp90388+020HT77kfFwql/r9/EpvEqXvo4sZL9sbuO\n4fCAvRkbFfAgNVVX5fFHtVnti+s9xYQuR6ad9A12S5RpX9F8Gij2yQ2hMpENc0gmPuNR5PF9U5FL\n58G0k552Fg7ae6YlnZOZm+WpRUDdvoY416i7M+ccx5fbcvLCiEB7qbuCJ+63t9m0ON7z1Yfc944t\n5WCIWawAhQGosfqAYf89xpgkXQ4zAOu1CGjP0k9FzWoHEpnRUfauzodk6Wb3i8cDpl1y3x9SqurI\n4+voiBa4YsWfJfYrBYgmkcf3TcuVWBfYABA2zwLtgau96hwPSPv0uZeO450vuxYvemw4yNNJ5KMy\n7U3yOY8EP5I8nuyr5hnt/QxI5iDfDMhoB4Afu2ILds3K98Z9mxuuwS+ti6bDW2ZkI7o4Pe0DeTwj\ni06+oD2hPJ6x0L72sPYS24guB6YdkMzo9lbF3GExQlyrU5R48418o0x7lGhH0tO+lS1hvWsMnVzh\njDupMe0AStPiut9kLcROXQB8vBY25PEbdd4U7XtFC3ccWcIT3v4f+I0P3+P5aJHIUnmusnN5IvXC\n67fjbS+40pVlnVvv4W3//kMx+WAZ92n6FZkss/YyXn/LHvf5B755BIZpEQb7/GDaHfbrRdeLG9sX\n7z+De4/ZN4yapAgYjTx+sl7CKhpY5YOJhtG2TZmUWmz1XOAHIFc5P+0b7BTJTV0jj+eco2SQG0IW\nua+AJDGcQLh7vEcen4cfhNTTbo8v3QC5YrNrjs7gDZDGosdMif35mR8I46Bz673RxdIB8iJnewlv\nfKpgaj98+zE3JeLYUjv7fcmYb+xbHAa7T0C7meWCseKfAijbGRW004kzZdqTTPg3EdDuMu3iujkQ\nITLPr0yLu+zoHCNgM4pkVi2Fad9JgOHRiEy76hzPGJPz2Tdf5t0ucn49eXc9EmAHbKCqVmSmvRPE\ntJP96Me0VyfE+Gp0gK5eLZEkq13rvk+qUirgN58lqxWedPGcB8gDwEUhJnTO7zkVr6fdYVopaBtc\nf+qciMizY5emr516QoQZva11DNl/JkvQTpjinUUxd1hsRk9foEx71S/yranI48OKLCbMD2Lfhu1r\ndwwwU42+JZny29giHjwVX4WkTzXYYNo36nwphWl36qN3Hfcwc1SWyvOchEoTqSUwxvDqm/fgnS+7\n1n35o3ced7YMVcmILk+Gi2bKL+EF123HzECid3y5jf/ve0fxiFbCP7qedvTs7blkywSu3m6fCz3D\nwl9+2Z4cni9MOxCe1b7U7Mnbm6M8njIarQL5uxp5fM+0pJV7Fje7OWpReTxrhcvjDWsERnSkp31w\n7IKY9ofPruevBqBFJMNP2Mzd3si7Hll2Y3Cc/vaRmU1WJ90oPfTWcMveSVyzQ1zbH/zWEfRNjbdG\nVvvSp69dZtqDGU6zQ0B7OcPrWsO006z2tW60yTPtK61Zw4J2Ko+3lRI7ZsZcWfK59S6WQq5tv6JO\n7dvLZJIaVxoPAOMye0xB4NHFViQ5LQUcLkt49kHxAdWEDvA9v8LqtU/cg8fukkFCVPf4QHl8N0JP\nO2OyGV1TL5GXQHtE74Ig53innnv1Nly3U/zbb7t8XruIsS2CPN4ZAwGgH8c9PggcMQa89APA7luA\nF793uFhPqa/d7nOOY0a33jXkeNZMQbsMOp2Kc33T+6c/0x5THq8w7UAKoH1wrUlGhMMy2oqD/FCg\nnWnaNh5FtQHaL9Sqykw7rTuOLEnPS6Z4P1/Q7p1IAcBzrtqGq7bLgKcME0UnPqJQsnPe8yp6YXeW\nUSsX8cobd7kv/feP/wB/+/VDAEY4oQfkhQyjDVj24PlCwrbb8ys+Whn/oCZd0E4l8t6s9oVmT87p\nzXF7x8jETQLtGqa907NkGX9mTLtsRLfW6QfGqXX6Zv6SbtrTPmAzgoyBDpxeH61KZUycgw1zGU+9\nTEy8P3aXvXDoxFvVR7UwVyhIjAXrrOANT9nnPv/cfadxcrlje2vkMQ759LU3YjDtBgHtVpb7kt5r\n1k4CgOSeHVUeT6+zqjlkT/usOHZYOgSYBooFhouJRD6pMRQFqdtKFLTHiHtzSmLaz2BmTJi9rXcN\nLEVICaCLCI0ozvGAPH52o++HHTNj+Ogbb8E//byIu4qa0y6BdtVdnY75fvJ4QFnkCAftRxajyuOD\nmXbAbk/585+6Hs+6cit+6Wn7ceulm7Fvs/fcjCKPT8K0mxZ3mdYZv97hK38SeP2ngatfEuk3fYsy\n7av2GL0tYuybYVpo9UzZiC7TnnYxB9sMAdoT97RHYdqjyOMVMAykx7RL7RF0/E1SZIFmBzuXCLTb\n1zbHNDRtG4+i2gDtF2qV6+AFGxRVmSE5r9/uAe0jilryAe2FAsObnyHLvMYKI2LZAVnaM+jBe8NT\n9kv9h06NFHgUCkpfuz2pfP61F0nKwyr6buY5ipV8F0BITQ56Smlfe39Rz7Q32Gjc7inTvsaCmfaO\nYebjRluqAoNru8JMVNHHiWX/CYrd057zYpI2p91/4nfg9NpoF5I0rTpOffyu44N+did6coTXuCKR\nv+3yeXdy/eDpNXz7kD2O5mI0KTGhAnhMxADtnIKxLCfNM7vdawYLPwIe+XYyefyAaS/DQIkP9jEr\nJtvH1XEhD7YMd8FS6mtPKJGnoH1rSZHHxy1FHs8YkyTyUdzPWxLTHsE5HlAWWk553w8pmvwRlWmX\n+saTyOMBJfZN7yC/darm3pPPrHUj9Yy3IjDtALBzdgz/59U34C3PvByMMeyb894vo8jjq0Xi5xKR\naRfbyGWmPYveYU1P+5aIoN0Zl+RF9ix72gXTPmOKhZw4TLsc+eYDzST3+Hg97fNYAsCHBu3OtTab\nJmgnSRs72Bk8mMCks9UzMQZhFIxSfWQq0yxrA7RfqMUYOJFwUbb99sOL0kdL1LU519xhMpC35W26\n9dLNeNPTLsYl8+N41Y278PE3XC/ezLOfHVAmyjZYm6qX8S+/+ETccrE8GI2MhXNKI5HfPFHF43eL\nfZ0LCxehpgctBue4OE8PHPbGeSy2eqhj9D3tawhm2ttq3FtWIIQxj4P8e77ykO/Hu31rpEZ0jQhG\ndA+eWhtt8kKDXMfNc7jt8nk3y/uRxRbuProsmPZRyvgV0D5WKeFmYuD5D9+ygV8u+9Knp50CijCG\nk5HvZZoKUZsCrnm5eP6Nd0lGdJHd4wcTZznfeTy54aTOjI44yB9I6CBPAd58gfxGEnm8xhF916w4\np+KCduEcT0C7jmmf2SMeLx2OurVuSW0aEXva14aVxwOedgJdlYsFzA/ScjgPdzoHojHtutIx7dSw\nza/KJSKPj8i0O9tYRxcVNtiXpVo2Y1BIT3uQPH6tY4DBwjjLyz1eLDBM9gRoD/OjoRXJiI4qO8Yj\nLNBVGq5HToWZmMfy0FGTzuLCDFIE7dMCtO9kZ3F4oRka6adWs2s86vvZgQ3QfkEXq8lZ7U59//iK\ndMLTOK3zgWkHbJnXm595Gb7w67fiD154NfZOkUEqb6ZdmSg7NVUv4wOvfwL+8MVXu33j120VWfMj\nWcVTHOSdesaVgi0ZqYSf1E37NmH7dB0rEJP/7tqC53O2e/yImHYCQJY52VejZNoBWSLPWvjw7Ud9\nJ/gdQ5XH59vT3nCZ9iB5/JqysJA3007ATGsBtXIRz7xSsBCfu++029M+UsM8zVh02+WCUfn+8YHR\nZB770odpjyOPp2NUoZaxV8UTf1k8fvDfsYsfc5+udaL2tNvnsHydD+FdoTGjk+TxCVkvKqWVjOii\nSGbVUph2AJ6+9rBqqfnivXWxUF+q6aO/ZvaKx0uHfZ3Y/YqO3dHd48V2Tnjk8SQxJAi0S0y7f+wb\njV1ToyV1pW0xiFD7lZ72ufFKaKY7AFSK8SPfXOf4PMCRpqc9qhHdWsdQEmnGbaViVkV62se6ZwDY\n5/JSLHk88dPwk8fHZdoBYO4S9+FrS5/D2bVu5EVMXTkLdJvouDMsaJ/YZqtCAWxia6jzTmwV0nrX\nkFs2HoXO8cAGaL+gi9Vkl2mn+iZ3XcQBoGxR86wcCqdjaQAAIABJREFUncTrM8JYqb3kxgZpq08X\nFvJm2ok8vrMsTR5KxQJe/vhd+NQvPwkPvO1ZuHUvmXieJ0w7AAmASKAjT2WFUrVyEV9+y1NxzSV7\n3NcKnSXP55ZaPYyNSB5fJ4zGMid/VxP51u6ZitwuIyM6QGHaW7A48M7PH9B+1OMen3NPu7NPPnL7\nMXzlgHcSe269i4Vmb8Q97d4FxGdfJa6Zz/7gpJtJPdLtDAHt7sfy2EafnvbxGJFvBfK9Ytagff5y\n4NJnu09vWPy0+3gt4iTVkcenlu9Mzeg++9vAV/4Il86J+9vBM0mZdgFSN4GMVUnk8Q15QQumIYF2\nx4Q1qDwZ7Sq40CkV6tOiNc1ouwsGUYuy0Unc4xsqmx1VHk8XRnyYdkAG7d94aAGvf9938bZ/+6Gv\nsV+T7kNVBRBQs42Ka/qq/t2gKhaYe1gsjkjZ2Pos7IzAEZXHr54ALBPzk1X3pTNrXd9t9sS9ZW1u\nW2m4Cz0Fq49LmK0MCDORpUWZ9pov0x6zpx0AbvpF9+HPFD+LLVh0o4GTVDsLpr1QkCIhd7IzeCim\n30eza8jZ8RtM+0add1XVM+0AcPsRIUevEKa9kCfQLJaUVemAvjUSS5dLZBWtUlUAcMvwdbKtlYsS\nUB4JIPaZSNMexJG6dCtVLhZw6R5h6lfqecHwoseIbjQ97QsWOZ46I7q+hQmmyGazKoVpB4BvPnRO\nO+HreOTxOUe+DSSIi80efuEfbvewSgcGpjIjyz8HZGAyiB285eI59/gfXmjhAWc76b7McQHJ/uNe\n0L5zdszjr5GLsoL+20m0VZyc9qIhxqhyPcNFLqeuean7cL5zyH0ctafdUYuMY0jneKco0w4AX/oD\n7L73z1wH+dOrXZxT0l6iFGVlZzgF7Qnk8cUymXRzoHWO3E84ji6ET56balxZVBnvEBL5MZr80TMj\nudwHGtFFyWkHlHYCf9C+nYDnd/3HQXzpwbN479cP4WsHvbGnANAKyWn3K8aY5CC/LYI03vleXLZ9\nXQvaMwJHlQbxhOgDiw+jVi5itmEzsqbFfa8dO+4tp352p0gLyD9W3o6L2bF4Pe2SEZ0GmhldQSaw\nYvTFkitfBGyzE5vqrIdfKf1r5EQDXbnu8VJPewoLNzOyRP5IhMVCpyyLo9U3FQXIkDF052ltgPYL\nuXyYdsCOMnJKAu15Mu2A5F6J1RP+n5OY9hEATY0Znbb6ZCA5j+TxAPC7z7XNfmR5/Ggy2mmNT4uJ\nZM3Qg/bGeZDTvmiQ4+kjj2/kFSFDQPvmsn3jX+0YOLfunQS0eiPIQCeg/Um7atg86N/s9C18XZmU\nOqYyI/VakOTx9vbVykU8TWGxSzBQdoxsCqXhIouSlE+rzqtv3i1/LBemXS+Pl3Pag2XJZeLAXq7n\nMHEmzPZUm8rjYzLt0oR/iMU5jQFb4c7347ptAljde8w71oQVZdqnLPL9JEw74JHI75odw152Et+o\n/je849TPAWvBLHhblcdHlfHOEon84iH/z2mqXCy4Jo2mxSOZqcmRbwow7kTtaQ+PfAP8wbPT4qKW\nZ+EjRu2bE+fotggmdE5JDvIR9p/ThjCdFziaf4x4fPo+ALIzvp+E2hv3lkOM7NN/zyWc5tky/mfp\n/en2tNNzrTEXXe5fKAA/9j/cpy8rfgUrj9wXebtocW6nB5RhYNIhMFhx+Jx2QOlrP4MjC9EXFtp9\nE5znpAAZcW2A9gu5fHraAdn4pMrF42LtPAXtlGkfBRjWmNFpqz8iJ36nfOTxAPC6J+7B7zzncvzc\nTUL2O2qmHQCmZsQkZ8z0ykEXVen0iHraz1LQro18y8mIDpCu7b0T4mauk4w1u0b+5mnkGM2U+vip\nJwg1xUPn5G10evFHqgCpTwMYaEE7K4Bp9zg/i0jkAdWEbgTXtx9ov2k33vaTV2FmYO44WyFgeYTy\n+PWA/HPT4iiT5JLKWA5MO+mTbrSOojCIEY3aw9lxmfaUFudm9wFP/g1gy1XitfYiXjZ+t/v07qN6\nEBdUFLRPmmSROUnkGyDfpx/4d2yfqeNPy3+J7WwBu/lxmF//08jbM1YpKjLebJh2QGakozjINyXQ\nThbk+h3AHNyDCuVgtV8E93jAX6bu52MgMe0B7vG6unGfACjX74oOoCSmPYIZndOGIGVhZ9k7TBe9\nztgRgtfvFGPkdw95PXIAWx7fyMt/xqk9TwJe9RH36bWFh7DaMSKb/EmRbzpPgiT97E7tfxpObroJ\nAFBiFq458K543x9Uz7RgWkqs2thsOn4BKtMewUvDKee6ntowotuo87rI6taEktV+dk2AoCoXjwt5\nrDjSisy0U3n8KED7BcK0+0ykAbv//g1P2Y9nXkJuUCM0onNqek6wOBN8XepD65sWVj2mMaNh2s/0\nFaZdkVuOyohuV0NM5h4+6119bvaM/FlsOo701qTYoUNkGw3TwrcesidWo/ItAAAUivLksmW3Dz31\nsnnMjQtzyZHGvQG+oJ0xhlfftBtffsvT8L7XPR7XbRW9nZkdb+kYU/d4Ko/3B0rNniEx1rnce2qT\nLttcsPrYBvvcix75NnCPl5Q/Q17nT/894Be/ATztd92Xbl39N/fx3UfjM+3OBH87zmLcHHyfFZL3\nll5FMrW/+ReodhZwbeFh9yXz4H8Efr2pRr5Rg7YggCGZ0cVj2gGlrz3Cwsy6HzBWpfFBaQERctoB\nWR5Py8/HYBim/UXXb8f/+Ikr8bYXXInnXUPmXAsPAYe+Blh64Bg3q73pMu05ZWFvuVI8PmOzw3SB\n4tuHFtVvALBVafICew6gHQD2PNld8GmwLhpoY7kV1QRT7P+6DrSrTHvMOv2E33YfX7P2VeDo92L/\nRmdglpdq3JtT0/7y+B8cX8EPT6zqvgVAXNczGz3tG3VeF418Yy1csU1M9BeaXbfHi4L2Ui1nEEfy\nK7F20v9zEoOdc087IF/gGpbVrVEz7QHyePE6vVmNHrSX61MwByznBGtjYVUMrI676tiI5PF04nZg\noYd+YQCGuOnxNmj3LNmgKkvQTpj2i2qC/fVj2quMTAxyMaIj8tGlw9g/LSa5tF/uY3cdx+HBzXei\nMGKvBcVBHrBZ47973ePdnvGxUcbSAb6g3ampehlPu3weJTMHDxBfpl1cM0EMdrNryF4VWZtBOTW7\nz324u2DLulcjusc7EuvUetppXf/TtpQUwNzC7djH7EXse44uR+rHpuWwyi8vfUm8uPdWoFTx+UZI\nXfMyIUXuN4H//H3p7WZlDt87vIh3fOZ+PKwZg9qq83lUw6yhY9/iMe3rfu7xUU3oAJssGThdo7fu\nUbw5FcS0WxoDtag57boqFQt47RP34NU370GxMBiLlx8B/vIJwAeeB3znr7TfKxOmvR9BHi+M6Mjc\nI0twJMnjfwgAeMJeAdrvPrqsjQZb6xj53atpMSa1msyz5cgO8qE57ZJPRHxFzebLbsKnzJvEC/f8\nU+zfaPXt458JaFey2s+td9HsGvjKgbN43p9/Hc9519dcAsCzXT1N28aGe/xGnXdFjejQwp5NY650\nsW9yfP6Hp/Hiv/qmJI8v5860UwfQ4/6fM8gAe8Ew7eeXPN4tCubPA3k8CgWsM3HeLS2Im89S055M\njyqmTmU0FkxqRifLVjv9PJl28dvzlTDQrrrH57D/ZvcC0wNJfGcFF5/4hPvWkYXWoMfUxJ9+8aD7\n+v4ZMhEdCWinDvKi7/6aHdP49K88GX/0kmvwpicRlmoUfhAhoN2tPBYPI0S+BbGb66oZVF6LcQS0\n72E2aI8sj9e5x6d1nU9uAy59pvv0x2u23Hel3Y9lugTYAK8EAy8vflm8+LjXJ9+2QhH4sbeK53d+\nQHq73W7hZ973PbznKw/jVz50N9TyyOMlKW928vi4DvKSezx1aI8a9wbYwIz+m3wc5GfGyq7hIK1O\n33KTKmglzWn3rfs+ZhvrAsDnfkcbqReXaXeN6JBT7/Dmy0QC0eLDQL+N+Ymaq+zqGZZWqbLe7Sv3\n6hznvROi5Woey5H72ilor5aKtscDHeejXlM+tW2yhk/iVve5cebB2L/hmtCp8vg0anqP+3AnOwuA\n45HTC/i7T4mFyV/7Z+/YA1CDxJwWk0ZYG6D9Qi5qRMdauGnfJknq+Vv/ei/uOLIksUelrGN31JLk\n8ecx036hGNEFyOPdkvbl6I3oAKBVEAtMK4vi5rPQtM/NkTBykB2IAWCFxr4p3gbtvmJEl+V2kgW5\nmaLYN1rQ3htBT3uhCNwscrHr33s3tjTsiWbPtHB8qY2P3XnczT6fGStjO91do1jwahDQ3pTN8srF\nAl72uJ146TWEjR810x5Z8ZNDTzuRD0cF7Wtdpe0lr4kzAe27BqA9qhFdV5vTnuJ2732K+/CpdSE/\njyuRb/VMPL1wF7awwffGtwCXPWe4bbvkGXLvPSlj5aQbm/f94ytS+52zPU7VPe7xAazg1A7b8BGw\nI9/87mk+FTerXe5pJ8C4S0F7BO8FCpoGvdZqMcZiSeTlnPYUQLupgMVT3/d8JK57vLONucmQy3Vy\nPXPg7AMAZIn8dzUS+bWOgSmQcylsISbNkpj2peignez/ua//HvCu64D3PkOM9UMy7YUCQ39KtKO0\nTj6I7zysZ679yrnOM2Hax2bdOdU462AnO4M9H3oqPrD28/iZ4mcAAKdWO9qvCgUIbdvYYNo36nwr\ncnPZNdbHK2/chblx0evo9NJQQFQ8X93jR860XyBGdFHk8b0RLyxoqlsW52pz2cu0N0aULV8uFlAu\nCmn3CmhWu3wenFrpKJFv+cjjJ9CCo3g8ttT2yAGbnb4s687r+rn+VeLGuPwIXjFxl/vWw+fW8dE7\nhbLmF27dj9KozSY1We2eGvWiXFSm3chhHKKtTSvCiX2CgJ2g/PP1zujl8ZRpjyJBdyalmaVE7LzR\nfXilKcBeEtD+/OI3xQvX//TwSQeMAY95gfatTVw+F+84IgOlliqPj2qaVSgKxQ4ALB2Jvr2Iz7Sv\n+YH2OPJ4wDYdc+rb7/b92FYfB/mDGjM6Oac9njxeW13lb9z/Kc9HyrGZdnsbc83D1kjkb9wrxvLv\naMzo1joG5hiNQkxo0JikKNMeA7Q7LPZTC3ejfuff2C+euhe47+PAmQds5YRTCf899fl96HP73Jrs\nn8PP/s2XY6VXOHOP2TQz2p1iTOprf13x86i3bMzwyqLw1NC1QzRdefwG075R53OR1cMbthRRLhYk\n0O5+bJSOyGpPu48hihz5Noqe9qhM+4iBB2UBmhGAx3nQ0w4A/YrYv+1Vsd2LrR4Anp+xjab6ppjM\nr3Kyv5TFm2NLLYU5zFIeL67tYm/NzU3mHDisRKH0u+Kc5MVqOk6uUarSAB7/s+7TW5hgcr5+8By+\ne9ie2BcLDC+5YcfoF7w0Pe2eGvU2UkaovRwwXuYwDkkmYYddeW1keXxXNqLLTR5PtnvfoKfdtDju\nP6k3AHOKc+5OsKXFuTRNrLZe7Z5Xk91T2DowyvPr1fSrdt/AXnZKvHDps9PZvit+QvuyY6rl1PcO\ny/dImWkvRs9pB+TzbOFH0bcVsnt8mBEd51z6jCSPl4zoIriv3/gLrj8BDn0VOH6n9mOmpncdEIka\ntOSc9hSY9paSB3//Jz0fqcZk2pcH/dnTebp0U9B+ZgDaFaZdBcZrnb4M2sPOwTRL7WmPANo5526k\n7B+U3yu/+bnfAd73bOEJVaoDFz890abt2jyJR7gA/LtwCh/8dvSFspY2oz0l0A5IEZAvJa0/FxdO\nYHIAyH940mtIJ5j2DSO6jTqfi07wBj1ZTl6yU0WYqDL7hDY5A0peUJ9pVcbETdDqe28kTlHmaNRM\ne6AsdcQ97YRF8p3gjHobNcXJokh/nYD29R4m0UTFycauTIxUHRDEtB9faqUXBRVW9Le7a9i/WbCU\nD52RQbvZFceb573vLnqs+3AbE8f1b78unKCfdPGcvZg46gUv6rjb9BmHRu0HUSwRpo/Lsl2nOM9H\nETA2KwBrb93dZ1Hd40fHtIuJ357CaQA2cPrgd4Inp6sdw2UbJwtEuZLmdV4sA9tvcJ/eVLY9Hx48\nvSYZOIZVq2dK1xumdqSzfZsvk5lvUvNMjIe3H1aZdnEejBf6wsSzWAkHwVsIKDv6nVibO0bPxRAj\nuk7fgoOhq6WCZMIm97RHYNqndwFXvVg8/6Y+Qssv7ksX++bxBRi21EX9sw8AZw9IL5VLQmUWJZrM\naYuQQVvGMuQtXtC+baruRtv1TY6P3yX7JXmZ9hxBu8S0Lw+IieDqGhY4B366+EVsZ8pxay/a/wH2\nGPrKDyWSxwPAlRdN4RAX27eHncK/3XsyUvICYLcIAqo8Pr6TvW/tFT33k0z2fbhmkGZxj0aVZG8/\nz3cxaUS1Adov5KIyroG8S2XaqcFXC7XgKJOsSjKj85HIj5ppj9LTbvSEsUuhlNypd5jadIl4fO6A\n/jPnIWhn5MZuNsWEb6nVwxwjK6cJokyGrcfvEYP7qk9PO+ccZ5ZXUR4sLvBiJdsFMDpx7K5KkWpq\nXzsnx5vlDTRJ+8uMoQfCL7hu8JlRn5eR5PGUYR2RHwRV/bQ0kUa0lahYtSXGWRRjWqMwid3s+cvO\n1zs92WAyr/05NuuO5xXexTzs6/jjdx0PdJE/ty62dbqYYS8+kci/ufYpPLvwHZRh4LM/OBXwJbn6\n3RZmB6ySxUqJJ/GeYkzaPlrOfgSAH5xYlSTxFHBOGOScbWwOn3PsJlLzI9+ItblSTnsI8FjrimMv\nOccD8eXxAHDLfxOPf/gJ2yjtR/8BHLvDfflNt13sPn7tzUL6+6Mz6x4WvtnzUQEkLd0Yd++HpKc0\nWiyKjPvsehdFmNhElXFZA+J5Evs2kMcDwEtv2Ok+/sgdx+g3bNCOEcnjx2UjukcimEx2BwaYVxUC\nYg9LdeA1nwD2PTXxpj3nqq1obLvUff5/l/8Rn2e/hIf/9a2Rvq83okuRab/0Gb5vXcdskureY96F\n7PWBf4ozP0N5bDQ4IofaAO0XcmmY9rkJGUhSSVu/MKKTeJJI5P1Au8S0jzjyzQ+0jxp0APYCiPO3\n24v6Sf2oJb6aKjfIajzZv8eWWvLNNa3JZ4z6nedcgdsun8f26brCtIvtWm71UaQRcFmzhsqC3MXz\n4u/dT+RhXcNEyRJgg+V9vAnDV+94QUetXMAzrhxMYs4rpt0nX3nU2whIk77/v73zDo+jOhf3e3bV\niyVbknuv2MYNMD3GgdANBkJIo6bRkhBIg3SSkHuTG34JCaTcm1BCQhoJBBJaaKE3YwwYY4ONe2+y\nZXXt/P44sztnVruSbO/OzPF+7/PMo93ZXenV7Myc9p3vmHPJUwTpOGC099httBfFY6kliRwn+1Jb\nbS1NxJRumHTE8ti5kAkjImlunW5sNrd38fcFGY6ny1YjuVreRtoBRnrLLg3veI9fltzE14r+wENv\n9pCoNY1K41rrrBiU22P7vi9BrPv8+IHKu293JRxeW+014s0l36o6jfKzLw26kUeCuyQoGxZ1W7Wj\nJ/xz2nseaTejQro1itPXae8Lg6fBODdM2UnAz2bB78+B334A3nkUgPdPGsi35k3h88eP5yunHMRA\nNxKyub2Lp5b570HNbTkeac8U1bjw99DldV6YywQvXN373OYtu9sYwK7UdU1F3f7nUuiNAWO8OmHT\nxlSdZ96MIan70JINu3hznXfedA+PD3JOuz88/o11vZ/PyRHs4cr4zo77qv9N82+G4Yftl1pRPMZR\ns71OuWFqG8PVVqYuu9mfh6IXzwH5irToP5pE/cSML82K6UZ7ppH25vaughhlB2m0241ZmWjfDYku\nGtJG2s0bV0X/wYSCmYxud19G2sNOAJXlJhuFCn0sBnXjvOdb3+n+nggmoivt5/XGxtr0TTeRcHhl\n1Q7qfCPtAYaxucwa2Z9bL57Ndacd5M8eb4THr93R4lu+SuV73de0kfZpw70OOrPQCmW5N5OKOj3a\ni557Xxvzh7TNmz5UJ3xKJMKfAtPPCCHeuTrze8JORAf+6zvTFJggOw99I+3eKFBVqVdRzxZa2d7s\nVew64gFHLRghtRc3eEsb3bcoezLUrU3eSGNVPleJGD6bVCPV5ZT4yyxauzO12kJvVLV5Feyu6qE9\nvHMfGHgQXHQ/zPspiRkf83arnb5s6Oa8dl8StQ6jI7kvjaXyWhgyXT92ErC67yHy/uzxPY+0m8u9\nVaU32vdmyTcTc7Q9iZOAR74OiS6UUnzi2DFcc9IkKkuLOGuWF3X4p5e9e1B7ZyI1NaMu1kTpjncy\nLtG2V5jh8e49mqZNsPSB1O5DR3n1nldWZRgAMH9dWyfN7V2+aRK+DsZ8EYvraRtJ3BD5fmXFnDLV\n+/v3uCHyjuPQ3tacCq92YkV9y1OQK4xjMiG2jhtaf8Cef3wpe34SvORqw5XRkTPz4zDjY/r+c+L3\nYNq5ufEzyxeXGA57lj3V60fzPqcdiBnLYprMjL0LOKzYuofGFn/EVFNbJw1mJ02OnaKENNptJhb3\nj8i17aI+bU77IKN3vKx/jgv3vlLdhwzyZkU0lJH2PoTHR6FCD72HyEcwEV1FjdcYL2nXN9flW5rY\n2dyR1mgPPjw+ybDacrY7RmO8aVPq4bqdzVQHNZ8d3KX63Ip9RzOT6stSowrrG1vZ7C59sqetMy3R\nZMDnpVK+TrnLD9Xn27iGSr5yyiS+O98NbfStDlEWXLI8k1ovnJJd6yCRYWQuClEqZqVq+4rurwfZ\neZiejM6lymgsZcsg39HiVey6igI+llPPTj08aMtDXBR/mBuLf0njundo68w8ImuGx1c45rXex5HX\nvlJe60vgCDBEbWeU2sSDb/RttL22w2u0O/3yUK6POgoOu4RYnRexMKa0icvneuem2chrMRrtZe1G\nY7GvYcmj3+c9Xvl0nzUr92JOe1O2JHSwb+HxoOfgDpnRff+Wt2HRH7vtPu8w7x702JLNbN6t74vJ\n0dh6Gnmq5CrUL46AV3/Xd490ujq8fBgqBkd7y3Pyyq2pDoFDRvVPzV54a/0u3zFKJ3l9NPga7QGN\nYGfIIA8wf6bXCfKym2ehub2LOsdrwKnKhmDLm4o6bxlD4MT4AioX/h+883DWj7R0dFFKu9fwVHEd\nVXn2L+G6tZk7h/aVuvEZd+9Z9kTG/SZ5zR6fZELmRnud2s0Ipe97L6ct87enrdPX1iEf98SIII12\n20kLkU8faff1ilZHYKQ921rtZrhqGA230hpSjaT23b4QshRRqNADmOFD2zKMtEchjD+NqhrvO63o\n2k1HVyI1UtMQ1tIsaQzrX856xzj3jNDktTta8rcMVCZiMV/lsahzD9OGedd6comoPe2dlJvLvYUx\nj8vIWXHpjFLe/t4pPHrNcVwxd7wXvtpiFLL5PnbZKC73IjkSnV42XpModMwN6GWk3ZymkfdG+2jv\n8XZvpL0vGeS7Wr2KXaI44JH2MXNT95LYns1cX3wHH4w/zTdjt/LW+u7Zh8HfaC9NGEnh8rG+/Ok3\nwrVrfBXUI2JL+Nur63r4kEetkT9C5SoJXSaMUcNzDypi7iSv8/XVVTvo7ErgOI5vfntJq9Fo72vW\n7lHHeI+f+xnc9znf+ZYNM8t6c3sna7Y3c9mdC/ivB5eQSJszbjZIq3MRHg+60/LYq73ncaP+9cQP\nutUjxg+s4jB3dLsz4aSWxXxksZ7uMD/+jFfO3L8fDTVzPnv5ADj0Yt14B1jxJPz7W+A49Csr5qDB\n+v9NOLBwdfaVc5JJ6HzltZEpPa9kyCAPcMhIL1LgrfW7aO3oCjcJHeiyO1M95o27s36ktaOLYWZo\nfL9hOikp5D4PVZbInNK1L/T60eb2Tspoo1y5AwXx0tznKhl5JC0lOuR+l1POSwkvymKWWg7Af9Km\nluhGu1HHCKutEwDSaLedtLmv6YnozOQxgd1g0zFHuDYvzvye3cZ82CBCrtKJxTLmCPDRHnJm6ST1\n5kh7hkZ7cwQaSGnEK73e2FrVxLam9lQG4jpCLGAN6itL2Rz3CtvEzjWpx2t3tFCl8hgymwmz8rjp\nTWaO8KJB7lu0np/8exmvrNxBeZhLOgLU+BNNlhXHUekVjUajMWImpgwaMzN2phB539SSkBLRmSMh\n25Z3f923BnaeOzgzJKIDf6M92+ic2Wh3gk7qFy/KGE46N76I19/L3HGcDI8voYMix21sxYryF/lV\n1s+33veRsSXd5uZmo67Lq+AX1ebxejIqv2UtWxhWW84Qd+3xPe1dvL1xN22dXlb2kniMeLPZAd/H\nTthRR+GbMvDq7+DuS3oNEa/wLfnWxU2PvcNDizfy6/+s4P7X/VF9WZd7A3+ZuTfh8QBTzoKTf6Ab\nxpc/62XT3rUO1r+m/wcjqufDs7360F9eXoPjODzsNtp9gyygG9hP/ABW996g8mGujlFZr+tg087z\n9j33M/jPjwBSnQjQfSk/k2Sj3VenrA6oTpkhgzxATUUx4xr0vaUz4fDmukZ2hTmfPUmm49KefXWI\nlo4uf2h8lhUcckKWqIOapnezr6ri0tKe8CcOrqjLfadCvJjVJ/yCe7qO4bMdn+eFxOTUSzPdee1P\nLtvsS4C6p62LweZIe66nDEUIabTbjq+huZPykrhvvtbgmHHzCqv3adhhXi/vxje6rX8N+EKRAysI\n0jE7NdYv7P66OR8/xBFhX6U+vdGeSPgbI/m8+e8NRs6AGppYt7OFl93wyqiEx8diiqKaoXQ6+lyN\n7dmcyrWwdkcLVUGt0Z7ESFjFv77ErKFeg/yfr2/gpsfe4Rv3vhlueDykRdJkGSncZSQAy+fIYG/0\n1mj3jbSHlLjTXNZxx3vQldYoNpPT9cvzsawd6d27d69PXQ9VfVj2zQyPD2y5N5Pp52Xc3bTsPxn3\nJ0faK9Pns+dzxZXR3gjzEbElgMPdPSTLA7001yACarSbZWLTJpRSvnnQL6/c7l+qrDTu71Tqa4Op\nvD9Mnufft36hXgO9B8zGd3N7p+/Yfe+fS3zVlx27AAAgAElEQVTvNadxVJnZ4xfc7h9MqDYS5/YF\npeCoK+GMm3SH+rjjvdeWPQQ3z4YbJ+kGPHD69CGp62fF1j384cXVrHQzjA+Jp0WB/G4+/OeHcOfZ\n2aMUM2GOtCc7Ec64CQ4yjvGLv4REF4cZq6cs6GFe+5aM4fFhjLQv8XXmzDJG2xeu3snSjbvTVqQJ\noa6WaeAp24AVOnu8Lwldvutt0zLfG3tbvaGlo5PRyhhgy1NZPnLWSXyp67M8lZjBawmvvntIXHdi\nr9newgpjicw97enh8Xt5DVuENNptx2zgupW5+iovg/yIEuPmFdZIe1k/b96Xk+jea9zWZKzrWhps\n0hCTCSd6j1//S/fXdxjr/PYf1f31oDAb7Tve84fg7d4AXW6YZ/mAvR81yBdGo71W7eGpZVtYs11X\njgfGzHM0xM4QYHD/SjZiZEN1G6HrdqaNtOcjZDadE77tNXa2LuXYjZnnOPrD40MYafct6Zil0d4Y\nxUb7mu6vmzk3wor6KK3yGg6JTmhM61wwHfM9dy9e7P++dup7YFUv4fGtHV1s3+41AMorczwvvC8M\nmekPu3YZsPG5jG9PNtprlRkan2fvwTOgRHcADlPbGKE284/X1tHemT1plV6j3Tu2qmZE1vfuN2ZH\nvxsNN3u0d398ZdUOX2h8RXHcn4dhb+7nH/wtnP93f4Pn2Zt6/Ig50r6tyb9k2damNl/CKvM8TZ2/\nGxbBv77kfWjaefs/uGFm+H76x3oK254t8OR/uc5FnDnTu26/ce+bqcdTy3vIp7Pwzr47mJnjk9m9\ni8vgQ3d49cCWHbBhke/7fHXVzm5L0SXxwuNDaLRXD/HqhW27fOVJcr12gIVrdrBg1Y60FWlCuI9n\nGnjauTrryghbmtqCG2kHmPNlGDOHHVPO55edZ3j7Vz7T48da2rsYr4wy3kwQmEPKS+JMHKTvi2aj\nfWpsJcXo6/jJpd7xamrrZBBmeLw02oWoYo7KuIWlGSI/JBaBOe3grzytSrsxpI+yh7GWPPhHZt7+\np+5MMDHCQ6kNsdFeWuU1lBKdfi8jwzMDxhAZjM6DGvbw55dWpp4PLTKOc4jh8aCT0a0z57W7o7Fr\ndzRThTEKm+/KPOiQxg98J/W0+o07gO4VqvBH2o1Ge2O2RntEwuPNBs7OVd1fN6+l/iFeP7557Wkh\n8mbHSE0AxzJDMjpzhDNTIrrF6xspMZK5lVaG0HmoFHzkLvjIH+k6+zep3TM7FrJpV2u3tycb7e+P\nvebtzPc9NF7ki6iZE3uDHc0dLFiVPUy5pb2LIcoYSc1nx01FvU6KBXo1jY5W38jsKyu3+6ZH1Be3\n6Gg60BEaQ2b2/W8VlcL4E+ATD3rRHcsfg+/UwD2XZ0wcaZ6HSzft7vZ6cq44ZMke/8qtkHAb9oOm\nwRk/7btvNoZlWZZr2UOphx+ZnbmjZSg9LLu14I7MyTMzYYb7m9Fr8SL/Ot8rnmBobTl1lXqgp6Wj\ni/VZVjAIdU67UjDIWK/9Pz9M3YtmjfDOx1dX7WTh6h1pc9pDGAjo6H5/AXSUQAY27Gz1z2nPd6O9\nYSJcdD8V5/yM5xPecU0se7jHKSnN7V1MCKDRDjBqgB6A2E4/Vid0vbDY6WCy0uX2k0v1teI4Dtv3\ntKeFx0ujXYgqvka7brA1GBnk6xzjRA5rpB3SssOmNdrDns+eZPB0aDhIP+5o9i2NAvgr+WGOtIN/\nXvsWb0kjf6NjdFA2vRMvorNYjxrHlEPzbu+8HGDOkQu90V7hb7Q3rqWxpYPdrZ2+0a3AlhQ57JM6\nYgJQzVu5ZEJbt7f4l3wLodGeNqc9I5EZaTeu2/Tw+K5OaDRG38OcWlLXx0Z7EB0gZsP1tbsAf/b4\nTCPtr6zcQaU5nSToOe1JymvhoNOITz6NTnRDbXJsDW8u7b7qxtbduvPrrLhRPh38wfw7GuHUF8Yf\nQZHoMUy5pXk3A5Tu6Owknt/opFjM//ubNnLQ4H6pRu+mXW3871PeyPrcMr0sE6Cj6/YmqVuSAWNh\nynz/vkV3weJ7dOff6hdSSWG7Ld2Wxj9f90LKM2aPX2900Jz0vdycp4MPhnhJ9/3Kq25PG1bjWyMd\nYFJ9MeWtPTTad62Fd/7dNwdzbnJ6WTV2rvd4uc4YPrre+79XbWsmE8lOrdDyJJkh8gvvhF/PgdZG\nJg2uTkVcbNzVyqK1jWkdCyE02odm7qxqXbso4/4NjS1pI+15jJ4xKC2Ks6H2UBod3UCO7VwFa17K\n+v6Wji7Gx4zypz5/jfYhtd70tNccb7Q9Oa/92Xe3snxLE5t2tbGzuYOB0mgXrMAcBXFH2pONdkWC\n6q6IZFQceSSpRDMbFvmXWGkyGu1hzWcH3Zs77UPe8/QQeTM8PsyRdoCBRq9zcmQD/Bl3wxwpzIQR\nIt/frXSW0k5plxuOGvR6qhkYMaCc9Y5RyWlcw3vu3KlxymiQmhn880ksBqOOTj29fMxGZhsjXYA/\nbD+q4fFRnNPemBYev2utjlwB3XkY5nKJPa3VHnTUgnlPfOteePuBXrPHL1gVkUa78ffXVU9LPW1d\n+rjv5T1tnbR0dDFOrWN6zL2Hxku6Nx7zwcyPpabBTIqtZW5sEa/0MNLescP7/repAXrp13xidv5u\neJ14THGIMa/970bG+/m1K733Zpia0GeO/2b3ivdD18IvjoJbT4YfjYV7LmdwucPgftlzTzzz7tbU\ncnTdssd3dfhHPjMt3bYvFJXqAYB0nERq7XSlFJccPTr10rHj6/nTeWnXckU9NEz2LV/Iq3f0zcEX\nHp+WJ2bsXO/xmhehvZnRdd71+d42b3pIZ1eCN9Y28sTSzal596Es+QZw6EWpDmxAh5qvfpF4TDFj\nuL/eEGr2eIAZH4X6SThl/XlceZE0ixdmTii4vrE12PB4g1GD+vNA1xHejtf/nPW9Le1d/npQHkfa\nLz56NEUxt80w7NDU/pNqdF0i4cDPHnuHJRt2UUEr/ZL1oHipNyXkAEQa7baTITz+tGlDiCkYGN9D\n3HHDqcpqws14Xl4LQ9yCLH1e+24jPD7MkXbwZxxe9ayXBCqR8Ffywx5pH+xVQH2N9qiOtAPxft53\nO8ItoOqVud5nfTjrdxvMGFHrG2l3Gtfw+BJ9fo6NGYmAzEiHfGNkmB647WX+etnRXDrHu+59hWgY\nnUkVdd5SR227/B1ySSIz0m6Gx6/R13WS7RGaWmLmrdhujLQ7TrBz2kGffzM/7j3/1zXUFHkNoPTs\n8Y7j8OrqHVSqCDXagd1DvEZkv63+RKPJUcSz4kYipokn63Ir35TX6szjLlcW/YPXV23utmRZkq6d\nXiN5e1EADZKRR3mP3URVHzq0+zU8pKaMMXuM42pG1+0tdePg6rfgMiPqYc8Wb+3xjmZYdBexhb/j\npKnZO/q7Eg4bGnVlvlv2+K3LvPwvNSNyW9EfniVEfqsX4fGhw4Zz88dm8fOPzuKOTxxO/zajfBl1\nLHxlOVzxPLz/697+957KHCLf3qyXcXvsu7ozwkxEl57ctd9QL6Kwqx1WP8eYeq+DcpXbSf3425uY\nfcOjnHHzM1xy28u8u7mJClqpTOZQiZcGmzNn8DS4+k1/J6KbMPiEyf7OA/+c9hBG2str4YoXUF9Z\nQfmRl3j7Ny9OdSKZbN3RyCC3M8RRsUCnkI0bWMW9XV4dg8X3QGd7xvcWt21PJflLFJX7p5vlmFF1\nlfz24tl8+eRJzD3+tNT+w4q8yJ77Fq3nvkXrGZy+3FtYU2wDQBrttlM9GIrcxnjrTmjezpFj63ju\n2hN45FNGL1jYjWHwF+LvGRl8ozLSDrqhm1wuoqMZtrg98bs36AIOdG9v2EupDTF68je+7j2OcKNd\nDTo49XiqWgnAYfVGhT/k0HiAsfWVNJZ452DrlpU8vHgTZbR52V1VPNgoBnPEauWz4Di+vBUTzTlm\nAw8KziuJUmkZ5NNC5DtavEpkrCjcaTollV64aKLDf++J0rVjNtq3LPXmGbbsgE53RKGkKrhK80nf\n9+aG7t7AwTsfp4hOYiRYs90fTrtqWzNbm9qp8I20h7/0ZHyE15Aa1PSW77WtTe0U0ckH40a28ukf\nDkoNjrwCJ6ajFw6LLeMviS+zakmWMFUjmqUxiEb7aPP+oxvR86YPYd50/0j4pw+vI5Yqi5R/9Yt9\nIRbTDbVJp2d/z1v3csrU7nWb0XVeI3Rjoz4Pm9Kzx28wys1MI+P7Q7Z57du81V6UUsybPpQzZgwl\nHlOwc6X3vuSggFL6XpC8Z7Y3+afDJXngyzpp39M3wlM/TguPz9AZMfb93uN3/s0oY6R9pTvSftNj\n77Kj2b/OvD8EOYT8QyWVMO4E77nbaL/gqFGMNUL868LOHg/6/I3FOPJIr947kdWs2tbU/b3GNZ2o\nHqoTgAbE+IYqXnImsS4ZYdiyHZbcl/G9A9u8aNP22nF5H2Q5bmIDV75/PP3GHAoxfUzKdr3HvPG6\n/uM4cM/Cdf7M8QdwaDxIo91+lMo4r31wTRk1XUZva9iNYYAxx3mP3blUQLRG2gGGe6E4rH1F/4zS\nfHbQ4dnJ0c3GNV7imagmogNfdMCUmD6esxuMRnsYWV7TUEoxYIgXmty6dRVLN+1mjLnMSf/RUJRh\nzmK+GHSwN21gz2bY+k5qCkwJHYxRZgRA/sLVesQcGXj2Jv8yZWY4d/XQ/Ifz9ka2Zd92RGhqyYCx\n3soBu9Z5Gbl989mHBldprhgAR12RenrIkh/xWMmXeKP0k7S/95wv43QyiVpVxEba+4+fnXo8qtO/\n6sbWpjbOiD3P0OSITWUDTDgpOLmaYajDL009HR9bT92/PumPBHGp2OyNZu8oCaCCOuJILxndpsXQ\nvB2lFDecPY1htXrAoLaimA8PXKej6EB3KucqSuGwS/zPD/+MNz989Qsc3tDR7SPTjHDpDW6j3Uyw\n1r+i2B+hZkau5YKRR+Bbdz5J+hKtJtmm3ynl7wRY94r/c8ufgNd+7z1/6df++1p6eDzAROPcfvPv\njBngdQInw+BXb+u+rngDISShS2foLO/xBp2ToLQozvXz9ZTBIjpT0+9QsdBDpVW/oeyO6fwF1aqF\n7Svf9L2+q7WDgR3efT0W0Hz2JJOH9MMhxj+6jM65f13TPZcKUN+6MvXYCWqKIOiVD4zpK18e+bbv\n5UEUxnJvII32A4MB3ee1A9FrDI8+JtVbxubFnp9vpD0CnpkKyCjNZwfdEztwsvd84xs6LDk5ohkv\niV6Po3HTTY60H9zfqHBFYKQdYMRYr+Fb2boRRSJtPnuAofHQbV47q55JjbSPURsoUrqi3FwxPJil\n6DIx6RTv8aK74P7Pe89989lDzByfJNuyb1EaaY8X+6ZFsMLt5AwzC//M81P373hbI6Nim6lUbXwu\n8XuWbPBGtpZt1lNeKqI0px1oGDSMte7Ul1I6aF7nVZ637m7lM0X/9N58xKV6bnKQnPQ9np/4VVoc\n3SHYr3kNrE5bni6RYOA6LxnZ8n5HkHdKq4yGkgOrtFNNeTH3XHk0XzvtIP78maOofPd+7zP7Exqf\nzrjjvTmtww6Fk26Akcn7oUPRsgeYOMi775XEYwyp8ea5b9zVSnN7Z2pd55iCCQOr/RFqQ3I80l47\nEk77Hz1/fPanvP09Ndp7GhjINJAA0L7Hf68FHY1j/q708HjQAyjJRveezYzd7f3O1duaaW7v7DbK\nDiEt95ZO/QQodu8nuzek1q9/34QGLjxqFHUYo+wV9eF3EivFmiojX0La6kkbdrZybMzrQFK57kDq\nhalD+zFnYgO/6TyN9Y7bwdHaCH/6GHR6SW5bO7oY1uF1BpUOmZL+q/KLMXV15Io/M2qAN93XHx4f\nwJSxEJFG+4FAhnntQLTCzkFX3EYYlYxkiLyvcyECnuZ8tLUL9M+ojbSDv6Lxxl/hWWO5mtpR4RdW\n6QycoudrAWPVBsppZVy5EVobkUb7zLFD2e7oSmCJ6qKBRsaao9lm6HJQmA24dx5NjbRPUl6DuLk2\n4M4EkyMu9897fu0PXpKnqMxnT2J2um0yRj18SRxHB6aTFTOENRmZFHTmeJOqBpg8r9vu2bFlLH39\nxdTzZDiyLxFd2NOJgHhMsbzYu0Z2vuuFn1eu+Q+TY7oDpz1WrldtCJpYnMo5V/DXLiMiLT0p1NqX\nKW/T+UC2OdVsqJ1FIKR3ILnTNQZWl/GZOeOY1K9Dz4VNksus+7E4XHAPXHgfXPyAjnKacqb3+uJ7\nuOmI3dxT8i2Wll7EW6UXctG7V1GJHlnf0NjC0o27UzNMxtRXUl4c8zfacx0eD3D4p+HCf8AhF3n7\ntu3DSDvAcC9KhHULvMeP39B9FYx0Mq10Eov75oZXvP036qt0Z1F7V4JXV3mN86FGB0goy72lE4v7\n6z4bvBUArj9zKg+fb3hFod4L7Gg4PPW436YXfa+tb2zxLzM54eSgtAAdXfjjD00nVlXPpe3X0Oq4\nA2tb3vZd0xsaW31rtMfymIQuIzM+AkX6XFQbX+fT47xz0R8eH4GBvzwijfYDgWyN9qiNtIM/c+mL\nv4J3H/UneIvCBTd0lhd+t+VtPYIdtZF28Fc0Ft6p57MliUKjI52SCpwBusEbUw4n12+juis6y70l\nmT7cn4xuSmwVE+JmCHqAYWFJzIJ8+WM0lOpRkAkxr0HcURfCfPYk8SI482a/54Lb9c+orNGeJFPH\noeP4R9qjMLVknNFof+9pnYAq6DXa0zn0koy7a5f8IfV4w85WiulkonFuUroPS3/lgc1VXnRS59pX\nU48nr74r9XjZsHNCC6mdPKQf9zleA9lZfK9/zWdjrukjXYdRWZ49c3pOMRvtL/8Gfv0+f11j0Z+g\n0/UcMgOGHZLbv19WA2OP02GyAAcZnUcrn2byIx9nVuxdSlUHRYl2hm1/kcuL9LHa2NjKkg1ewtMp\nQ2t0Q7fVrfSX1ea3M9FcCWLHSt+0DB89DQwMnUUq3H7Tm7DkfnjhV/DiL733nH5j9wZ6+YDsESPT\nz/MeL7mfz5Y9TA06rPy55d6c+KG15VxwpPbxzWkPc4DFDJE3lu1TSlG79knvNTOJYogkjLw0w3ct\n9K2FvnvdMsa5SW7bVJn/WguIgdVlXD53PG84Y/l5p7FawSu3pR7uXPU6R8aMXCANAdc3yvv7VlI4\no+Oh1GNfoz2I5KwhIo32A4E+jbRHoDEM/orougXw+w/q5Cqg581lmn8VNCWVxpJqjk52EsWR9p5G\nB6LQ6MhAbKgXJva1WR16jmSSMLK8ZqC8JM7uMu/G/7/FN3JGzMgqHXR4PED9eG+d2s5Watc9RTym\nmKS8Dq9EfYiNdtBh/Ede7j1f9Eed1djslIvCSPuY93lzdNe/pvNBtOzQme9Bh15GoQOpfqIX6tfW\nqO9DQWeOT2fMHJh1PpTVsGeUlxBq9q5H6GrT4cfrG1s4I/YcQ8z54blaTms/aa7zQk/Lt7gjrTtW\nMWG3Hv1KOIodB2fumAiC4niMrTXTWZXQ90LVtgveeUS/mOjyNdofTsxmRP+AVoQZeaS/42XjG/D3\nz+jGh+PAAq9yn61jJ6fUDNMZ1nvgk/EHGcgONjS28tYGb1Ru8pBqv++Q6fnNDVFSCf3c+16i0x/d\nk2Tnan0PAp2rJn2QpbTaPx3uz+fDQ1/1cgiMO15Hh5z6Iy8XRnEFHPfV7F6Dp3sNr45mLm76P/5a\ncj1xunh+hZcPaXBNGV85ZRKHjx7AzBKjIy7MUewhxjro6/0rQfDOw97jgEets1EzemZqLfTaru2+\nZTyrVj+Wery2drbXMRUwyUR+f+56P5245eOaF3TEXKKLkc9cS4nSme/fK5sSTj3IuLfUrPgnpejk\n0P7w+IhNC80xVjXalVLDlVK3KqXWK6XalFIrlVI/VUr17/3TBzBmo33bu14vnjlfMwph56B7SEuz\nZDyuGhj6kl8pzDlkq1/wd4bUjg5cJyODppIx2Q341rWMFMZ8rYHrHoEVT7rP1P6t65tjmqZ8lA5H\nF1zJgipFXUhh6JPPSD2MvX0f9VUlTDDC42ODAp5jlokxx3lJ3Fob4ZXf+itVUWi0l9UY14ejR9vN\nuab9R0djyRil/J2ci+9JC48P4VgqBfNvgWtXU3HRX1mNblz0o5nNT/yKRMJh064WLvXND78stIpo\nOrFh3ghd/6Z3oG03vHoHMXSZ+XRiGiPGhXsdjW6o4t6EcS986kd67vJfL06FQu9yKng2cbAv63de\nKa3WIepT5usVIADWvgxv/wuWPugtZVZS7V82NZ+c8796vniykj75TLhqUaohWq7a+ULR39jY2Mpb\n63cxXq3lluKfcv4bn4BnfuL9nmnnZfjlOcZs4Nw+D9642//6G3/1Ho8+NnM9KFuZXlwB836ir81p\n58K1q+Hrm+DaNXDkZdmdlIK513p5hoCJsXWcFnuRhauN8PjacqrLivnLR0dwtGOEcecyb8HeYo60\nL38cVrsh59uWew3i4opQRq0zMaKumpcSXqeLs9Kb1z5si7dixbahc4PU8jHc7QDcSg1Px41otFdu\ngwW3U7dDf/ftTpxHx389nDJyxOFee6e9id8c20Q/mphgrqAjieiigVJqHLAAuAR4CfgJsAK4Cnhe\nKZVh4k6B0G+Y1wvesl2HnC99ENZ74X++EK0wicV9WYh9RGF0K4kZVvX0jTrhCejl9QLO7pmV0ipv\nfd8xc+DzC+H4b8KJ38vtnMJcYkYHrHgS3Moy40+ITgQDcMKZF7B43v20Dk6rKKlY5sQ+QTDZmMe5\n7GEmVDQzSm0GoMtRlAwOKXO8SSzmW3OaR77hjSypmBctEDZj53qPn/kp3GVU3KM0tWT8B7zHz9/i\n5diA0MMAVSzOSwO941b7yk1s276F83iUScnQ+OJKmB3C/PAsDBo0hNUJXc4UOR3w80N904r+zImM\nGFCR7eOBMKa+kr90zvXmlm58A34w1DfKfnvXSXRQxOigGu2gc72c9zswstzz72/Bg1/xnh9yQXD5\nC2qG6ZDwq9+C69bCh+/U1+7JP0i95bz4k9Q0r2TD2pX8oeQHnB5/iX7bFnm/Y9Qx/lwc+WLGR73H\n7U1wz6Vedm7Hgdf/4r2ebalBc/WdonKYeg7MugDO/7v/nhWL606yeFHvXlPPhmve8s27/1bxnVxb\n9EeuLvorFbR6Sf1e/R0qObI/5rhw65T1E7xOjESHjjxoXOtFpYC+x0eks7C2opjXYlNTzzsW3a2n\nSbz7GBP2ePX0znEnhqEHwDAjaue2NqOz+NXfwRPeNfXLrvnEB08lFJTy1YOO7XiWR2YvoFq5K0MM\nGBudQbU8YU2jHfgFMBD4vOM4ZzmOc63jOMejG++TgBtCtQuTWEyHLCb597fgn9d4z2d8NPQKno+5\n18IXl8JVr/v3N2/P/P4wmDLfm3/b5WXQZPYng88q3BNn/BS+vBwuul/fsOZ8CY75fKDrfO4VQ2bo\nzPbpHPaJ4F16IB5TzJx9DGWfftjfATLmuPBGYQdN9Uax25u4fefFxJTu9FjpDKayMqTM8ekcciHU\nZOjYOu6r0emYMUewN7wGrUZuBTPJVdhMPtMY0XKgw12GKVbsz4IfEtXHfIo1biO4vLORhpsncEPx\nrd4bDr1Iz0WMCCPrKrgnYYy+NXl5XzY6/VlTP0evmR0iY+orWUcD/68z84j1rZ2n8JPOcymKKYbW\nhtAoed8XvUGC7cu96S8l1fq1oInF/B0F445PNXCLVILvFN3BzUU/YZCZ+Rx0UqszfhZMdN+MD8PF\n//Lu34lOeOIG6GzXif22uEtYFVfAQVnWpT/4HDjuWh25cuUL8KHbYP7NMGo/521XDYQTvkXCXUK2\nQTVyWdH9XFV0D78v+QEjy9t0FvFX7/Q+E3Z5rRSce5uesw96KdTbTvXNwQ50ycZeUEqxosZLRley\n5hm47TS4+xPE0B0hT3VNo27o2Gy/Iu9UlBRRV6nrZk93TqajwW2Yd7ZAs85xsN4ZwC86zwznvpPE\naLSrRX9k8Ju/8V47/pvRidbNE1b8d0qpscBJwErglrSXvw3sAS5QSoW/rkxYHHmFN09z81uw2537\nWNng63mODNWDdQX+8M94+6JUWS4uh7nX+feV9gunUtIbYY387gsVA+CEb/v3VQ+NzNyzbsSL4Zzf\nwEnf1x05J4fYN6gUHHVl6mkRXtj+vxOHUloUkdt5xQD41KN6JAgApY/f3GtD1fIxfLY39zNJ7Sj4\n+N06S21UiBfBh26HGqOBXlSul5MKa3k/g+OmDOeWWObjtapkHBz3lYyvhcWI/hX8pPNcvth+WWqF\niCS/6jyDCYNztLb4fjDGnVv6267TWF5khFUXlbHpqG/y3c4LcIgxvH85RfEQrvnKOnj/17rvP+bz\n0SiLlIIPfCf1dE78DQ6NGdNfBk2DiafAh/+gc4UExehj4Zz/856/+Tf4fgPcaST+Omhe9us6Fof3\nXwen/jD30UCV9XrUPo1DYu8y55HT4ZYjjDrlwOwdC0HSfxScd4c3XWPnati6VD9WsUg12gESDZP5\nbeep3o61L6U6izc6/flix+UMCbMxjBci7xDjncO/7yVkdvlJ57m0UcLgmoByaWRi2CH+qWGOWw8a\nOgumnBWOU4BEpJbXK8e7Px9xnGR8jsZxnN3As0AFcGRvv0gptSDTBoScxWk/qR2ReS7Z6TeGlgm3\nT5z4Xb3+74ST/EmsosCMj/ozZB77hWgfS1s4+rNw2o9Jzcc/5vN9C+ULi1gMjv6cDg0dFFJYWJLZ\nn4L5v0iNMDQ5ZXy34wJ+1PkRVBTmYSepHqxHgj77CnxugT5+USJe7EVQxIrg2GvgihdgQnjhiVmp\nrIfz/6Y7tmZ+XI+yHRZesjSTsuI4ztQP8mrC3/h5KTGJOybeEqlRdoDK0iLqq0r5W2IOc9p+ysXt\nX+Ez7VdzZtv3uL3rZCYMCn9pumSjvYs4VznXwKTTYcbH4PLneG34+STvmyODDI1P54jLdAM0OaWt\nZqQeOIgKww7hlaq53XY/OvIquPwZ+KvoTWoAACAASURBVNifYcIHun8u34yY7c98n870AObXZyF2\nzOdoUXpqyFpj9ZTi1q2ww1gO8+jPRieSb8wc+MhdqaXAAD14dcp/h7O6Rg+M6F/B9zrP5+ed/oZl\nq1PMpe1X017eQL+ycI/r8P7e1KClRRP1Uq4u7zKCv3XNAfzLAAaOUr78PnpfXA8MHOCj7AARrin7\nSE7WXJbl9XfQI/ETgceyvOfA55gv6GRFXe26J+r0H8OkU3v/XJgUl8NZ6cETESFeBB/8Ddx/lU4+\nduSVvX9G6BuHf1qH/e7ZHG5CG9tQCmZ9HA46jb/c/Sd+tLiGrWRJ7BgFwsgw21dO+x9dgW6YFJ2w\n/Ww0TISP/6X394XA/FkjuGjBtRwXW0S5amOz059nEgdzTV2EcpQYTBpczdZ3t9FEBU8mZvpemzgo\n/OiFoTXllBTFaO9M8OaeGhrPuoOacl2ZX7V4eep9o+tCnHuvlG5gTjxZL0c4fHYkIj9Mnht1BVPe\nfJ4K1cYWp4brOy7kUyde0/sH880J34blT3hTXcr76+kuk8/w57AImv6j+f3M3/Pci8/zbOJgToq9\nwneKf0d9cm32kmo4/htwxKU9/56gmXgyXHAvPPJ1vfrQCd/0JbyNCjpXhuLGzvPoPGg+V0/Zw8I1\nO7nmhXLec4YwZ0T4UT7mvPZ1O1r0971nM4mt7/K5leeRIEZRTFFXFfIU0ZkfhZd+rVdPGDBWJ2KM\nSNLBfGNLoz1ZK23M8npyf69nveM4GVNwuqPtOV5cNGAGTYFPPgJblunwpYgVolYyeBp8+vGwLQ5M\nBh6E7QEuoVHenzWDjmfr4nd7f6+QmaJSmBitEEobOWJsHeXV/fnnbv/c2iFhjsb0wLWnTOZHD7/N\nc8u30ZVwfK9NjMBIeyymGF1XwbJNeinUlVv3MMOt0K/a1px6X2CZ43uirAYm9zByHCJlg8Zz9qvX\nc7Bayb8ThzJ6+FBmRqBhRMNE+PRjsGGR7uyISpJgYOzEg7nh+VYA/pk4ijcr5vLkRyr0nPuJp0Y3\nM/eooyJfTxtpJLh8sXkIHHIU/1i7mPeclQDMisC5OdxotK/d0QIlFfDB37Bq6x6W/PhJAAb1Kws9\n7wdDZsAnHtFLMR80LzIJB4PAlkZ7byTPIKfHdxUCQ2f5l8MQBOGAJBKVdqHgiccUJ00dxO9fWO3b\nPziijfZpw2u485NH0NjSwYzrH/G9Nqw2xLmaBqPrKr1G+7YsjfaQs9xHnYHVZSx1RrLU0fkgzj8y\nQtE0Ayf7112PCOmdGrVV5TDqaL0J+8X4gd4g2utrG2nvTLBw9Y7UvlkjI9hod9nQ6D2OTGfsiNl6\nKzBsmQCQHEnPFgfaL+19giAIBzTzZw7loMHVxBT81znRCwcUCoeTpgzutm9omMmK+kBNeXG3EPNY\n2CNILmMavA65FVv2pB6v3OY9Hl0vjfaeMEN9Ac6cEaEVdCJKetjzzub2kEwOPEYMqEiNtje3d/Hc\n8q0sXr8r9XoUokDMOe1rd3gdhBt2tqYeD4lIx2ahYkuj3U0JycQsrycnTmab8y4IgnBAURyP8cDn\n38fLX/8AHz08/OW/hMLlyLF1VJX6A/eiOtJu8oOzvc6uz8wJb7mldMbWe43251dsA2BbUxvrd+oR\nL6X8FWyhO4eM7M8hI2uJKf09lxXHw1aygnLjOMk5llveN8FL8Per/yyn052eM7ahktqKDEvhBowZ\nabRuZwsJ12/jLqPRbsF9/UDGlvD4J9yfJymlYmYGeaVUNXAM0AK8EIacIAhCGMSikBRGKHhKimIc\nPKwfL6zYntpnQyPp6PH13PyxWaza1swFR0UnfPrYCQ3EY4quhMNL723nc39cyL/f2khyCv6QfmVW\nHN8wiccUd192NG2dCcpL5Fj1lV9fcCgX3voSAFefGOFEohbyvgkN/OFFPY3IvFfOGhGNVTYqS4sY\nUFnC9j3tdHQ5rG9sobw4zp9e9qY+hZo5XrCj0e44znKl1CPoDPFXAj83Xr4eqAR+7TjOnkyfFwRB\nEAQhf8ybPjRVEa2tiMiSUH1g3vTohU0Pqy1n/syh/P3VdQDcv2i97/UTpwwKQ8s6YjElDfa9ZM7E\nBh7+whziMeWbhy3sP0eNqyOmIC3/ZSTmsycZMaCC7Xv0tIhP/24BjuOwZruO8KkoiXPS1O5ToYTg\nsCU8HuAKYDPwM6XUvUqp/1JKPQ5cjQ6L/3qodoIgCIJQoHzosOEcPnoAJfEY35o3JWwd67libves\n4hMHVXHjh2bwrTOmhmAkFAqTBldLgz0P1JQXd5u7XhxXvrD5sLn4aC/iaMmGXby9cTegp+T87COz\nGCpz2kPFipF2SI22HwZ8FzgFOA3YAPwMuN5xnO09fV4QBEEQhPxQWhTnL5cdRWtHl4Ru54DxA6s5\nZepgHlq8EYCjx9Xx24tmy8ixIFjM3EkDeXX1TgDKimP89znTI7USzNmzhtPSnuCb/3gztSRmPKb4\n9hlT+IBE+ISONY12AMdx1gCXhO0hCIIgCEJ3pMGeO75/9sGUl8SpryrhmhMnSYNdECznkmNG88a6\nRjq6Elx36mQmDa4OW6kbHztiJBMGVfHAGxuYOKiaY8fXM0KWmIwEVjXaBUEQBEEQCoH6qlJ+8uGZ\nYWsIgpAjqsuK+b8LDwtbo1dmjx7A7NEDwtYQ0rBpTrsgCIIgCIIgCIIgFBTSaBcEQRAEQRAEQRCE\niCKNdkEQBEEQBEEQBEGIKNJoFwRBEARBEARBEISIIo12QRAEQRAEQRAEQYgo0mgXBEEQBEEQBEEQ\nhIgijXZBEARBEARBEARBiCjSaBcEQRAEQRAEQRCEiCKNdkEQBEEQBEEQBEGIKNJoFwRBEARBEARB\nEISIIo12QRAEQRAEQRAEQYgo0mgXBEEQBEEQBEEQhIgijXZBEARBEARBEARBiCjSaBcEQRAEQRAE\nQRCEiCKNdkEQBEEQBEEQBEGIKNJoFwRBEARBEARBEISIohzHCdshEiiltpWXlw+YPHly2CqCIAiC\nIAiCIAhCjlmyZAktLS3bHcepC9tlb5BGu4tS6j2gH7AyZJWwOMj9+XaoFj1jgyPY4WmDI9jhaYMj\n2OFpgyPY4SmOucMGTxscwQ5PGxzBDk9xzB02eNrgCDAD6HIcpzRskb2hKGyBqOA4zpiwHcJEKbUA\nwHGcQ8N2yYYNjmCHpw2OYIenDY5gh6cNjmCHpzjmDhs8bXAEOzxtcAQ7PMUxd9jgaYMjeJ62IXPa\nBUEQBEEQBEEQBCGiSKNdEARBEARBEARBECKKNNoFQRAEQRAEQRAEIaJIo10QBEEQBEEQBEEQIoo0\n2gVBEARBEARBEAQhosiSb4IgCIIgCIIgCIIQUWSkXRAEQRAEQRAEQRAiijTaBUEQBEEQBEEQBCGi\nSKNdEARBEARBEARBECKKNNoFQRAEQRAEQRAEIaJIo10QBEEQBEEQBEEQIoo02gVBEARBEARBEAQh\nokijXRAEQRAEQRAEQRAiijTaBUEQBEEQBEEQBCGiSKNdEISCQCmlwnboiaj7JVFKDQrbQRAEwQai\nfl+Pul8SKXcEQRrtgmAFUSxYlVL9wnboC0qp8wAcx3HCdsmGUuos4BSlVGXYLj2hlLoPeEgpVRu2\nS08opUqVUnH3ceTLuShe35mw4VgKuSOq56UNZY+UO7lDyp38EdVr3MSWYxkERWELCAcWSikV1UJK\nKTURGAnUAk8BOxzH6QjXqjtKqWOBWcBY4AngacdxdkTp2Cql7gGWK6V+6DjOlrB9sqGUehCYrpR6\nz3Gcl8P2yYRS6lbgHOBpYAGwJ1yjzLgVp3nAGmA08FqUzkkApdTFwNHAJOANpdT/OI6zKkqeSqlp\nwDCgCngR2O44zh6lVMxxnES4dh5KqdPQ33MD8DLwclSv9Sh9v5mwoeyxodwBO8oeKXdyh5Q7ucOG\nssemcgdCKHscx5FNtv3agB8AlxjPVdhOGRz/H7ASSLjbQuAyoDJstzTPW4BNhucO9/hGxhP4vuF3\nA1AftlMWzweAVuBqoDpsnyyO9wK73fNznLtPuT9jYfsZng8B7cBz7vd+S9hOGRzvBHYCze51kwAe\nBgaE7WY4/gpYZ1w/a4G/AmPDdkvz/D3QaHgmgCXAB4DSsP1cx8iXO65X5MseG8od1zPyZY+UOzn1\nlHInd56RL3tsKHdcz9DKntD/edns3tyLPgG8AJxr7I9MBQq4zy1Enwe+Azzu3mTfAQ4P28/w/Id7\n0/8zcBLwSeBtYAUwImw/1zEG/BroQvfQR7LyBDwItLgVpxpjf5TOy2+4Fafreirgw3Y2juXlwOHA\nNmADMCvsY2g43uUeyxuBGcAo4DGgDZgWtp/reI9bsfsb8HHgeuAl9xraBHwgbEfX849Ak1vJO8V1\nvc/13A18CRgcsmPkyx3XJ/Jljw3ljusZ+bJHyp28HEspd/bfM/Jljw3ljusZatkT+skkm70b8EX3\n5H3bvdjeAD5kvB56QQX8zK2QXAc0uPsGAz903X8RtqPr9Cv3xvRVwzMO/Lfr+b6094fWGw6ci+6x\nvQJY5Pp9PyqVJ+B+dKjfF4H+aa9NAGYCNUBFiI5V6DDZl4BB7r4yYIxboP4cuAk4JOTv+gG34nRN\n8li6XgngU2F/167PZW6F5HqzEuoW/BuAI9znRe7PwO9LwFfcY3Z92vU9HngSb3RzvvtaKN85cLp7\n7dyY4dr5BrDRPR++mfw/QnCMfLnjekS+7LGp3HH/fmTLHil3cuop5U7uPCNf9thQ7rguoZc9ofzj\nstm/AXOAd4H1wJHAF9yL7vWoVKCA09yL/fZkoQ7E3Z9j3YvuaUCF7PkpdKjSz4G6tNducQuAQ4Dz\n3ZvbMPe1sCr2J6BD1sa5jxfijXoMcd/TDxgfgtsTSRdjXxUwFx0S2GrcdG8npJEkYJpbCF1vHK9P\nAcvwh4btQVeoh4R0LJOjRv2M/R903VYAo8M4fmmetwNbMlw7X3fP02uA3wL/R0ijm+gRg/XGfSiW\n/IlX+UugQwMPM98TsGeyUjLH8CsyXv8MsNo9Ly8P2hMLyh3370e+7MGycsf925Ese5ByJ9fHUsqd\n3HlGvuwh4uWO+/ciUfaEchLJZv/m3ugTwDz3+VDga2GcxFn8Yuheuw5gkumBTsBYBLyJ7rXvh1uh\nCtFzV3pBhA5V3IjuBV1uFKjvAhNDPLaDgM3Axe7zs4BXXbfr0CMKy9HzfmoDdrvX9XgMN5QKHV63\nAR2W+jTwDDqpTQJ4lhAqUMDB7rXyPff5GcB29Ny9c4FjgJ+6+/YAVyXPl4D8zkL3In8Ft+Jk/m3g\nbrewP9V9Hvj1g27wNLjn2jqM0Tbg/e713QIsdn8m3Ovs40EdS/f6HgJsda/bCuO1ZCNutnufesx1\nfI3wRrG/4TqcmDzGGb77K93juBM3VDWo+xARL3eM7zzSZQ8Wljvu341k2YOUO7nyk3Ind57WlD1E\nvNxx/1Ykyp5AvxjZDqwNPZpQbTwf1MNJXBSwW4lbiH/Nfd7tRgk8CqyKwHGspXvl7v3o+Y9twFXo\nHvvR6EQdyZvrwJB8i4G3gNuMffPR2UiToVYtBBjGlnZzv931eAQ9P3M9upI0zi3Iit3C6in3fT8l\n4CQn6NG2rcAr6Ir7fe41U5r2vivdY7mDAEePXKdZQFXaeZnspf+Me+weCOMcTHP9s+vy/9DZez/p\nnovtwIfRoZ/FeCG/O3AbHwE6PoWuCCdDJpPHMY4eJXwb6A/8y3VMhioG2jgCPu3+/bvpPoJkXmM/\nct/3IAEn2yLC5Y77N60oe7Cs3HH9IlX2IOVOrv2k3Mm9Z+TLHiwod9y/H3rZE+g/LNuBsdFD72am\nk9h8P7pSEEjIlVsAjM6wP1kQPITuKY2nOU4ibV5NQL5JL4UOT0wAJ2R433/Q8xADT8hi3PD/BDxp\nng/AJejKXsItAAKt3KV9h3fgjRC9AJSZx9h9fIxbmL1ICFmS3UK/HR0GuAb4QfL/SPtffuv+HxcG\neR728p4aYCk67PPEvn4uT+fi+/BG28ztHPN97uM73de+GNSxRFfcfuz+3afQo13F7usfRycle8L9\n3k9333dz0Oej61PtXi9bgY8AJVmOuUKHJq8goJEZLCh3jHu4NWUPFpQ7aedepMoe/GG8Uu7s5/fb\ny3uk3Om7Z+TLHuPeE9lyx/27WRvgBFz2yIL1wl7jOE5XD69tQt/sb0D3MH8THYKFUuoC4Dbgx0qp\nogA8dzmOszLDS/HkW9A94BXJ/0kpdQrwC+CrSql4hs/mDce9wt2fXwZmO47zmFIq5rpVuG9dDFSi\n1/0NFMdby3MhMEMpNcpxnC6l1GDgu+iK01rgVOBSpdSQAN26kt+Z4zgXoTO7tgNXOo7T6q5F6hgf\neQd9o51MgMcy+X2iMyFvR3/Xg9HZcUGfAl1KqVL3+aPuz5og/NKOUTeUUnHHcRrRSaxK0KNxvX4u\n1xjn4vPo0M5voAvPK4F/Aw8m159VSpW5733E/VkekKPj6PW4f4IOQT0WnbDqUaXUf4Bb0YmgLnLv\nQe+i5zvXBuFn4l47LehR1Qr08TzKvA+6x7LE/a4XoUdhpwTh514TGessUSl3jHt45MsepZRKc45k\nuZP0iGrZ4zhOZ/JeHfFyp9h9GNVyJ+F6ZrzGpdzZa8/Ilz2O4zjuPTmy5Y779zuz3ZMDL3vy2Tsh\n24G1sRc9mujep6+jbwKvo5PdbEDPSZkapiP+0Y61xv6T0D2nrcCUsI4l/l46lf5+dAGwnDwvf9GL\n40fRFZMBQB161Ggb8An3hvU8utf2G+R5Dle6Z9rxO5+0URf8vbbvoQu0kiAd3X3VwH/hrY+8GG/O\naLHxvv9xj/WcfDr29p1neO+R6IpyC24CmyC23hzRIwtr8OZEmufDTeg5fKcF5Wicb8PRI3FL3O97\nGXr5mGHGe/ujEwL9Ns9+E4GT3XveQWmvDcAbZXsNvUZueYbz8o/o5EDDgnTs6X5CCOXO3ngSUtnT\nF0ciUO700TPUsqcHx1LjcajlTi/Xd2TKnX28xgMtd3pwTK93hFru9PKdR6LsAY5Gd258Dfhw2muR\nKHd68uzlvAyk7MnryS6b/RtwM24CDff53lTqa4Fr0SF1CXTv7sFRcUT3JC9xHycrTY3A9CgdS/wV\nlgvQiVjuwJ33FYYjej3SdejexVXud3uF8fq56OVE8tL50ZsnWUJp047lFe55+UOzUAjCEa9S3IAe\nOdgIdKJ7m0cb7zsLXVF+hTyFfO7nNZ6cY/ap9OMbliO68rQbmIdb6Lv7z3QL+5dxlzsK8PtOVtgr\ngXp0aOVA0pZ/QoestgAf29vvYi88f4we7UuGc74GfC7tPYPQI4YJdDjqZzFC/NChlOvQcwtrwnDs\n4bOBlDv740mAZc9+OAZW7vTF07hnhlb29MExYxgtwZY7fbm+o1Du7M81HlS509v3HUt7b+Dlzl58\n56GWPejycZ3h6FttwX1PqOVOXz17+Gzey56c/8OyHTgb8Be8Xrkzjf29jXSZN7LPu4XCNvJTiO61\nI966mY+j58acg85Yuov8Ndj39ViavbVJzzXA2DAd3Zv+Bvf976GXDklvOOdlvl6OjuXZ6F7nd4FR\nYTjiNeQa0OGdyUJ3BTq5zd/QyYy25OPa2c9jmaxAJ+fBLSdPc3H76mg4fRw9qvEaetmiw4FvoysB\n24HJIX3fmTptzHvlGejMw4vI0/xr4B/o0bNX0KM/D6NHdzcCp7vvSd4fB6FHCzaj7+EL0SMPv3W/\n762kjegE5Zjlc4GVO/vqScBlz34cy8DKnb31JKSyJ0fHMt/lTl+u72QegDDLnX09lkGWO31yJMRy\nZy++81gG38DKHuAedEP2LnQnxofQU0S24kV5JOtDoZQ7ffXM8rng2jz5+Mdls38DvoTX25VA9xTO\nN17vSxj6xegQrO3kITRxXx3xCq1n3AtyoXuh5qvBvl/HEh1O9xV0g2ATMC1MR+Omfw46mc6XzIKg\nL+dGyOflF9AZUzeTn8iPfTmWVeg5Z3ehw6sS7nf9T9wMz1E8lu77XkWPwuWjsN9rR3TI7O/xlttJ\nbm9G6T5kvF6MruQtcc/JfIVx/xy9XM61eGv2DkSH9PlGE/AqUDXoUbf7jOO4Cz2SmY/Ojz479vA7\nLiaP5c7+eBJg2bO/x5IAyp19OC9DKXtydF7mu9zZl+MYRrmz38fS/Uw+y529diTgcicXx5IAyh7g\nf9FRRNcBA4z917mO3RJbokesAyt39sWTzNOfLibfZU8+fqlsdm/uTXylexGPBb7onrSr6GNlFL3c\nxUPuDSUfjcxcOCbXVt1G/hrs++WJTraRvHE9R35GtvbJEZ1UaSwZem4jeiwPRidiSQAL8nHz3xfH\nDMd1FDAdPccrX6Goubh+kqOGpwITouBonIsNwKXAH9Bz4L5AHubA5eg4XuV+5pl8nJPu3zgdPXp2\nG92X1DkCXfl9E53gKZbJ2b1+jkInz8pHSPxeO2b4HXktd3LomdeyZ38dCaDc2QdPc8Q6sLInB8cy\niHInF9d3EOVOLq6dfJc7+3wsCajcyeGxzGvZg27ErkV3LgxIe+1X6PvfZHQn3HwyTGskz+VODj3z\nXvY4jjTaZUvb0L3Vl6J7i+Yb+77J3ldGPwiMi5ojegShBB1KtIT8hYDt97FE9zh+zv09OU8AlKvv\nO1uhECVPdGXkenSCouFRcyRLZSpqnhl+Xz7mXe+zY77PxXwcR+BE8jd3NA780nUal+l4oZfbeY8M\nc2yDOJ7765j2u/JS7uTqWJLnsicXx5I8lzu5/M7zeX7m6Fjmu9zJyfXd0/0pCp4Zfl8+yp19dgzi\nPpmPY0meyh73Pnc7uhwcnfbaSejpNjvRIe/J0fQncTsxyXNy4Bx6mp2JeSt7Un8jqJNMNns2dLKK\n+ejlIMwbQbbKaPrNK+8X2/46uvvqyFNikBx7+tZPjZpjPt3ycCxL8nl+FsqxDMIzl/ch8lQhzYFj\nWQDHMY5ufKXWY057vRhdCVmT7XiR/065XDjmPKlXPjzdfXkre3LomO9ypyDOS3df3sodG46jLZ65\nvg9lOhci5FmaD7e0vzEcmGH+feAY4Gn0/P/PAnOAqcCf0GXmg/n2yrVnENdP6m8FfXBki/ZGL72u\nZKmMuq8dJ452edrgaIunDY62eIpjzl37k2HE1Kig3I9OXFSGkQGbPM1rtdXRFk8bHG3xFMfC8rTB\n0QZPMi8hWQH8Ar1k30lp7x+Mnj6SAI4K8Dha4Zn6+0H/Qdns3/Aqo6uBU919F7r7bg3bzxZHWzxt\ncLTF0wZHWzzFMaee96GXuakw9p2EzoT832H72eJoi6cNjrZ4imNhedrgGGVPYAZwqPs42fFd5v78\noVs2zo3A8YukZ+gnlmx2bsC38EaRfoq3Xmq3TJDiaL+nDY62eNrgaIunOObEL45eJmi1sS+va4cf\niI62eNrgaIunOBaWpw2OUfYkc9JYc9+D6DnkdUF62eQZ+sklm30bXq9TclmJBLCDPCxjciA72uJp\ng6MtnjY42uIpjjlzVMC/gaXu81PQS5FFqRIaeUdbPG1wtMVTHAvL0wZHyzzN9c0vQS87eAdGdEAU\ntih5xhCEvUApFXMcJ+E+XYtXCT3GcZw3wzPzsMER7PC0wRHs8LTBEezwFMfcoJRS7sMuoEQpdQ46\n9G8c8D7HcV4PTc7FBkeww9MGR7DDUxxzhw2eNjiCVZ6p8lEpdRZwDXpptesdx2kOVc4gcp5h92DI\nZucGfAa9RuR2YGrYPrY62uJpg6MtnjY42uIpjjnxKwKecP0WALuI0GiMLY62eNrgaIunOBaWpw2O\nlnmWAl8E3gE2E6EItKh6FiEUHGkjQPvy+eHAmcAg9DIJi3Mm5/2NyDu6fyfynjY4un8n8p42OLp/\nJ/Ke4pg79tcT6ESvzT0SONbJw2iMDY5gh6cNjmCHpzjmDhs8bXAEOzz31dGNBhiJDjGfAzwPnOE4\nzts5Vkz+PSs8+4KExxcYaaEes5VSpyqlhu3lr9kE3AxMcPIQ5mmDI9jhaYMj2OFpgyPY4SmOuSMH\nngngP+gM98flu3IXVUdbPG1wtMVTHAvL0wZHWzz3x9HRw9e7gd8BVwPnBtFgj7JnnwlriF+24Df8\nyRSuRmcxfg+dpCIWlpdtjrZ42uBoi6cNjrZ4imP0PIGhQH2hOtriaYOjLZ7iWFieNjja4plDxxjG\nOumF6rlX/1PYArKF8KXrtYO7gL8Cp4ftY6ujLZ42ONriaYOjLZ7iWFieNjja4mmDoy2e4lhYnjY4\n2uJpg6NNnn36X8IWkC3gLxzOAZqB3wDjw/ax1dEWTxscbfG0wdEWT3EsLE8bHG3xtMHRFk9xLCxP\nGxxt8bTB0SbPvm6SiK5AcBMqxIDT0T1Ov3Qc591wrfzY4Ah2eNrgCHZ42uAIdniKY+6wwdMGR7DD\n0wZHsMNTHHOHDZ42OIIdnjY4gj2ee4tyeyKEAkAp1Q94GWhyHOfQLO+JOY6TUEqVOI7THqyhHY6u\nQ+Q9bXB0HSLvaYOj6xB5T3HMHTZ42uDoOkTe0wZH1yHynuKYO2zwtMHRdYi8pw2OroMVnnuDZI8v\nLJS7VSqlypVL6kXv5I0Dn1ZKDRRHqz1tcLTF0wZHWzzFsbA8bXC0xdMGR1s8xbGwPG1wtMXTBkeb\nPPuMNNoLBKVUDGgDFgMTgdMcF/c8Ntcx/BFwFVAvjnZ62uBoi6cNjrZ4imNhedrgaIunDY62eIpj\nYXna4GiLpw2ONnnuLdJoP8BwT9RuOI6TcBynFbjf3XWLUur45MeSJ69Sah5wMvAOsL5QHW3xtMHR\nFk8bHG3xFMfC8rTB0RZPGxxt8RTHwvK0wdEWTxscbfLMGU4EsuHJlpsN/5qEU4FTgY8BRwMlxms3\nAglgF3AhMA4oAa4EXgc2ApMK1dEWTxscbfG0wdEWT3EsLE8bHG3xtMHRFk9xLCxPGxxt8bTB0SbP\nnP7PYQvIlqMv0n/yfhlY556kqo8JMAAAB6pJREFUye1vwDzjPTcYr7W4J3MCWAYcXKiOtnja4GiL\npw2OtniKY2F52uBoi6cNjrZ4imNhedrgaIunDY42eeb8/w5bQLYcf6FwnXsi3g+cDcwFrkevU7gC\n+KDx3rOA/wEeA/4AfB4YLo72eNrgaIunDY62eIpjYXna4GiLpw2OtniKY2F52uBoi6cNjjZ55uz/\nDVtAthx+mXACsBX4CzDF2D8faATWAoMzfC4ujvZ52uBoi6cNjrZ4imNhedrgaIunDY62eIpjYXna\n4GiLpw2ONnnm9H8OW0C2HH6ZcC067OMD7nOF7llaCmwARrv7i4BK4z0q+Vgc7fG0wdEWTxscbfEU\nx8LytMHRFk8bHG3xFMfC8rTB0RZPGxxt8szp/xy2gGw5+BJJrUX4MLDG2H828DawKXnyuvsnAJ8F\nSsXRPk8bHG3xtMHRFk9xLCxPGxxt8bTB0RZPcSwsTxscbfG0wdEmz7z872ELyLaXX5jRM5R8jJuQ\nAbgd2A0cDpyY6eR13/dXdLbEoYXqaIunDY62eNrgaIunOBaWpw2Otnja4GiLpzgWlqcNjrZ42uBo\nk2dQW+gCsu3lFwaD3K0fUJH22pXohAwPoNcc3Jjh5P0EsAb4OVBWqI62eNrgaIunDY62eIpjYXna\n4GiLpw2OtniKY2F52uBoi6cNjjZ5BrWFLiBbH78oOB74b/ekbATeA+4FTjTeUws85J7Ee4Aj037H\n2eg1CRenn9iF4miLpw2Otnja4GiLpzgWlqcNjrZ42uBoi6c4FpanDY62eNrgaJNn0FvoArL14UuC\nHwLrgS50b9LrwBa8NQevBqrd984HnkUnZ/iJe9LOBH6M7m3aAkwtREdbPG1wtMXTBkdbPMWxsDxt\ncLTF0wZHWzzFsbA8bXC0xdMGR5s8w9hCF5Ctly8I7ga2o3uYpuOGdwCHuCdl8iT+FjoxQxyYB/zT\neC2B7ql6FDioEB1t8bTB0RZPGxxt8RTHwvK0wdEWTxscbfEUx8LytMHRFk8bHG3yDGsLXUC2Hr4c\nPU+jCfg6MMjdV5L2nmuMk/RSd58CSoFz0XM+rgOOAuoK0dEWTxscbfG0wdEWT3EsLE8bHG3xtMHR\nFk9xLCxPGxxt8bTB0SbPMLfQBWTL8sXA/e7J+0Wg1t1nZlGMG4+vdU/gNuAIcbTP0wZHWzxtcLTF\nUxwLy9MGR1s8bXC0xVMcC8vTBkdbPG1wtMkz7C10AdkyfCnwuHtC3mjsi2V4X8x4fLv7mS9le3+h\nOdriaYOjLZ42ONriKY6F5WmDoy2eNjja4imOheVpg6MtnjY42uQZhS2GEEWa3Z+XKqUOdh+r9Dc5\njpNQSsWUUgp4xt39geRr4gjY4WmDI9jhaYMj2OEpjrnDBk8bHMEOTxscwQ5PccwdNnja4Ah2eNrg\nCPZ4ho402iOEeyLiOM484DagAnhJKXWY4zhdSqlu35fjOAlHdzO9gj7xdxa6oy2eNjja4mmDoy2e\n4lhYnjY42uJpg6MtnuJYWJ42ONriaYOjTZ5RQhrtEcJxHCd5kjqO80l0+EcZ8JR7EifST2Lj+QD0\nCb+m0B1t8bTB0RZPGxxt8RTHwvK0wdEWTxscbfEUx8LytMHRFk8bHG3yjBROBGL0ZfNv+Odt3Iqe\nt9EMHGa+jj9Jw13AVmBG+muF6miLpw2Otnja4GiLpzgWlqcNjrZ42uBoi6c4FpanDY62eNrgaJNn\nFLbQBWTL8sX0fhIXG69fBKwHfgNUiaN9njY42uJpg6MtnuJYWJ42ONriaYOjLZ7iWFieNjja4mmD\no02eYW+hC8jWw5eT/SQ+3Nh/KvAasAQYLY72etrgaIunDY62eIpjYXna4GiLpw2OtniKY2F52uBo\ni6cNjjZ5hrmFLiBbL19Q5pN4D3AIcBiwENgGTBVH+z1tcLTF0wZHWzzFsbA8bXC0xdMGR1s8xbGw\nPG1wtMXTBkebPEM7PmELyNaHLynzSbwLeMf9OU0cDxxPGxxt8bTB0RZPcSwsTxscbfG0wdEWT3Es\nLE8bHG3xtMHRJs9Qjk3YArL18Yvyn8S/cU/ircDBYbvZ5GiLpw2Otnja4GiLpzgWlqcNjrZ42uBo\ni6c4FpanDY62eNrgaJNn0JtyD4hgAUqpmOM4Cffxr4FbHMd5PWQtHzY4gh2eNjiCHZ42OIIdnuKY\nO2zwtMER7PC0wRHs8BTH3GGDpw2OYIenDY5gj2eQSKPdMsyTOKrY4Ah2eNrgCHZ42uAIdniKY+6w\nwdMGR7DD0wZHsMNTHHOHDZ42OIIdnjY4gj2eQSGNdkEQBEEQBEEQBEGIKLGwBQRBEARBEARBEARB\nyIw02gVBEARBEARBEAQhokijXRAEQRAEQRAEQRAiijTaBUEQBEEQBEEQBCGiSKNdEARBEARBEARB\nECKKNNoFQRAEQRAEQRAEIaJIo10QBEEQBEEQBEEQIoo02gVBEARBEARBEAQhokijXRAEQRAEQRAE\nQRAiijTaBUEQBEEQBEEQBCGiSKNdEARBEARBEARBECKKNNoFQRAEQRAEQRAEIaJIo10QBEEQBEEQ\nBEEQIoo02gVBEARBEARBEAQhokijXRAEQRAEQRAEQRAiijTaBUEQBEEQBEEQBCGi/H8o3VG8xOF+\nPAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1182cbf28>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 272,
       "width": 502
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "mean, std = scaled_features['cnt']\n",
    "predictions = network.run(test_features).T*std + mean\n",
    "ax.plot(predictions[0], label='Prediction')\n",
    "ax.plot((test_targets['cnt']*std + mean).values, label='Data')\n",
    "ax.set_xlim(right=len(predictions))\n",
    "ax.legend()\n",
    "\n",
    "dates = pd.to_datetime(rides.ix[test_data.index]['dteday'])\n",
    "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
    "ax.set_xticks(np.arange(len(dates))[12::24])\n",
    "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可选：思考下你的结果（我们不会评估这道题的答案）\n",
    "\n",
    " \n",
    "请针对你的结果回答以下问题。模型对数据的预测效果如何？哪里出现问题了？为何出现问题呢？\n",
    "\n",
    "> **注意**：你可以通过双击该单元编辑文本。如果想要预览文本，请按 Control + Enter\n",
    "\n",
    "#### 请将你的答案填写在下方\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
